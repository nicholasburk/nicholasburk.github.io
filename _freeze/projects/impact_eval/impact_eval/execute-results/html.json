{
  "hash": "f5c07670bdba6d444a184b790ac5bf89",
  "result": {
    "markdown": "---\ntitle: \"Impact Evaluation\"\ndescription: \"A summary of some topics in the field of impact evaluation from my volunteer work with the American Red Cross\"\ntitle-block-banner-color: white\nimage: \"thumbnail.jpg\"\ndraft: false\n---\n\n\n## Intro/Overview\n\nOne of the volunteer projects I worked on for the American Red Cross (ARC) was to help develop a cost benefit analysis for one of their programs. The International Federation of the Red Cross and Red Crescent Societies (IFRC) has a nice summary dashboard they maintain with high level information about some of their specific interventions: [https://go.ifrc.org/](https://go.ifrc.org/). \n\nOne of the programs that IFRC supports is early action or anticipatory action. Unlike a responsive program that would wait for a disaster to occur and then provide aid afterwards, an anticipatory action program aims to provide preventative aid before the disaster occurs. This can be effective for certain types of disasters where we can reasonably predict the event in advance.\n\nFor example, we might predict a coming flood using data about river levels, weather, upstream water flow, etc. If we wait until after the flood occurs, the flooding could spread water-borne diseases among the affected population and a relief effort might involve providing medicine or other forms of healthcare. Alternatively, an anticipatory action might be to provide chlorine tablets or other water treatment options before the flood occurs so that water-borne disease does not become prevalent after the flood. If effective, this type of preventative action can allow an organization to help more people with the available budget, since these types of preventative actions are often more cost efficient than treating problems after they become more serious.\n\n## Setting Up\n\n### Loading Libraries\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(lmtest) # to adjust model using clustered error covariance\nlibrary(sandwich) # to compute clustered error covariance\nlibrary(dplyr) # for data manipulation\nlibrary(tidyr) # for data manipulation\nlibrary(ggplot2) # to make plots\n```\n:::\n\n\n## Generating the Data\n\nI will generate my own fake data to use as an example for explaining some general ideas. Continuing with the flood example, let us say that we want to evaluate the effectiveness of an anticipatory action (intervention) that aimed to reduce healthcare expenditures by providing chlorine tablets immediately before a flood event to help reduce the incidence of water-borne disease. Based on the available budget, the treatment can only be provided to part of the affected population. Some time after the event, we survey the affected population to see what their actual healthcare expenditures were. This is the data that we use to evaluate the impact of our program.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# set seed for consistency\nset.seed(42)\ndf = data.frame()\n\n# assume total population of 1000 households\nn_households = 1000\n\n# each household has 1-7 individuals\nhh_size = sample.int(7, n_households, replace = TRUE)\n\n# generate data for each individual\nfor(i in 1:n_households){\n    hh = data.frame(\n        hh_id = i, # household ID\n        hh_size = hh_size[i], # household size\n        age = sample.int(65, hh_size[i], replace=TRUE), # age from 1-65\n        female = ifelse(runif(hh_size[i]) < 0.5, 1, 0)\n    )\n    df = rbind(df, hh)\n}\n\n# only 100 households receive the treatment\n# and treatment is not purely random\n# households that are larger, have more young or elderly,\n# or are more female are given some preference\ntreatment_select = df %>% \n    group_by(hh_id, hh_size) %>%\n    summarise(pct_young_old = mean((age <= 12) | (age >= 50)),\n              pct_F = mean(female)) %>%\n    mutate(treatment_prob = hh_size + 2*pct_young_old + 2*pct_F)\n\nhh_id_treatment = sample(treatment_select$hh_id, 100, \n                         prob = treatment_select$treatment_prob)\n\ndf$treatment = ifelse(df$hh_id %in% hh_id_treatment, 1, 0)\n\n# simulate actual healthcare costs\n# hh_size increases costs because of increased exposure opportunity\n# age young or old increases costs due to weaker immune systems\n# treatment is effective and reduces costs\ndf$hc_cost = 50 + 5*df$hh_size + 20*((df$age <= 12) | (df$age >= 50)) - 20*df$treatment\n\n# add cluster error and individual error\ndf_cluster_error = data.frame(hh_id = 1:n_households, \n                              e_household = rnorm(n_households, sd = 5))\n\ndf = df %>%\n    inner_join(df_cluster_error, by = \"hh_id\") %>%\n    mutate(e_individual = rnorm(nrow(df), sd = 5)) %>%\n    mutate(hc_cost = hc_cost + e_household + e_individual)\n```\n:::\n\n\n## Average Treatment Effect\n\nThe metric we want to estimate is the average treatment effect (ATE). The ATE is the expected effect of the treatment on the target outcome. In our example, this would be the dollar amount by which the treatment reduces healthcare expenses for each individual, on average. Estimating this impact is important for evaluating an intervention because it tells us how effective the intervention was at achieving the desired outcome, and hopefully can be generalized to other events and inform decisions about whether to use this same intervention in future scenarios.\n\n### Basic Calculation\n\nThe definition for ATE is the average difference in outcomes for treated vs non-treated individuals. More formally:\n\n$$\n    ATE = E[y_1 - y_0]\n$$\nOr, writing this in terms of an estimate from a sample:\n\n$$\n    \\widehat{ATE} = \\frac{1}{N} \\sum_i (y_1(i) - y_0(i))\n$$\n\nThe problem in both of these definitions is that we never observe both $y_1(i)$ and $y_0(i)$, because any one individual either receives the treatment or does not, so we cannot observe what happened to that same individual under both scenarios. So instead, the practical solution is to examine the difference in means between the treated and untreated groups, which should be a good estimate of the ATE under certain assumptions.\n\n$$\n    E[Y|X = 1] - E[Y|X = 0]\n$$\n\n::: {.cell}\n\n```{.r .cell-code}\n# calculate simple ATE by hand\nEY1 = mean(df$hc_cost[df$treatment == 1])\nEY0 = mean(df$hc_cost[df$treatment == 0])\nATE = EY1 - EY0\nATE\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] -16.81879\n```\n:::\n:::\n\n\n### Linear Regression\n\nA simple regression yields the same estimate for the ATE, since using a binary indicator for the treatment is effectively the same as just taking the mean for each group. This is a useful approach if you want to do anything beyond getting this single point estimate.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# estimate via simple linear model\nfit = lm(hc_cost ~ treatment, data = df)\nsummary(fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = hc_cost ~ treatment, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-46.734 -10.373  -0.274  10.843  46.409 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  82.8386     0.2561   323.4   <2e-16 ***\ntreatment   -16.8188     0.7474   -22.5   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 14.98 on 3872 degrees of freedom\nMultiple R-squared:  0.1157,\tAdjusted R-squared:  0.1154 \nF-statistic: 506.4 on 1 and 3872 DF,  p-value: < 2.2e-16\n```\n:::\n\n```{.r .cell-code}\n# 95% confidence interval for ATE\nconfint(fit, \"treatment\", level = 0.95)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n              2.5 %    97.5 %\ntreatment -18.28405 -15.35352\n```\n:::\n\n```{.r .cell-code}\n# same thing using the standard errors directly\ncoef(fit)[2] + qt(0.025, df=fit$df.residual) * summary(fit)$coefficients[2,2]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntreatment \n-18.28405 \n```\n:::\n\n```{.r .cell-code}\ncoef(fit)[2] + qt(0.975, df=fit$df.residual) * summary(fit)$coefficients[2,2]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntreatment \n-15.35352 \n```\n:::\n:::\n\n\nThe estimated coefficient on the treatment variable matches the manual calculation, as expected. The estimated ATE of about -17 means that receiving the treatment reduces healthcare expenditures by an average of $17 per person. However, this estimate does not quite match the true effect of -20 which is the effect I used to generate the data, and the 95% confidence interval does not include the true effect either. This is because of some additional complications I added while generating the data which will need to be addressed.\n\n## Additional Considerations\n\nThe simple estimate for ATE above is useful to look at, but there are usually more factors that need to be considered in this type of analysis. These will differ substantially based on the situation. Here we will examine a few that are relevant to this example.\n\n### Covariate Adjustment\n\nA confounding factor is a variable that influences both the dependent variable and independent variable. To estimate the effect of X on Y, we must suppress the effects of confounding variables that influence both X and Y. We say that X and Y are confounded by some other variable Z whenever Z causally influences both X and Y.\n\n![](confounding.png)\n\nIn this example, there are variables such as household size and age (Z) which influence both the treatment effect (X) and healthcare expenditures (Y). They influence treatment because treatment was not assigned randomly; the assignment was weighted to prefer larger households and households with young and elderly members. They have a direct effect on healthcare costs based on how those costs were simulated in this example.\n\nWe can account for these factors by explicitly estimating their effects in the model. Conditioning our estimate on these additional variables is sometimes called covariate adjustment. Another way to think about this is that the simple version of the model suffers from omitted variable bias and the previous estimate of the ATE was incorporating effects which were actually attributable to these confounding factors.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# add indicator for young/elderly age group\ndf$age_young_old = ifelse((df$age <= 12) | (df$age >= 50), 1, 0)\n\n# estimate via simple linear model\nfit = lm(hc_cost ~ treatment + hh_size + age_young_old, data = df)\nsummary(fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = hc_cost ~ treatment + hh_size + age_young_old, data = df)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-23.5940  -4.6384   0.0414   4.9515  24.1408 \n\nCoefficients:\n               Estimate Std. Error t value Pr(>|t|)    \n(Intercept)    49.58349    0.35093  141.29   <2e-16 ***\ntreatment     -19.71544    0.35279  -55.88   <2e-16 ***\nhh_size         5.05749    0.06416   78.83   <2e-16 ***\nage_young_old  20.12108    0.22854   88.04   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.046 on 3870 degrees of freedom\nMultiple R-squared:  0.8043,\tAdjusted R-squared:  0.8042 \nF-statistic:  5303 on 3 and 3870 DF,  p-value: < 2.2e-16\n```\n:::\n\n```{.r .cell-code}\n# 95% confidence interval for ATE\nconfint(fit, \"treatment\", level = 0.95)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n              2.5 %    97.5 %\ntreatment -20.40712 -19.02376\n```\n:::\n:::\n\n\nAfter adjusting for these confounding factors, our ATE estimate is much closer to the true value and the 95% confidence interval includes the true value.\n\n### Inverse Probability Treatment Weights\n\nAnother method for addressing this concern around confounding is through the use of propensity scores, or inverse probability treatment weights. This is a popular method for observational studies. Unlike a randomized controlled trial (RCT) where an experiment is designed in advance to have treatment and control groups balanced across other covariates of interest, an observational study has no control over the treatment assignment and will often have treatment and control groups which are not balanced across other covariates. This technique is essentially re-weighting the observational data to make it look more it came from a balanced RCT design, which is important for estimating ATE since the calculation assumes we are comparing similar individuals.\n\nThis method starts by building a model to predict a propensity score, which is the probability that the treatment was assigned to an individual. Observations are then re-weighted by dividing by this probability in order to achieve better balance.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# estimate propensity score\nfit_ps = glm(treatment ~ hh_size + age_young_old + female, data = df, family = binomial())\nehat = predict(fit_ps, type = \"response\")\ndf$ipw = df$treatment/ehat + (1-df$treatment)/(1-ehat)\n\n# estimate ATE using weighted linear model\nfit = lm(hc_cost ~ treatment, data = df, weights = ipw)\nsummary(fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = hc_cost ~ treatment, data = df, weights = ipw)\n\nWeighted Residuals:\n     Min       1Q   Median       3Q      Max \n-128.314  -12.129   -0.438   12.748   85.704 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  83.1895     0.3267  254.60   <2e-16 ***\ntreatment   -18.9913     0.4627  -41.04   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 20.34 on 3872 degrees of freedom\nMultiple R-squared:  0.3032,\tAdjusted R-squared:  0.303 \nF-statistic:  1684 on 1 and 3872 DF,  p-value: < 2.2e-16\n```\n:::\n\n```{.r .cell-code}\n# 95% confidence interval for ATE\nconfint(fit, \"treatment\", level = 0.95)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n              2.5 %    97.5 %\ntreatment -19.89851 -18.08407\n```\n:::\n:::\n\n\nThis is an improvement over the un-adjusted estimate. One additional diagnostic we can examine here is the extent to which this re-weighting improved covariate balance. This is often done by comparing Standardized Mean Differences (SMD) for the covariates of interest before and after adjustment using these weights. A common rule of thumb for good balance is a SMD threshold of 0.1, although this rule is somewhat arbitrary.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# calculate SMD for covariates of interest\ncov_bal_smd = df %>%\n    group_by(treatment) %>%\n    summarise(hh_size_mean = mean(hh_size),\n              hh_size_wmean = weighted.mean(hh_size, ipw),\n              hh_size_sd = sd(hh_size),\n              age_mean = mean(age_young_old),\n              age_wmean = weighted.mean(age_young_old, ipw),\n              female_mean = mean(female),\n              female_wmean = weighted.mean(female, ipw)) %>%\n    summarise(hh_size_unadjusted = abs(diff(hh_size_mean)) / sqrt(sum(hh_size_sd^2)/2),\n              hh_size_adjusted = abs(diff(hh_size_wmean)) / sqrt(sum(hh_size_sd^2)/2),\n              age_unadjusted = abs(diff(age_mean)),\n              age_adjusted = abs(diff(age_wmean)),\n              female_unadjusted = abs(diff(female_mean)),\n              female_adjusted = abs(diff(female_wmean))) %>%\n    pivot_longer(everything(), values_to = \"SMD\") %>%\n    mutate(adjusted = ifelse(grepl(\"unadjusted\", name), \"unadjusted\", \"adjusted\")) %>%\n    mutate(covariate = sub(\"_[^_]*$\", \"\", name))\n\n# make love plot\ncov_bal_smd %>%\n    mutate(adjusted = as.factor(adjusted)) %>%\n    ggplot(aes(x = SMD, y = covariate, color = adjusted, shape = adjusted)) +\n    geom_vline(xintercept = 0) +\n    geom_vline(xintercept = 0.1, lty = \"dashed\") +\n    geom_point(size = 3) +\n    labs(x = \"Absolute Standardized Mean Differences\", y = \"Covariate\") +\n    theme(legend.title = element_blank())\n```\n\n::: {.cell-output-display}\n![](impact_eval_files/figure-html/covariate-balance-1.png){width=672}\n:::\n:::\n\n\nWe can see that covariate balance as measured by SMD improves after re-weighting the data with the inverse probability weights. The balance for household size is under the threshold of 0.1 after re-weighting, and the balance for the other covariates has also improved although they were already under the 0.1 threshold.\n\n### Clustered Standard Errors\n\nThere are certain situations where we might expect to observe clustering in our data. In this fake example, we have created clustering in the errors by adding an error component at the household level as well as the individual level. The rationale is that people in the same household will tend to have similar experiences. For instance, if one person from a household gets sick they are likely to pass that sickness to the other members of the household.\n\nThis clustering is present in the treatment assignment as well. The treatment is assigned to an entire household, not to specific individuals. This type of study design is similar to a cluster randomized trial where treatment is assigned at a more aggregate level even though outcomes are measured at an individual level. There are a number of reasons why this design might be desirable. In this fake example, if we had a family of two parents and two young children and randomly decided to only give the treatment to the parents, it would not be reasonable to expect them to follow the study design and treat only their own drinking water and let their children drink dirty water. They would likely give their treatment to their children, causing spillover effects which would reduce the power of our estimates.\n\nWhen clustering exists in the data it means that the errors are not all independent since there is dependence within each cluster. Observations within the same cluster (household) are similar to each other. This means the standard errors estimated by the model are too small and we need to adjust them to make sure our estimates are not overconfident. The adjustment will increase the standard errors, resulting in wider confidence intervals for our ATE estimate.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# first model using covariate adjustment\nfit1 = lm(hc_cost ~ treatment + hh_size + age_young_old, data = df)\nfit1_coef_cl = coeftest(fit1, vcov = vcovCL, cluster = ~hh_id)\nfit1_coef_cl\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nt test of coefficients:\n\n               Estimate Std. Error t value  Pr(>|t|)    \n(Intercept)    49.58349    0.48875 101.450 < 2.2e-16 ***\ntreatment     -19.71544    0.67000 -29.426 < 2.2e-16 ***\nhh_size         5.05749    0.10010  50.526 < 2.2e-16 ***\nage_young_old  20.12108    0.21921  91.788 < 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n\n```{.r .cell-code}\n# 95% confidence interval for ATE\ndf_clust_ci = data.frame(\n    model = \"covariate adjustment\",\n    clustered_std_err = \"no\",\n    ci_95_lower = coef(fit1)[2] + qt(0.025, df=fit1$df.residual) * summary(fit1)$coefficients[2,2],\n    ci_95_upper = coef(fit1)[2] + qt(0.975, df=fit1$df.residual) * summary(fit1)$coefficients[2,2]\n)\ndf_clust_ci = rbind(df_clust_ci, data.frame(\n    model = \"covariate adjustment\",\n    clustered_std_err = \"yes\",\n    ci_95_lower = coef(fit1)[2] + qt(0.025, df=fit1$df.residual) * fit1_coef_cl[2,2],\n    ci_95_upper = coef(fit1)[2] + qt(0.975, df=fit1$df.residual) * fit1_coef_cl[2,2]\n))\n\n# second model using inverse probability weights\nfit2 = lm(hc_cost ~ treatment, data = df, weights = ipw)\nfit2_coef_cl = coeftest(fit2, vcov = vcovCL, cluster = ~hh_id)\nfit2_coef_cl\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nt test of coefficients:\n\n             Estimate Std. Error t value  Pr(>|t|)    \n(Intercept)  83.18950    0.41622 199.871 < 2.2e-16 ***\ntreatment   -18.99129    1.03750 -18.305 < 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n\n```{.r .cell-code}\n# 95% confidence interval for ATE\ndf_clust_ci = rbind(df_clust_ci, data.frame(\n    model = \"inverse probability weights\",\n    clustered_std_err = \"no\",\n    ci_95_lower = coef(fit2)[2] + qt(0.025, df=fit2$df.residual) * summary(fit2)$coefficients[2,2],\n    ci_95_upper = coef(fit2)[2] + qt(0.975, df=fit2$df.residual) * summary(fit2)$coefficients[2,2]\n))\ndf_clust_ci = rbind(df_clust_ci, data.frame(\n    model = \"inverse probability weights\",\n    clustered_std_err = \"yes\",\n    ci_95_lower = coef(fit2)[2] + qt(0.025, df=fit2$df.residual) * fit2_coef_cl[2,2],\n    ci_95_upper = coef(fit2)[2] + qt(0.975, df=fit2$df.residual) * fit2_coef_cl[2,2]\n))\n\n# display table\nrownames(df_clust_ci) = NULL\nknitr::kable(df_clust_ci)\n```\n\n::: {.cell-output-display}\n|model                       |clustered_std_err | ci_95_lower| ci_95_upper|\n|:---------------------------|:-----------------|-----------:|-----------:|\n|covariate adjustment        |no                |   -20.40712|   -19.02376|\n|covariate adjustment        |yes               |   -21.02903|   -18.40185|\n|inverse probability weights |no                |   -19.89851|   -18.08407|\n|inverse probability weights |yes               |   -21.02538|   -16.95719|\n:::\n:::\n\n\nAs expected, accounting for the clustering in the errors increases the estimate for the standard error of ATE and consequently widens the bounds of the associated 95% confidence interval. It makes our estimate less precise, but it is a more accurate representation of what our confidence in our estimate should be.\n\n## Conclusion\n\nWe discussed estimating ATE to evaluate the impact of an intervention and covered a few examples of the problems that can arise in this type of analysis. Of course, there are a lot of other problems that can arise and techniques available to address them which I have not covered here. The World Bank provides a general description of more of the common techniques for impact evaluations in their publication [here](https://www.worldbank.org/en/programs/sief-trust-fund/publication/impact-evaluation-in-practice) which the interested reader can download for free. Also, the use case I examined was focused on using observational data, but another approach would be to try to run the analysis concurrently with the intervention and have an RCT study design. The [Abdul Latif Jameel Poverty Action Lab (J-PAL)](https://www.povertyactionlab.org/research-resources?view=toc) has a lot of good resources on the subject, as well as impact evaluations in general. \n",
    "supporting": [
      "impact_eval_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}