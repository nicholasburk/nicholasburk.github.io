{
  "hash": "d309479b05793bf504f51fc04da25eb1",
  "result": {
    "markdown": "---\ntitle: \"LLM Transformer Implemented in Base R\"\ndescription: \"An exercise in understanding the mechanics of the transformer architechture used in Large Language Models (LLMs)\"\ntitle-block-banner-color: white\nimage: \"thumbnail.jpg\"\ndraft: false\n---\n\n\n## Intro/Overview\n\nOne of the core components in AI models like ChatGPT is a language model. These language models are usually focused on predicting the next token in a sequence, and are used to generate language. For example, if the tokens are characters then given the sequence \"orang\" the model should predict the next character is \"e\". One of the major innovations for these models is the attention mechanism introduced by Vaswani, Ashish, et al.(2017) in their paper \"Attention is all you need\". This model architecture is called a transformer, and is what is used in the GPT (Generative Pre-trained Transformer) models in ChatGPT.\n\n![](fig1.jpg){fig-align=\"center\"}\n\nIn the following work, I will be implementing the transformer architecture from scratch in base R. The figure above has both encoder and decoder blocks as it is the architecture used for translation tasks, but I will only be implementing the decoder block on the right part of the figure without the encoder block embedding section, since that is not necessary for the next token prediction task.\n\nMy work heavily references [this YouTube video](https://www.youtube.com/watch?v=kCc8FmEb1nY) from Andrej Karpathy, as well as his related [minGPT github project](https://github.com/karpathy/minGPT). Karpathy is well known for developing and teaching the first deep learning course at Stanford, CS 231n: Convolutional Neural Networks for Visual Recognition. His materials break down the core mechanics of the transformer model architecture using the pytorch library in Python. My implementation will differ by being built in R (which I am more familiar with than Python) as well as not relying on any libraries and implementing the backpropogation from scratch. This will make it slower and less efficient than Karpathy's implementation which relies on the pytorch library to handle the backpropogation, but will help improve my understanding of the full process.\n\nFull disclosure, around 80% of the content in the code blocks was constructed with assistance from an AI model: Google's Gemini 2.5 flash. I have implemented feed forward neural network models before, but this example is more complex than the proof of concepts I usually work on and Gemini was very helpful in figuring out how to structure the various functions and pass parameter information back and forth between them.\n\n## Setting Up\n\n### Data Preparation\n\nSince the intention is to follow Karpathy's example it makes the most sense to use the same dataset, which is a text file containing the complete works of William Shakespeare. The original file can be found on the [Project Gutenberg website](https://www.gutenberg.org/), but the file Karpathy uses is much cleaner so it is easier to use [his file](https://github.com/karpathy/ng-video-lecture/blob/master/input.txt) directly.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# text corpus\ntext_corpus = readLines(\"input.txt\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in readLines(\"input.txt\"): incomplete final line found on 'input.txt'\n```\n:::\n\n```{.r .cell-code}\ntext_corpus = paste(text_corpus, collapse = \"\\n\")\n\n# Create a vocabulary of unique characters\nchars = sort(unique(strsplit(text_corpus, \"\")[[1]]))\nvocab_size = length(chars)\n\n# Create a mapping from character to integer (stoi: string_to_int)\nstoi = setNames(1:vocab_size, chars)\n\n# Create a mapping from integer to character (itos: int_to_string)\nitos = setNames(chars, 1:vocab_size)\n\n# Helper functions for encoding and decoding\nencode = function(s) {\n  sapply(strsplit(s, \"\")[[1]], function(char) stoi[[char]])\n}\n\ndecode = function(l) {\n  paste(sapply(l, function(idx) itos[[as.character(idx)]]), collapse = \"\")\n}\n\ncat(\"Vocabulary size:\", vocab_size, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nVocabulary size: 65 \n```\n:::\n\n```{.r .cell-code}\ncat(\"Character to index mapping (first 5):\", paste(names(head(stoi, 5)), head(stoi, 5), sep = \":\", collapse = \", \"), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCharacter to index mapping (first 5): ':1, -:2,  :3, \n:4, !:5 \n```\n:::\n\n```{.r .cell-code}\ncat(\"Example encoding of 'hello':\", encode(\"hello\"), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nExample encoding of 'hello': 28 22 36 36 42 \n```\n:::\n\n```{.r .cell-code}\ncat(\"Example decoding of [1 2 3 4 5]:\", decode(c(1, 2, 3, 4, 5)), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nExample decoding of [1 2 3 4 5]: '- \n! \n```\n:::\n:::\n\n\n## Hyperparameters and Model Architecture\n\nUsually in a transformer model, one layer of multi-headed attention will be followed by one feed forward layer, and that will constitute one transformer block. Then there will be multiple transformer blocks in sequence that make up the model. In this example, only a single transformer block will be used for simplicity, since keeping track of all the parameters for the gradient update is already complex, even for only one block.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nblock_size = 8    # how many characters we consider for prediction\nn_embd = 16       # embedding dimension, also model dimension (d_model)\nn_head = 4        # number of attention heads\nhead_size = n_embd / n_head # dimension of each attention head\nlearning_rate = 0.01\nn_iter = 100000     # number of training iterations (more needed for real performance)\ndropout_rate = 0.10 # Dropout probability (e.g., 0.10 means 10% of neurons dropped)\ngradient_clip_threshold = 1.0 # Gradient clipping threshold\n# Batch size is 1 for simplicity in this from-scratch example.\n```\n:::\n\n\n### Training and Validation Split\n\nThe data is split into 90% training data and 10% validation data. One important note is that the first 90% of the file is used as training while the last 10% is used for validation, there is no random sampling in creating this partition. \n\nThis is a deliberate choice due to the autoregressive nature of this model, if we used simple random sampling to make this split, we could lose the independence between training and validation by training on very similar data to what would exist in validation. As a simple example, let's pretend we have a context window of 5 characters and the word \"incredible\" is in the dataset. We could end up with a case like this:\n\n- input: \"incr\", target: \"e\", split: training\n- input: \"ncre\", target: \"d\", split: validation\n- input: \"cred\", target: \"i\", split: training\n\nThe problem in this case is that the model has already been trained on most of that string already, so when it sees it in validation the performance will look similar to performance on the training set because the data is similar, and not because the model is actually generalizing well.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create training examples (X, y)\ndata_encoded = encode(text_corpus)\n\nX_data = matrix(NA, nrow = length(data_encoded) - block_size, ncol = block_size)\ny_data = matrix(NA, nrow = length(data_encoded) - block_size, ncol = block_size)\n\nfor (i in 1:(length(data_encoded) - block_size)) {\n  X_data[i, ] = data_encoded[i:(i + block_size - 1)]\n  y_data[i, ] = data_encoded[(i + 1):(i + block_size)]\n}\n\n# create 90%/10% train/validation split\nnum_samples = nrow(X_data)\ntrain_indices = 1:floor(0.9 * num_samples)\nval_indices = setdiff(1:num_samples, train_indices)\n\n# debugging, try to intentionally overfit\n# train_indices = (1:20)*20\n# val_indices = (1:20)*20\n\nX_train = X_data[train_indices, ]\ny_train = y_data[train_indices, ]\nX_val = X_data[val_indices, ]\ny_val = y_data[val_indices, ]\n\ncat(\"Shape of X_train:\", dim(X_train), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nShape of X_train: 1003846 8 \n```\n:::\n\n```{.r .cell-code}\ncat(\"Shape of y_train:\", dim(y_train), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nShape of y_train: 1003846 8 \n```\n:::\n\n```{.r .cell-code}\ncat(\"Shape of X_val:\", dim(X_val), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nShape of X_val: 111539 8 \n```\n:::\n\n```{.r .cell-code}\ncat(\"Shape of y_val:\", dim(y_val), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nShape of y_val: 111539 8 \n```\n:::\n:::\n\n\n### Core Utility Functions\n\nThese are helper functions that handle things like the softmax operation, neuron activation, dropout, etc. Most operations are being applied to matrix objects and we want the output to also be a matrix. There are a lot of scenarios where base R functions want to simplify the data type of the output and may try to return a vector or something else when the input was a matrix, so the matrix() function is used heavily here to ensure the output is still a matrix object.\n\n#### Layer Normalization\n\nI struggled a lot with the implementation of layer normalization, so it is worth adding some additional explanation here around some aspects I found confusing.\n\nLet's assume the input data to LayerNorm is $X \\in R^{N \\times D}$, where each row $X_i \\in R^D$ is a sample. LayerNorm operates independently on each sample, so for an example of the forward pass we drop the batch index $i$ while focusing on a single vector $x \\in R^D$.\n\n__Forward Pass:__\n\n$$\n\\begin{aligned}\n\\mu &= \\frac{1}{D} \\sum_{j=1}^D x_j \\\\\n\\sigma^2 &= \\frac{1}{D} \\sum_{j=1}^D (x_j - \\mu)^2 \\\\\n\\hat{x_j} &= \\frac{x_j - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} \\\\\ny_j &= \\gamma_j \\hat{x_j} + \\beta_j\n\\end{aligned}\n$$\n\nThe $y_j$ elements are the output of LayerNorm. An important observation is that the third line above shows the normalization, which is happening over the rows of $X$. Note how the first two lines calculating $\\mu$ and $\\sigma^2$ are over the dimension of the columns $D$, so each row has its own mean and variance and is being normalized. However, the affine transformation in the fourth line shows parameters $\\gamma_j$ and $\\beta_j$ with the index $j$ over the column space, so this last transformation is a scale and shift over the columns.\n\nI also had trouble understanding the backward pass for LayerNorm. The gradients for $\\gamma$ and $\\beta$ are fairly straightforward since they are just an affine transformation at the end, but passing the gradient back through to the input data was tricky, and this difference between transformations over rows vs columns complicates things somewhat.\n\n__Backward Pass:__\n\nWe want $\\frac{dL}{dx_j}$, the gradient for the input row data.\n\nUsing the chain rule:\n\n$$\n\\frac{dL}{dx_j} = \\sum_k \\frac{dL}{d\\hat{x_k}} \\frac{d\\hat{x_k}}{dx_j}\n$$\n\nwhere $k$ is a summation index (over columns) we are introducing to iterate over the columns, while $j$ is the specific column index of the individual element within the row vector $x$ that we are differentiating with respect to.\n\nComputing $\\frac{d \\hat{x_k}}{dx_j}$:\n\nrecall that:\n\n$$\n\\begin{aligned}\n\\mu &= \\frac{1}{D} \\sum_{k=1}^D x_k \\\\\n\\sigma^2 &= \\frac{1}{D} \\sum_{k=1}^D (x_k - \\mu)^2 \\\\\n\\hat{x_k} &= \\frac{x_k - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}}\n\\end{aligned}\n$$\n\nThis has two dependencies, one directly through $x_k$ as each element of $x_k$ is $x_j$ when $k=j$, and also indirectly through $\\mu$ and $\\sigma^2$ which are functions of $x_j$.\n\nFor the mean $\\mu$:\n\n$$\n\\frac{d\\mu}{dx_j} = \\frac{1}{D}\n$$\n\nFor the variance $\\sigma^2$:\n\n$$\n\\begin{aligned}\n\\frac{d\\sigma^2}{dx_j} &= \\frac{1}{D} \\sum_{k=1}^D \\frac{d}{dx_j} (x_k - \\mu)^2 \\\\\n&= \\frac{1}{D} \\sum_{k=1}^D 2(x_k - \\mu) \\frac{d}{dx_j} (x_k - \\mu) \\\\\n&= \\frac{2}{D} \\sum_{k=1}^D (x_k - \\mu) (\\delta_{kj} - \\frac{1}{D})\n\\end{aligned}\n$$\n\nNote that $\\frac{dx_k}{dx_j} = \\delta_{kj}$ (1 if k=j, 0 otherwise)\n\nNow we have to think of that summation in 2 parts. First, when k=j, then $\\delta_{kj} = 1$ and we have:\n\n$$\n(x_k - \\mu) (\\delta_{kj} - \\frac{1}{D}) = (x_j - \\mu) (1 - \\frac{1}{D})\n$$\n\nThen for the other terms where $k \\ne j$:\n\n$$\n\\sum_{k \\ne j} (x_k - \\mu) (\\delta_{kj} - \\frac{1}{D}) = - \\frac{1}{D} \\sum_{k \\ne j} (x_k - \\mu)\n$$\n\nHowever, this second term can be simplified because summing over a vector minus its mean is zero.\n\n$$\n\\sum_{k=1}^D (x_k - \\mu) = 0 \\\\\n\\sum_{k \\ne j} (x_k - \\mu) = -(x_j - \\mu)\n$$\n\nSo the second term becomes\n\n$$\n- \\frac{1}{D} * -(x_j - \\mu) = \\frac{1}{D} (x_j - \\mu)\n$$\nthen putting both parts back together gives us\n\n$$\n\\begin{aligned}\n\\frac{d\\sigma^2}{dx_j} &= \\frac{2}{D}((x_j - \\mu) (1 - \\frac{1}{D}) + \\frac{1}{D} (x_j - \\mu)) \\\\\n&= \\frac{2}{D}(x_j - \\mu)\n\\end{aligned}\n$$\n\nNow we can use the results for $\\frac{d\\mu}{dx_j}$ and $\\frac{d\\sigma^2}{dx_j}$ to calculate $\\frac{d\\hat{x_k}}{dx_j}$. Using the quotient rule:\n\n$$\n\\begin{aligned}\n\\frac{d\\hat{x_k}}{dx_j} &= (\\sqrt{\\sigma^2 + \\epsilon} \\frac{d}{dx_j} [(x_k - \\mu)] - (x_k - \\mu) \\frac{d}{dx_j} [\\sqrt{\\sigma^2 + \\epsilon}]) (\\sigma^2 + \\epsilon)^{-1} \\\\\n&= (\\sigma^2 + \\epsilon)^{-1/2}(\\delta_{kj} - \\frac{1}{D}) - (x_k - \\mu) \\frac{1}{2}(\\sigma^2 + \\epsilon)^{-3/2} \\frac{d\\sigma^2}{dx_j} \\\\\n&= (\\sigma^2 + \\epsilon)^{-1/2}(\\delta_{kj} - \\frac{1}{D}) - (x_k - \\mu) \\frac{1}{2}(\\sigma^2 + \\epsilon)^{-3/2} \\frac{2}{D}(x_j - \\mu) \\\\\n&= \\frac{1}{\\sqrt{\\sigma^2 + \\epsilon}} [\\delta_{kj} - \\frac{1}{D} - \\frac{(x_k - \\mu)(x_j - \\mu)}{(\\sigma^2 + \\epsilon)D}]\n\\end{aligned}\n$$\n\nFinally, we can plug this back into the first equation for the gradient we are after. To simplify the notation slightly, we will say $\\frac{dL}{d\\hat{x_k}} = \\hat{g_k}$\n\n$$\n\\begin{aligned}\n\\frac{dL}{dx_j} &= \\sum_k \\frac{dL}{d\\hat{x_k}} \\frac{d\\hat{x_k}}{dx_j} \\\\\n&= \\sum_k \\hat{g_k} \\frac{d\\hat{x_k}}{dx_j} \\\\\n&= \\sum_k \\hat{g_k} \\frac{1}{\\sqrt{\\sigma^2 + \\epsilon}} [\\delta_{kj} - \\frac{1}{D} - \\frac{(x_k - \\mu)(x_j - \\mu)}{(\\sigma^2 + \\epsilon)D}] \\\\\n&= \\frac{1}{\\sqrt{\\sigma^2 + \\epsilon}} (\\hat{g_j} - \\frac{1}{D} \\sum_k \\hat{g_k} - \\frac{(x_j - \\mu)}{(\\sigma^2 + \\epsilon)D} \\sum_k \\hat{g_k} (x_k - \\mu)\n\\end{aligned}\n$$\n\nIf you look at the layer norm backward function implemented below, you can see that the above formula is what is used to calculate the gradient d_X.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Softmax activation function (row-wise for matrices)\nsoftmax_matrix_rows = function(X) {\n  t(apply(X, 1, function(row) {\n    exp_row = exp(row - max(row)) # Numerical stability\n    return(exp_row / sum(exp_row))\n  })) # wrapped with t() to transpose back to original shape\n}\n\n# Layer Normalization\nlayer_norm_forward = function(X, gamma, beta, epsilon = 1e-5) {\n  mean_X = apply(X, 1, mean)\n  var_X = apply(X, 1, var)\n\n  X_norm = (X - matrix(mean_X, nrow = nrow(X), ncol = ncol(X), byrow = FALSE)) /\n           sqrt(matrix(var_X + epsilon, nrow = nrow(X), ncol = ncol(X), byrow = FALSE))\n\n  out = X_norm * matrix(gamma, nrow = nrow(X), ncol = ncol(X), byrow = TRUE) +\n        matrix(beta, nrow = nrow(X), ncol = ncol(X), byrow = TRUE)\n\n  return(list(out = out, X_norm = X_norm, mean_X = mean_X, var_X = var_X, X = X)) # Store X for backward\n}\n\nlayer_norm_backward = function(dout, X, X_norm, mean_X, var_X, gamma, epsilon = 1e-5) {\n  N = nrow(X) # Number of samples (batch size)\n  D = ncol(X) # Number of features (embedding dimension)\n\n  # Gradients for gamma and beta\n  dgamma = colSums(dout * X_norm)\n  dbeta = colSums(dout)\n\n  # Gradient of X_norm\n  d_X_norm = dout * matrix(gamma, nrow = N, ncol = D, byrow = TRUE)\n\n  std_inv = 1 / sqrt(var_X + epsilon)  # (N,)\n  std_inv_mat = matrix(std_inv, nrow = N, ncol = D)\n\n  # Sum over features (columns) for each row\n  d_X_norm_sum = rowSums(d_X_norm)                   # shape (N,)\n  d_X_norm_dot_Xnorm = rowSums(d_X_norm * X_norm)    # shape (N,)\n\n  term1 = d_X_norm\n  term2 = matrix(d_X_norm_sum / D, nrow = N, ncol = D)\n  term3 = X_norm * matrix(d_X_norm_dot_Xnorm / D, nrow = N, ncol = D)\n\n  d_X = (term1 - term2 - term3) * std_inv_mat\n\n  return(list(d_X = d_X, dgamma = dgamma, dbeta = dbeta))\n}\n\n# ReLU activation function\nrelu = function(x) {\n  matrix(pmax(0, x), nrow = nrow(x), ncol = ncol(x))\n}\n\nrelu_grad = function(x) {\n  (x > 0) * 1\n}\n\n# Dropout layer\ndropout_forward = function(x, dropout_rate, is_training) {\n  if (is_training) {\n    mask = matrix(runif(nrow(x)*ncol(x)), nrow = nrow(x), ncol = ncol(x)) > dropout_rate\n    out = x * mask / (1 - dropout_rate) # Scale up during training\n    return(list(out = out, mask = mask))\n  } else {\n    return(list(out = x, mask = NULL)) # No dropout during inference\n  }\n}\n\ndropout_backward = function(dout, mask, dropout_rate) {\n  # Apply the same mask and scaling to the gradient\n  return(dout * mask / (1 - dropout_rate))\n}\n\n# He Initialization function for weights\nhe_init_weights = function(fan_in, fan_out, ReLU_activation = FALSE) {\n  # Only account for halving the variance if ReLU actiavtion is being applied\n  if(ReLU_activation){\n    std_dev = sqrt(2 / fan_in)\n  }else{\n    std_dev = sqrt(1 / fan_in)\n  }\n    \n  weights = matrix(rnorm(fan_in * fan_out, mean = 0, sd = std_dev),\n                   nrow = fan_in, ncol = fan_out)\n  return(weights)\n}\n\n# Gradient Clipping function (by global norm)\nclip_gradients_by_norm = function(gradients_list, clip_threshold) {\n  # Flatten all gradients into a single vector to calculate the norm\n  all_grad_elements = c()\n  for (grad_name in names(gradients_list)) {\n    all_grad_elements = c(all_grad_elements, as.vector(gradients_list[[grad_name]]))\n  }\n\n  global_norm = sqrt(sum(all_grad_elements^2))\n\n  # If the global norm exceeds the threshold, scale all gradients\n  if (global_norm > clip_threshold) {\n    scale_factor = clip_threshold / global_norm\n    clipped_gradients = lapply(gradients_list, function(grad_matrix) {\n      grad_matrix * scale_factor\n    })\n    return(clipped_gradients)\n  } else {\n    # No clipping needed\n    return(gradients_list)\n  }\n}\n```\n:::\n\n\n### Model Parameters Initialization\n\nMany of the parameters will be initialized using He initialization. He initialization (also known as Kaiming initialization) is a weight initialization technique widely used in deep neural networks, especially when using Rectified Linear Unit (ReLU) as the activation function which we are doing here. The weights are determined like this:\n\n$$\nW = N(0, \\sigma^2) \\text{ where } \\sigma = \\sqrt{\\frac{2}{n_{in}}}\n$$\n\nThe basic idea is to choose starting weights that won't immediately cause problems with vanishing or exploding gradients. We want to keep the variance of activations and gradients consistent as we move forward and backward through the layers in the network. Consider the simple feed forward layer as follows:\n\n$$\n\\begin{aligned}\n&y = Wx + b \\\\\n&a = f(y)\n\\end{aligned}\n$$\n\nwhere:\n\n- x is the input vector to the layer.\n- W is the weight matrix of the layer.\n- b is the bias vector.\n- y is the linear output before activation.\n- f is the activation function (e.g., ReLU).\n- a is the activated output of the layer.\n\nFor simplicity, let's assume the biases b are initialized to zero (which is common and is what we are doing here) and that the elements of x and W are independent and have zero mean.\n\nReLU is defined as f(y)=max(0,y). This means that for y<0, the output is 0, and for y>=0, the output is y. When initialized, roughly half of the inputs to a ReLU neuron will be negative (resulting in zero output) and half will be positive (resulting in the input value). This effectively halves the variance compared to a linear activation function.\n\nWe want the variance of the output a to be roughly equal to the variance of the input x.\n\nConsider the variance of y=Wx:\n\n$$ \nvar(y) = var(\\sum_{i=1}^{n_{in}} W_i x_i)\n$$\n\nAssuming $W_i$ and $x_i$ are independent and have zero mean:\n\n$$\n\\begin{aligned}\nvar(y) &= \\sum_{i=1}^{n_{in}} var(W_i x_i) \\\\\n&= \\sum_{i=1}^{n_{in}} E[(W_i x_i)^2] - E[(W_i x_i)]^2 \\\\\n&= \\sum_{i=1}^{n_{in}} E[W_i^2] E[x_i^2] - (E[W_i] E[x_i])^2 \\\\\n&= \\sum_{i=1}^{n_{in}} (var(W_i) + E[W_i]^2) (var(x_i) + E[x_i]^2) - (E[W_i] E[x_i])^2 \\\\\n&= \\sum_{i=1}^{n_{in}} (E[W_i]^2 var(x_i) + E[x_i]^2 var(W_i) + var(W_i)var(x_i))\n\\end{aligned}\n$$\n\nSince we assume $E[W_i]=0$ and $E[x_i]=0$:\n\n$$\n    var(y) = \\sum_{i=1}^{n_{in}} var(W_i)var(x_i)\n$$\nAssuming all $W_i$ and $x_i$ are identically distributed:\n\n$$\n    var(y) = n_{in} var(W)var(x)\n$$\n\nNow, for ReLU, a=max(0,y). If y has zero mean and is symmetric around zero, then roughly half of the values of y will be positive and half will be negative. The negative values become zero, effectively reducing the variance by half. So, for the output after ReLU:\n\n$$\n\\begin{aligned}\nvar(a) &\\approx \\frac{1}{2} var(y) \\\\\n&\\approx \\frac{1}{2} n_{in} var(W)var(x)\n\\end{aligned}\n$$\n\nRemember the objective: We want the variance of the output a to be roughly equal to the variance of the input x. So we substitute $var(x)$ for $var(a)$ in this last equation, solving for the variance in weights $W$ that achieve the desired outcome.\n\n$$\n    var(x) \\approx \\frac{1}{2} n_{in} var(W)var(x)\n$$\n\nand therefore:\n\n$$\n\\begin{aligned}\n\\frac{1}{2} n_{in} var(W) &= 1 \\\\\nvar(W_i) &= \\frac{2}{n_{in}}\n\\end{aligned}\n$$\n\nMost layers don't have activation functions applied, so we only double the variance in the He initialization for the layer where the ReLU activation is used. In other cases, the same logic applies, just without the need to double the variance.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(42) # For reproducibility\n\n# Character Embeddings - typically not He-initialized, using uniform\nC = matrix(runif(vocab_size * n_embd, -1, 1), nrow = vocab_size, ncol = n_embd)\n\n# Positional Embeddings - typically not He-initialized (often fixed or small random)\nP_emb = matrix(runif(block_size * n_embd, -1, 1), nrow = block_size, ncol = n_embd)\n\n# Multi-Head Attention Parameters (using He Initialization)\n# For Wq, Wk, Wv, fan_in is n_embd\nWq = he_init_weights(n_embd, n_head * head_size)\nWk = he_init_weights(n_embd, n_head * head_size)\nWv = he_init_weights(n_embd, n_head * head_size)\n# For Wo, fan_in is n_head * head_size\nWo = he_init_weights(n_head * head_size, n_embd)\n\n# LayerNorm for Attention - gamma initialized to 1s, beta to 0s\nln1_gamma = rep(1, n_embd)\nln1_beta = rep(0, n_embd)\n\n# Feed-Forward Network Parameters (using He Initialization)\n# For W_ff1, fan_in is n_embd\nW_ff1 = he_init_weights(n_embd, 4 * n_embd, ReLU_activation = TRUE)\nb_ff1 = rep(0, 4 * n_embd) # Biases are kept at 0\n# For W_ff2, fan_in is 4 * n_embd\nW_ff2 = he_init_weights(4 * n_embd, n_embd)\nb_ff2 = rep(0, n_embd) # Biases are kept at 0\n\n# LayerNorm for FF - gamma initialized to 1s, beta to 0s\nln2_gamma = rep(1, n_embd)\nln2_beta = rep(0, n_embd)\n\n# Final linear layer to logits (using He Initialization)\n# For W_final, fan_in is n_embd\nW_final = he_init_weights(n_embd, vocab_size)\nb_final = rep(0, vocab_size) # Biases are kept at 0\n\n# Store all parameters in a list for easier management\nparams = list(\n  C = C, P_emb = P_emb,\n  Wq = Wq, Wk = Wk, Wv = Wv, Wo = Wo,\n  ln1_gamma = ln1_gamma, ln1_beta = ln1_beta,\n  W_ff1 = W_ff1, b_ff1 = b_ff1, W_ff2 = W_ff2, b_ff2 = b_ff2,\n  ln2_gamma = ln2_gamma, ln2_beta = ln2_beta,\n  W_final = W_final, b_final = b_final\n)\n\ncat(\"Parameters initialized.\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nParameters initialized.\n```\n:::\n:::\n\n\n## Transformer Block Implementation Functions\n\n### Multi-Head Self-Attention\n\nThe self-attention mechanism helps the model learn weights associated with prior tokens in the sequence. It could actually look forward as well if allowed to do so, like when translation is the desired function, but usually for next token prediction the attention mechanism is restricted to only look backwards at previous tokens. This is handled through the \"is_causal\" parameter in the function below, which will set the weights for any future token positions to zero during the softmax step.\n\nThe multi-headed piece of the mechanism is basically because multiple \"heads\" of the attention mechanism are run in parallel, and then the output is joined together in a final linear layer at the end. This lets the model learn more than one way to pay attention to previous tokens, if that is beneficial for the prediction.\n\nThis mechanism is summarized in figure 2 of Vaswani, Ashish, et al.(2017), reproduced here:\n\n![](fig2.jpg){fig-align=\"center\"}\n\nThe key equation from that paper which we are implementing here is:\n\n$$\n\\text{Attention}(Q, K, V) = \\text{softmax}(\\frac{QK^T}{\\sqrt{d_k}}) V\n$$\n\nThe softmax function produces weights which are what determine how much attention is paid to previous tokens, while the V matrix contains the values emitted by those previous tokens.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmulti_head_attention_forward = function(x, Wq, Wk, Wv, Wo, n_head, head_size, ln_gamma, ln_beta, dropout_rate, is_training, is_causal = TRUE) {\n  x_in = x\n\n  # Pre-LN:\n  ln_out_x = layer_norm_forward(x, ln_gamma, ln_beta)\n  x = ln_out_x$out\n\n  Q_all = x %*% Wq\n  K_all = x %*% Wk\n  V_all = x %*% Wv\n\n  outputs_per_head = lapply(1:n_head, function(h) {\n    start_col = (h - 1) * head_size + 1\n    end_col = h * head_size\n    Q_h = Q_all[, start_col:end_col]\n    K_h = K_all[, start_col:end_col]\n    V_h = V_all[, start_col:end_col]\n\n    scores = (Q_h %*% t(K_h)) / sqrt(head_size)\n\n    if (is_causal) {\n      mask_causal = upper.tri(matrix(0, nrow(scores), ncol(scores)), diag = FALSE)\n      scores[mask_causal] = -Inf\n    }\n\n    attention_weights = softmax_matrix_rows(scores)\n    # Dropout after attention weights (before multiplying by V)\n    attn_weights_dropout = dropout_forward(attention_weights, dropout_rate, is_training)\n    attention_weights_dropped = attn_weights_dropout$out\n\n    attended_values = attention_weights_dropped %*% V_h\n    return(list(attended_values = attended_values,\n                attention_weights = attention_weights, # Original for backward\n                attention_weights_dropped_mask = attn_weights_dropout$mask,\n                scores = scores, Q_h = Q_h, K_h = K_h, V_h = V_h))\n  })\n\n  concat_heads = do.call(cbind, lapply(outputs_per_head, function(o) o$attended_values))\n  attn_out = concat_heads %*% Wo\n\n  # Dropout after attention block output (before residual)\n  attn_out_dropout = dropout_forward(attn_out, dropout_rate, is_training)\n  attn_out_dropped = attn_out_dropout$out\n\n  out = x_in + attn_out_dropped\n\n  return(list(\n    out = out,\n    attn_out_dropped = attn_out_dropped,\n    attn_out_dropout_mask = attn_out_dropout$mask,\n    concat_heads = concat_heads,\n    outputs_per_head = outputs_per_head,\n    x_for_qkvo = x, # Input to Q, K, V linear layers (post-LN)\n    ln_cache_x = ln_out_x\n  ))\n}\n\nmulti_head_attention_backward = function(dout, x_in, attn_out_dropped, attn_out_dropout_mask, concat_heads,\n                                       outputs_per_head, x_for_qkvo, ln_cache_x, Wq, Wk, Wv, Wo, n_head, head_size, ln_gamma, ln_beta, dropout_rate, is_causal = TRUE) {\n\n  # Backward through dropout on attn_out\n  d_attn_out = dropout_backward(dout, attn_out_dropout_mask, dropout_rate)\n\n  d_Wo = t(concat_heads) %*% d_attn_out\n  d_concat_heads = d_attn_out %*% t(Wo)\n\n  d_Q_all = matrix(0, nrow = nrow(x_for_qkvo), ncol = ncol(Wq))\n  d_K_all = matrix(0, nrow = nrow(x_for_qkvo), ncol = ncol(Wk))\n  d_V_all = matrix(0, nrow = nrow(x_for_qkvo), ncol = ncol(Wv))\n\n  for (h in 1:n_head) {\n    head_cache = outputs_per_head[[h]]\n    d_attended_values = d_concat_heads[, ((h - 1) * head_size + 1):(h * head_size)]\n\n    # Backward through dropout on attention_weights\n    d_attention_weights_dropped = d_attended_values %*% t(head_cache$V_h)\n    d_attention_weights = dropout_backward(d_attention_weights_dropped, head_cache$attention_weights_dropped_mask, dropout_rate)\n\n    d_V_h = t(head_cache$attention_weights_dropped) %*% d_attended_values # Use dropped weights for V_h grad\n\n    # Backward through softmax (simplified Jacobian-vector product)\n    d_scores = d_attention_weights * head_cache$attention_weights # Element-wise multiply for part of softmax grad\n    d_scores = d_scores - rowSums(d_scores * head_cache$attention_weights) * head_cache$attention_weights # Sum over columns and apply second part\n\n    if (is_causal) {\n      mask_causal = upper.tri(matrix(0, nrow(d_scores), ncol(d_scores)), diag = FALSE)\n      d_scores[mask_causal] = 0\n    }\n\n    d_Q_h = (d_scores %*% head_cache$K_h) / sqrt(head_size)\n    d_K_h = (t(d_scores) %*% head_cache$Q_h) / sqrt(head_size)\n\n    d_Q_all[, ((h - 1) * head_size + 1):(h * head_size)] = d_Q_all[, ((h - 1) * head_size + 1):(h * head_size)] + d_Q_h\n    d_K_all[, ((h - 1) * head_size + 1):(h * head_size)] = d_K_all[, ((h - 1) * head_size + 1):(h * head_size)] + d_K_h\n    d_V_all[, ((h - 1) * head_size + 1):(h * head_size)] = d_V_all[, ((h - 1) * head_size + 1):(h * head_size)] + d_V_h\n  }\n\n  d_Wq = t(x_for_qkvo) %*% d_Q_all\n  d_Wk = t(x_for_qkvo) %*% d_K_all\n  d_Wv = t(x_for_qkvo) %*% d_V_all\n\n  d_x_from_qkvo = (d_Q_all %*% t(Wq)) + (d_K_all %*% t(Wk)) + (d_V_all %*% t(Wv))\n\n  ln_grads_x = layer_norm_backward(d_x_from_qkvo, ln_cache_x$X, ln_cache_x$X_norm,\n                                   ln_cache_x$mean_X, ln_cache_x$var_X, ln_gamma, epsilon = 1e-5)\n  d_ln1_gamma = ln_grads_x$dgamma\n  d_ln1_beta = ln_grads_x$dbeta\n  d_x = dout + ln_grads_x$d_X\n\n  return(list(\n    d_x = d_x, d_Wq = d_Wq, d_Wk = d_Wk, d_Wv = d_Wv, d_Wo = d_Wo,\n    d_ln1_gamma = d_ln1_gamma, d_ln1_beta = d_ln1_beta\n  ))\n}\n```\n:::\n\n\n### Position-wise Feed-Forward Network\n\nThis is a fully connected feed forward neural network with a single hidden layer. An activation function (ReLU in this case) is applied to the hidden layer. The hidden layer is 4 times the size of the input and output layers in this implementation, but there is no strong reason that needs to be the case. This image shows a common visualization for how this type of network looks.\n\n![](fig3.png){fig-align=\"center\"}\n\n$$\n\\begin{aligned}\n&a = f(W_1x + b_1) \\\\\n&y = W_2a + b_2\n\\end{aligned}\n$$\n\nwhere:\n\n- x is the input vector to the layer.\n- W1 and W2 are the weight matrices into and out of the hidden layer.\n- b1 and b2 are the bias vectors.\n- f is the activation function (e.g., ReLU).\n- a is the activated output of the hidden layer.\n- y is the linear output after activation.\n\nIn the visualization above, the rows of x are the nodes on the left, they become the nodes a in the center after the first transformation, and then become the output y after the second transformation.\n\nGiven that the transformations are all linear and the activation function ReLU(x) = max(x, 0) is simple to work with, the implementation is fairly straightforward.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfeed_forward_forward = function(x, W_ff1, b_ff1, W_ff2, b_ff2, ln_gamma, ln_beta, dropout_rate, is_training) {\n  x_in = x\n\n  # Pre-LN:\n  ln_out_x = layer_norm_forward(x, ln_gamma, ln_beta)\n  x = ln_out_x$out\n\n  # First linear layer + ReLU\n  hidden = x %*% W_ff1 + matrix(b_ff1, nrow = nrow(x), ncol = ncol(W_ff1), byrow = TRUE)\n  hidden_activated = relu(hidden)\n\n  # Dropout after hidden activation\n  hidden_dropout = dropout_forward(hidden_activated, dropout_rate, is_training)\n  hidden_dropped = hidden_dropout$out\n\n  # Second linear layer\n  ff_out = hidden_dropped %*% W_ff2 + matrix(b_ff2, nrow = nrow(hidden_dropped), ncol = ncol(W_ff2), byrow = TRUE)\n\n  # Dropout after FF block output (before residual)\n  ff_out_dropout = dropout_forward(ff_out, dropout_rate, is_training)\n  ff_out_dropped = ff_out_dropout$out\n\n  out = x_in + ff_out_dropped\n\n  return(list(\n    out = out,\n    ff_out_dropped = ff_out_dropped,\n    ff_out_dropout_mask = ff_out_dropout$mask,\n    hidden = hidden,\n    hidden_activated = hidden_activated,\n    hidden_dropout_mask = hidden_dropout$mask,\n    ln_cache_x = ln_out_x\n  ))\n}\n\nfeed_forward_backward = function(dout, x_in, ff_out_dropped, ff_out_dropout_mask, hidden, hidden_activated, hidden_dropout_mask,\n                                 ln_cache_x, W_ff1, b_ff1, W_ff2, b_ff2, ln_gamma, ln_beta, dropout_rate) {\n\n  # Backward through dropout on ff_out\n  d_ff_out = dropout_backward(dout, ff_out_dropout_mask, dropout_rate)\n\n  d_W_ff2 = t(hidden_activated) %*% d_ff_out # Use original hidden_activated here for grad of W_ff2\n  d_b_ff2 = colSums(d_ff_out)\n\n  d_hidden_dropped = d_ff_out %*% t(W_ff2)\n  # Backward through dropout on hidden\n  d_hidden_activated = dropout_backward(d_hidden_dropped, hidden_dropout_mask, dropout_rate)\n\n  d_hidden = d_hidden_activated * relu_grad(hidden)\n\n  d_W_ff1 = t(ln_cache_x$out) %*% d_hidden\n  d_b_ff1 = colSums(d_hidden)\n\n  d_x_from_ff = d_hidden %*% t(W_ff1)\n\n  ln_grads_x = layer_norm_backward(d_x_from_ff, ln_cache_x$X, ln_cache_x$X_norm,\n                                   ln_cache_x$mean_X, ln_cache_x$var_X, ln_gamma, epsilon = 1e-5)\n  d_ln2_gamma = ln_grads_x$dgamma\n  d_ln2_beta = ln_grads_x$dbeta\n  d_x = dout + ln_grads_x$d_X\n\n  return(list(\n    d_x = d_x, d_W_ff1 = d_W_ff1, d_b_ff1 = d_b_ff1,\n    d_W_ff2 = d_W_ff2, d_b_ff2 = d_b_ff2,\n    d_ln2_gamma = d_ln2_gamma, d_ln2_beta = d_ln2_beta\n  ))\n}\n```\n:::\n\n\n### Transformer Model Forward Pass and Loss\n\nThis function puts together all the pieces we've built previously to get a single forward pass for the full model. It starts by getting the token embeddings for the input data (from the C matrix parameter) and adding the position embedding parameter data. This feeds into the single transformer block for this model (one layer of multi-headed self-attention, followed by a feed forward neural network with a single hidden layer). Then finally one last linear layer which converts the inputs into probabilities over the vocabulary space.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntransformer_forward = function(X_batch, y_batch, params, dropout_rate, is_training) {\n  # 1. Input and Positional Embeddings\n  x_emb = t(sapply(1:block_size, function(j) params$C[X_batch[1, j], ]))\n  x = x_emb + params$P_emb\n\n  # Dropout after combined embeddings\n  embed_dropout_out = dropout_forward(x, dropout_rate, is_training)\n  x = embed_dropout_out$out\n\n  # Transformer Block\n  attn_out_cache = multi_head_attention_forward(x, params$Wq, params$Wk, params$Wv, params$Wo,\n                                                n_head, head_size, params$ln1_gamma, params$ln1_beta,\n                                                dropout_rate, is_training)\n  x = attn_out_cache$out\n\n  ff_out_cache = feed_forward_forward(x, params$W_ff1, params$b_ff1, params$W_ff2, params$b_ff2,\n                                     params$ln2_gamma, params$ln2_beta, dropout_rate, is_training)\n  x = ff_out_cache$out\n\n  # Final linear layer to get logits for all tokens in the sequence\n  logits = x %*% params$W_final + matrix(params$b_final, nrow = block_size, ncol = vocab_size, byrow = TRUE)\n\n  # Softmax to get probabilities\n  probs = softmax_matrix_rows(logits)\n\n  # Select the probability of the true token for each entry in the flattened batch\n  y_batch_flat = as.vector(t(y_batch))\n  indices = cbind(1:length(y_batch_flat), y_batch_flat)\n  correct_probs = probs[indices]\n  \n  # Compute Negative Log Likelihood / Cross-Entropy\n  token_losses = -log(correct_probs)\n  loss = mean(token_losses)\n\n  return(list(\n    logits = logits, probs = probs, loss = loss,\n    x_emb = x_emb,\n    embed_dropout_mask = embed_dropout_out$mask,\n    attn_cache = attn_out_cache,\n    ff_cache = ff_out_cache,\n    last_x_output = x\n  ))\n}\n```\n:::\n\n\n### Transformer Model Backward Pass\n\nThis function puts together all the pieces we've built previously to get a single backward pass for the full model, producing gradients for all the parameters. It works backwards from the final output of probabilities, computing gradients for each of the model parameters and the associated data, and passing the gradient for the input data back to each prior step along the series of model transformations.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntransformer_backward = function(cache, X_batch, y_batch, params, dropout_rate) {\n  grads = list(\n    d_C = matrix(0, nrow = nrow(params$C), ncol = ncol(params$C)),\n    d_P_emb = matrix(0, nrow = nrow(params$P_emb), ncol = ncol(params$P_emb)),\n    d_Wq = matrix(0, nrow = nrow(params$Wq), ncol = ncol(params$Wq)),\n    d_Wk = matrix(0, nrow = nrow(params$Wk), ncol = ncol(params$Wk)),\n    d_Wv = matrix(0, nrow = nrow(params$Wv), ncol = ncol(params$Wv)),\n    d_Wo = matrix(0, nrow = nrow(params$Wo), ncol = ncol(params$Wo)),\n    d_ln1_gamma = rep(0, length(params$ln1_gamma)),\n    d_ln1_beta = rep(0, length(params$ln1_beta)),\n    d_W_ff1 = matrix(0, nrow = nrow(params$W_ff1), ncol = ncol(params$W_ff1)),\n    d_b_ff1 = rep(0, length(params$b_ff1)),\n    d_W_ff2 = matrix(0, nrow = nrow(params$W_ff2), ncol = ncol(params$W_ff2)),\n    d_b_ff2 = rep(0, length(params$b_ff2)),\n    d_ln2_gamma = rep(0, length(params$ln2_gamma)),\n    d_ln2_beta = rep(0, length(params$ln2_beta)),\n    d_W_final = matrix(0, nrow = nrow(params$W_final), ncol = ncol(params$W_final)),\n    d_b_final = rep(0, length(params$b_final))\n  )\n\n  # Transpose y_batch to get elements row by row, then flatten\n  y_batch_flat = as.vector(t(y_batch))\n  \n  # Create one-hot targets for the flattened y_batch\n  one_hot_targets_flat = matrix(0, nrow = length(y_batch_flat), ncol = vocab_size)\n  one_hot_targets_flat[cbind(1:length(y_batch_flat), y_batch_flat)] = 1\n  \n  d_logits = cache$probs - one_hot_targets_flat\n\n  d_W_final = t(cache$last_x_output) %*% d_logits\n  d_b_final = colSums(d_logits)\n  \n  grads$d_W_final = grads$d_W_final + d_W_final\n  grads$d_b_final = grads$d_b_final + d_b_final\n\n  # d_last_token_output = d_logits %*% t(params$W_final)\n  # \n  # d_x_from_logits = matrix(0, nrow = block_size, ncol = n_embd)\n  # d_x_from_logits[block_size, ] = d_last_token_output[1, ]\n  \n  d_x_from_logits = d_logits %*% t(params$W_final)\n\n  ff_grads = feed_forward_backward(d_x_from_logits, cache$attn_cache$out, cache$ff_cache$ff_out_dropped,\n                                   cache$ff_cache$ff_out_dropout_mask, cache$ff_cache$hidden,\n                                   cache$ff_cache$hidden_activated, cache$ff_cache$hidden_dropout_mask,\n                                   cache$ff_cache$ln_cache_x, params$W_ff1, params$b_ff1,\n                                   params$W_ff2, params$b_ff2, params$ln2_gamma, params$ln2_beta, dropout_rate)\n  d_x_from_ff = ff_grads$d_x\n  grads$d_W_ff1 = grads$d_W_ff1 + ff_grads$d_W_ff1\n  grads$d_b_ff1 = grads$d_b_ff1 + ff_grads$d_b_ff1\n  grads$d_W_ff2 = grads$d_W_ff2 + ff_grads$d_W_ff2\n  grads$d_b_ff2 = grads$d_b_ff2 + ff_grads$d_b_ff2\n  grads$d_ln2_gamma = grads$d_ln2_gamma + ff_grads$d_ln2_gamma\n  grads$d_ln2_beta = grads$d_ln2_beta + ff_grads$d_ln2_beta\n\n  attn_grads = multi_head_attention_backward(d_x_from_ff, cache$x_emb + params$P_emb,\n                                             cache$attn_cache$attn_out_dropped, cache$attn_cache$attn_out_dropout_mask,\n                                             cache$attn_cache$concat_heads, cache$attn_cache$outputs_per_head,\n                                             cache$attn_cache$x_for_qkvo, cache$attn_cache$ln_cache_x,\n                                             params$Wq, params$Wk, params$Wv, params$Wo, n_head, head_size,\n                                             params$ln1_gamma, params$ln1_beta, dropout_rate)\n\n  d_x_from_attn = attn_grads$d_x\n  grads$d_Wq = grads$d_Wq + attn_grads$d_Wq\n  grads$d_Wk = grads$d_Wk + attn_grads$d_Wk\n  grads$d_Wv = grads$d_Wv + attn_grads$d_Wv\n  grads$d_Wo = grads$d_Wo + attn_grads$d_Wo\n  grads$d_ln1_gamma = grads$d_ln1_gamma + attn_grads$d_ln1_gamma\n  grads$d_ln1_beta = grads$d_ln1_beta + attn_grads$d_ln1_beta\n\n  # Backward through embedding dropout\n  d_x_from_attn_and_pos = dropout_backward(d_x_from_attn, cache$embed_dropout_mask, dropout_rate)\n\n  # Gradient for Positional Embeddings\n  grads$d_P_emb = grads$d_P_emb + d_x_from_attn_and_pos\n\n  # Gradient for Character Embeddings (C)\n  for (i in 1:block_size) {\n    char_idx = X_batch[1, i]\n    grads$d_C[char_idx, ] = grads$d_C[char_idx, ] + d_x_from_attn_and_pos[i, ]\n  }\n\n  return(grads)\n}\n```\n:::\n\n\n## Training Loop\n\nThis is where we train the model by picking one example sequence of tokens at a time (batch size is 1 in this implementation), calculating the loss, and updating the model parameters with their calculated gradients. Gradient clipping is applied here as a preventative measure to address the potential for the exploding gradient problem. Training and validation loss are printed out after every 10000 training examples. Training loss is very noisy since it is calculated on only one training example at a time, as compared to validation loss which is calculated over the full validation dataset.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Training loop\nfor (iter in 1:n_iter) {\n  # --- Training Step ---\n  # Sample a random training example\n  idx_train = sample(1:nrow(X_train), 1)\n  X_batch_train = matrix(X_train[idx_train, ], nrow = 1)\n  y_batch_train = matrix(y_train[idx_train, ], nrow = 1)\n\n  # Forward pass (training mode)\n  cache_train = transformer_forward(X_batch_train, y_batch_train, params, dropout_rate, is_training = TRUE)\n  loss_train = cache_train$loss\n\n  # Backward pass\n  grads = transformer_backward(cache_train, X_batch_train, y_batch_train, params, dropout_rate)\n\n  # Apply Gradient Clipping\n  clipped_grads = clip_gradients_by_norm(grads, gradient_clip_threshold)\n  # clipped_grads = grads # or not\n\n  # Update parameters using clipped gradients\n  for (p_name in names(params)) {\n    # Check if a gradient for this parameter exists in clipped_grads\n    grad_name = paste0(\"d_\", p_name)\n    if (!is.null(clipped_grads[[grad_name]])) {\n      params[[p_name]] = params[[p_name]] - learning_rate * clipped_grads[[grad_name]]\n    } else {\n        # This case should ideally not be hit if `grads` contains all corresponding `d_` parameters\n        # but included for robustness if `params` has entries not tracked by `grads`\n        cat(\"Warning: No gradient found for parameter:\", p_name, \"\\n\")\n    }\n  }\n\n  # --- Validation Step ---\n  if (iter %% 10000 == 0) {\n    # Calculate validation loss (inference mode, no dropout)\n    val_losses = c()\n    if (nrow(X_val) > 0) { # Check if validation set is not empty\n      for (val_idx in 1:nrow(X_val)) {\n        X_batch_val = matrix(X_val[val_idx, ], nrow = 1)\n        y_batch_val = matrix(y_val[val_idx, ], nrow = 1)\n        cache_val = transformer_forward(X_batch_val, y_batch_val, params, dropout_rate, is_training = FALSE)\n        val_losses = c(val_losses, cache_val$loss)\n      }\n      avg_val_loss = mean(val_losses)\n      cat(\"Iteration:\", iter, \" Training Loss:\", round(loss_train, 4), \" Validation Loss:\", round(avg_val_loss, 4), \"\\n\")\n    } else {\n      cat(\"Iteration:\", iter, \" Training Loss:\", round(loss_train, 4), \" (No validation data)\\n\")\n    }\n  }\n}\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nIteration: 10000  Training Loss: 3.6363  Validation Loss: 3.1276 \nIteration: 20000  Training Loss: 3.2004  Validation Loss: 3.0208 \nIteration: 30000  Training Loss: 2.7591  Validation Loss: 2.9624 \nIteration: 40000  Training Loss: 2.0192  Validation Loss: 2.906 \nIteration: 50000  Training Loss: 3.4338  Validation Loss: 2.8703 \nIteration: 60000  Training Loss: 3.1453  Validation Loss: 2.8311 \nIteration: 70000  Training Loss: 3.7388  Validation Loss: 2.8091 \nIteration: 80000  Training Loss: 2.8175  Validation Loss: 2.7793 \nIteration: 90000  Training Loss: 3.1989  Validation Loss: 2.7818 \nIteration: 100000  Training Loss: 2.6022  Validation Loss: 2.7582 \n```\n:::\n\n```{.r .cell-code}\ncat(\"\\nTraining complete. Final Training Loss:\", round(loss_train, 4), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nTraining complete. Final Training Loss: 2.6022 \n```\n:::\n\n```{.r .cell-code}\nif (nrow(X_val) > 0) {\n  cat(\"Final Average Validation Loss:\", round(avg_val_loss, 4), \"\\n\")\n}\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nFinal Average Validation Loss: 2.7582 \n```\n:::\n:::\n\n\n## Generating New Text\n\nThe text generation function will run in inference mode (is_training = FALSE) to ensure no dropout is applied. It can be fed a starting sequence, or will pad with blank spaces if the sequence is shorter than expected or empty. It generates output similar to what it is trained on, so even if the starting sequence is very different from the training content, the output will still look like the training content. In this case, the model was trained on Shakespeare, so the output will look like Shakespeare even if the generation is seeded with text that looks very different from Shakespeare.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngenerate_text = function(current_params, start_string, num_characters_to_generate) {\n  generated_sequence_indices = encode(start_string)\n\n  if (length(generated_sequence_indices) < block_size) {\n    padded_start = c(rep(stoi[[\" \"]], block_size - length(generated_sequence_indices)), generated_sequence_indices)\n    generated_sequence_indices = padded_start\n    cat(\"Padded start string to block_size with spaces: '\", decode(padded_start), \"'\\n\", sep = \"\")\n  }\n\n  for (i in 1:num_characters_to_generate) {\n    context_indices = tail(generated_sequence_indices, block_size)\n    X_predict = matrix(context_indices, nrow = 1)\n\n    # Simplified forward pass for inference (is_training = FALSE)\n    x_emb_infer = t(sapply(1:block_size, function(j) current_params$C[X_predict[1, j], ]))\n    x_infer = x_emb_infer + current_params$P_emb\n\n    # No dropout applied during inference (dropout_forward handles this)\n    x_infer = dropout_forward(x_infer, dropout_rate, is_training = FALSE)$out\n\n    attn_out_infer = multi_head_attention_forward(x_infer, current_params$Wq, current_params$Wk, current_params$Wv, current_params$Wo,\n                                                 n_head, head_size, current_params$ln1_gamma, current_params$ln1_beta,\n                                                 dropout_rate, is_training = FALSE)$out\n    x_infer = attn_out_infer\n\n    ff_out_infer = feed_forward_forward(x_infer, current_params$W_ff1, current_params$b_ff1, current_params$W_ff2, current_params$b_ff2,\n                                       current_params$ln2_gamma, current_params$ln2_beta,\n                                       dropout_rate, is_training = FALSE)$out\n    x_infer = ff_out_infer\n\n    # Final linear layer to get logits for the *last* token in the sequence\n    last_token_output_infer = matrix(x_infer[block_size, ], nrow = 1)\n    logits_infer = last_token_output_infer %*% current_params$W_final + matrix(current_params$b_final, nrow = 1, ncol = vocab_size)\n\n    probs_infer = softmax_matrix_rows(logits_infer)\n\n    next_char_idx = sample.int(vocab_size, 1, prob = probs_infer)\n\n    generated_sequence_indices = c(generated_sequence_indices, next_char_idx)\n  }\n  return(decode(generated_sequence_indices))\n}\n\n# Example usage: Generate new characters starting with a given string\nstart_seq = \"Charizard is my favorite Pokemon.\"\nnum_to_generate = 1000\n\ngenerated_text = generate_text(params, start_seq, num_to_generate)\ncat(\"\\nGenerated text starting with '\", start_seq, \"':\\n\", generated_text, \"\\n\", sep = \"\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nGenerated text starting with 'Charizard is my favorite Pokemon.':\nCharizard is my favorite Pokemon.B\nS\nAhed,\nOshe un  bekoy tirde n min, w m s ta ktwicorrfandon, tal uorlee\nN\n\nT\nWls tiitind vsiw R\nBe  f girun y ifrovh woln\nLe w\nF tiuns n cthen ins, geou tonrda nd congr hen mpe-rle aromisono, telsd\nAs adinen sseq wGicwp AhRO\nRAe'.\nEite,\nThaloncis i'f- tind rgre tht theedis vrrtas, g ld ether,\nFinl'd\nUAh:Sli?AhnE\nSArah\nHHck T a,,\nWirw ale.od alr id r wRI-eatien'tathethyinrir,anvuthy,\n\nWil bdo an fe m.\n\nAig,\nN:\nAa!Nloue bu ounacsin frlaag.dg, apeitconeoomeyertaond,\nHeobellup sow? thasivr memongoosy kld onDimcy wur tthot n ead lvecon d-,\nYUd\nWOJn dgouin moe.UdAhoITiit ulvin wesir cper nod.\nSGiv\nTy s y I eEQI geth kranye irorldetVOoathye'ot th ns aonofras tes cathpen,:\nAif.\nCBs iulokega tou vShoylryheyerrertino, che capyy t. douus,\n\nYss ,\nG p nndendev\nQouerego kirourehet fy fe do srhe ri sas, wnduldeaali y bloiy g souronths, le.\nGos IAuthe, g net:\n\nIUSG!asot'ril bl,, IB wiworaf ple pesove ewe uran intes winof; b'roningl kesrk,.\n\n\nOingerodom? bisdthilan t:unl vAee\n\nFe kd:\nIAare IOeae whou\n```\n:::\n:::\n\n\nWe can see that while the language is not correct, the model is producing text structured very similarly to the input Shakespeare text. The validation loss achieved here is worse than what Karpathy achieves in his implementation, but this is due to the choice to simplify several aspects of the model (batch size of 1, only a single transformer block instead of 3 or 4 in sequence) to make it easier to implement everything from scratch and keep the matrices 2-dimensional for easier manual inspection in RStudio's global environment.\n\nThere are a number of improvements than could be made to this implementation. The most obvious ones are having a batch size greater than 1, and including multiple transformer blocks in sequence instead of just 1. Optimizing the weighting scheme in the gradient update to decay the learning rate or use some other algorithm like the Adam optimizer would also be a good choice. The reason this output doesn't look as impressive as ChatGPT comes down to the complexity of the model. In this implementation, the count of final model parameters is 5489, which is very small in comparison to the count of 1.5 billion parameters in GPT-2 or 175 billion parameters in GPT-3.\n\n## Attention Plot\n\nAfter training the model, there are a number of interesting visualizations we can create. One is an attention plot, where we pick an input sequence and show how the attention layer creates weights to pay attention to prior tokens over that sequence. We can examine the input sequence \"methinks\" which is a word that exists commonly in the input Shakespeare text. At this point, we will call a couple of libraries just for the plotting functionality.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyr)\nlibrary(ggplot2)\nlibrary(gridExtra)\n\nattn_sequence = encode(\"methinks\")\nX_predict = matrix(attn_sequence, nrow = 1)\n\n# Token and positional embedding\nx_emb_infer = t(sapply(1:block_size, function(j) params$C[X_predict[1, j], ]))\nx_infer = x_emb_infer + params$P_emb\n\n# Layer normalization\nln_out_x = layer_norm_forward(x_infer, params$ln1_gamma, params$ln1_beta)\nx = ln_out_x$out\n\n# Attention block\nQ_all = x %*% Wq\nK_all = x %*% Wk\n\nplots_per_head = lapply(1:n_head, function(h) {\n  start_col = (h - 1) * head_size + 1\n  end_col = h * head_size\n  Q_h = Q_all[, start_col:end_col]\n  K_h = K_all[, start_col:end_col]\n\n  scores = (Q_h %*% t(K_h)) / sqrt(head_size)\n\n  mask_causal = upper.tri(matrix(0, nrow(scores), ncol(scores)), diag = FALSE)\n  scores[mask_causal] = -Inf\n\n  attention_weights = softmax_matrix_rows(scores)\n  \n  # make plot\n  df = as.data.frame(attention_weights)\n  colnames(df) = c(\"m\",\"e\",\"t\",\"h\",\"i\",\"n\",\"k\",\"s\")\n  df$row = c(\"m\",\"e\",\"t\",\"h\",\"i\",\"n\",\"k\",\"s\")\n  df = tidyr::pivot_longer(df, -row, names_to = \"col\", values_to = \"val\")\n  df$row = factor(df$row, levels = rev(c(\"m\",\"e\",\"t\",\"h\",\"i\",\"n\",\"k\",\"s\")))\n  df$col = factor(df$col, levels = c(\"m\",\"e\",\"t\",\"h\",\"i\",\"n\",\"k\",\"s\"))\n  p = ggplot(df, aes(x = col, y = row, fill = val)) +\n      geom_tile() +\n      scale_fill_viridis_c() +\n      labs(x = \"prior tokens in sequence\", y = \"input token\",\n           fill = \"weights\", title = paste0(\"Attention Head #\", h)) +\n      theme(plot.title = element_text(size = 11))\n  \n  # Return plot object to list output\n  return(p)\n  \n})\n\n# Display plots in 2x2 grid\ngrid.arrange(grobs = plots_per_head, ncol = 2)\n```\n\n::: {.cell-output-display}\n![](llm_transformer_from_scratch_files/figure-html/attention-plot-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# Save plot for thumbnail\ng = arrangeGrob(grobs = plots_per_head, ncol = 2)\nggsave(file = \"thumbnail.jpg\", g, width = 6, height = 6/1.618)\n```\n:::\n\n\nEach of the four attention heads generates different weights during the self-attention process. The first token always gets a 100% weight since there are no other tokens at that point, but for later tokens we can see that the model has learned to pay attention (give higher weights to) tokens that are further back in the sequence, and which prior tokens are highly weighted differs from one attention head to the next, so they are providing meaningfully distinct information.\n\n## Conclusion\n\nWhile implementing everything from scratch without relying on any packages adds significant complexity to this model, it offers a detailed understanding of the core transformer architecture. The process of implementing some of this core functionality led to a deeper understanding of how and why certain calculations are what they are (See derivations in earlier sections regarding He initialization and Layer Normalization). This example serves as an educational tool for delving into the fundamental mechanisms of attention, normalization, and regularization within deep learning models, all without external dependencies.\n\n\n",
    "supporting": [
      "llm_transformer_from_scratch_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}