{
  "hash": "f7e26cd388a8d37122e76492945bca20",
  "result": {
    "markdown": "---\ntitle: \"California Fire Evacuations\"\ndescription: \"A proof of concept for the American Red Cross about making data driven predictions of shelter demand related to California wildfire evacuation orders\"\ntitle-block-banner-color: white\nimage: \"thumbnail.jpg\"\ndraft: false\n---\n\n\n## Intro/Overview\n\nOne of the volunteer projects I worked on for the American Red Cross (ARC) was for a chapter in northern California which wanted to improve predictions for shelter demand due to fire-related evacuations. When a wildfire happens, the local authorities may issue an evacuation order to tell people in the affected area to evacuate.\n\nARC helps by activating and staffing shelters near the affected area so that people who have to flee from the fire have a place to spend the night. These shelters are usually places like stadiums or schools with gymnasiums which have agreements with ARC to use their space as a temporary shelter in emergency situations. A volunteer from ARC will go to the shelter location to make sure it is open and ready to receive people in need.\n\nARC needs to decide which potential shelter locations to activate for any given evacuation order. There are a lot of factors that go into making this decision, but one of them is an estimate of demand for shelter. How many people will actually want to make use of the shelter that ARC will provide?\n\n## Setting Up\n\n### Loading Libraries\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tigris) # to download CA shapefile from the Census\nlibrary(jsonlite) # to parse responses from the OpenFEMA API\nlibrary(sf) # for manipulating shapefile geometry information\nlibrary(ggplot2) # to make plots\nlibrary(tidycensus) # to get US Census data\nlibrary(dplyr) # for data manipulation\nlibrary(tidyr) # for data manipulation\n```\n:::\n\n\n## Getting the Data\n\nThe first thing I needed to do is find data related to the problem. For data about evacuation orders related to fires in CA, the [OpenFEMA datasets](https://www.fema.gov/about/openfema/data-sets) looked like a good option. In particular, the Integrated Public Alert and Warning System (IPAWS) has an [API](https://www.fema.gov/openfema-data-page/ipaws-archived-alerts-v1) which lets users query historical alert data. This is useful for this problem about CA fires because many counties in CA began adopting IPAWS to distribute their alerts starting around 2013, so it should contain data about the evacuation alerts of interest.\n\nAnother data source is the US Census. The evacuation order has information about what geographic region is affected, but no information about the people in that region or their demographics. That information will have to come from the census.\n\n### Get Data from the OpenFEMA API\n\nThe IPAWS API lets users query based on certain available fields as listed in their documentation. However, it is a little bit tricky to get what I'm looking for because there is no \"state\" field to use for filtering so I cannot ask for results where \"state=CA\". Instead the user can pass in a geometry in the shape of a polygon as a search area, so it is necessary for me to first represent CA as a polygon in Well Known Text (WKT) format to pass to the API as a filter.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# get CA shapefile from the census\nca = tigris::states(cb = TRUE, resolution = \"20m\")\nca = ca[ca$NAME == \"California\",]\n\n# my approximate guess for longitude and latitude\n# for a polygon covering CA\nca_approx = data.frame(\n    lon = c(-124.2196,-119.9818,-119.9922,-113.9484,-114.3979,-117.2380,-120.1382,-124.5953),\n    lat = c(42.0019,42.0089,39.0026,34.6875,32.6730,32.4818,33.0436,40.3261)\n)\n\n# same as above, just transposed in format\n# to align with what the API expects\n# POLYGON((-124.2196 42.0019,-119.9818 42.0089,-119.9922 39.0026,-113.9498 34.6875,-114.3979 32.6730,-117.2380 32.4818,-120.1382 33.0436,-124.5953 40.3261))\n\n# verify coverage with plot\nggplot() +\n    geom_sf(data = ca) +\n    geom_polygon(aes(x = lon, y = lat), data = ca_approx,\n                 color = \"#F8766D\", fill = \"#F8766D\", alpha = 0.3) +\n    theme(axis.title = element_blank())\n```\n:::\n\n\n![](ca_approx.jpg)\n\nMy simple polygon covers California as desired. It is okay that it covers some space beyond California because I am only using it to filter results from IPAWS. It may pick up a few cases outside of California, but these will be eliminated later when I merge the IPAWS data with the US Census data since they will not match up with any CA census geometries.\n\nNow I can start making requests from the IPAWS API. I will skip all the trial and error I had to go through to get it to work and just show the final code that worked for me. Comments below in the relevant sections discuss some of these decisions.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# request to OpenFEMA API\n# give back status='Actual\" (as opposed to test alerts)\n# and state=CA (geo polygon information interects my approximated CA polygon)\napi_query_base = \"https://www.fema.gov/api/open/v1/IpawsArchivedAlerts?$filter=status eq 'Actual' and geo.intersects(searchGeometry, geography 'POLYGON((-124.2196 42.0019,-119.9818 42.0089,-119.9922 39.0026,-113.9498 34.6875,-114.3979 32.6730,-117.2380 32.4818,-120.1382 33.0436,-124.5953 40.3261))') and sent gt 'yyyy-01-01T00:00:01.000z' and sent lt 'yyyy-12-31T23:59:59.000z'&$orderby=sent&$inlinecount=allpages&$skip=\"\n\n# OpenFEMA API returns 1000 records max\n# so need to page through sets of 1000\n# https://www.fema.gov/about/openfema/api\nresults_df = data.frame()\nresults_poly = list()\n\n# IPAWS data is available starting in June 2012\n# I am querying one year of data at a time\n# because I keep running into errors where the API\n# doesn't want to return results when the page count\n# gets too high\nfor(year in 2012:2020){\n    \n    # set up counters to page through data\n    last_page = FALSE\n    skip_records = 0\n    \n    while(!last_page){\n        # query API for up to 1000 records\n        api_query = paste0(api_query_base, skip_records)\n        api_query = gsub(\"yyyy\", year, api_query)\n        json_result = readLines(api_query)\n        parsed_list = fromJSON(json_result)\n        n_records = nrow(parsed_list[[2]])\n        \n        # is this the last page of results?\n        # if so end loop\n        if(n_records < 1000){ last_page = TRUE }\n        \n        # convert results to single data frame\n        # plus list of polygons\n        for(i in 1:n_records){\n            # get metadata for each alert\n            df = data.frame(\n                sent = parsed_list[[2]][i,]$sent,\n                status = parsed_list[[2]][i,]$status,\n                identifier = parsed_list[[2]][i,]$identifier,\n                category = paste0(parsed_list[[2]][i,]$info[[1]]$category[[1]], collapse = \", \"),\n                areaDesc = parsed_list[[2]][i,]$info[[1]]$area[[1]]$areaDesc,\n                responseType = paste0(parsed_list[[2]][i,]$info[[1]]$responseType[[1]], collapse = \", \"),\n                urgency = paste0(unique(parsed_list[[2]][i,]$info[[1]]$urgency), collapse = \", \"),\n                severity = paste0(unique(parsed_list[[2]][i,]$info[[1]]$severity), collapse = \", \"),\n                certainty = paste0(unique(parsed_list[[2]][i,]$info[[1]]$certainty), collapse = \", \"),\n                effective = ifelse(is.null(parsed_list[[2]][i,]$info[[1]]$effective), \"\", paste0(unique(parsed_list[[2]][i,]$info[[1]]$effective), collapse = \", \")),\n                onset = ifelse(is.null(parsed_list[[2]][i,]$info[[1]]$onset), \"\", paste0(unique(parsed_list[[2]][i,]$info[[1]]$onset), collapse = \", \")),\n                expires = ifelse(is.null(parsed_list[[2]][i,]$info[[1]]$expires), \"\", paste0(unique(parsed_list[[2]][i,]$info[[1]]$expires), collapse = \", \")),\n                description = ifelse(is.null(parsed_list[[2]][i,]$info[[1]]$description), \"\", paste0(unique(parsed_list[[2]][i,]$info[[1]]$description), collapse = \", \")),\n                instruction = ifelse(is.null(parsed_list[[2]][i,]$info[[1]]$instruction), \"\", paste0(unique(parsed_list[[2]][i,]$info[[1]]$instruction), collapse = \", \"))\n            )\n            \n            # sometimes evacuation orders for fires might not be coded as category='Fire'\n            # or as responseType='Evacuate' so filter more broadly\n            if((df$category != \"Met\") & (grepl(\"Evacuate\",df$responseType) | (grepl(\"fire\",df$description,ignore.case = TRUE) & grepl(\"evacuat\",df$description,ignore.case = TRUE)) | (grepl(\"fire\",df$instruction,ignore.case = TRUE) & grepl(\"evacuat\",df$instruction,ignore.case = TRUE)))){\n                results_df = rbind(results_df, df)\n                results_poly[[length(results_poly)+1]] = parsed_list[[2]][i,]$info[[1]]$area[[1]]$polygon$coordinates\n            }\n            \n        }\n        # print out progress\n        print(paste0(\"Year: \", year, \", Records Processed: \", skip_records + n_records))\n        # increment page skip count\n        skip_records = skip_records + 1000\n    }\n}\n\n# reformat polygon matrix results to WKT text fields\n# so they can be represented in a column vector\npoly_vec = do.call(\"c\", lapply(results_poly, function(x){\n    paste0(apply(x[[1]], 2, function(y){paste0(y,collapse = \" \")}), collapse = \",\")\n}))\nresults_df$areaPolygon = sub(\"^[^,]*,\", \"\", poly_vec)\nresults_df$areaPolygon = paste0(\"POLYGON((\", poly_vec, \"))\")\n```\n:::\n\n\n### Get Data from the US Census\n\nThere is a lot of demographic information available via the US Census API, so it helps to have some idea of what to get in advance. In my conversations with ARC, they said age and income are the ones they would consider most important. People who are very old are less likely to leave their home in the event of an evacuation order and consequently less likely to seek shelter from ARC. Similarly, people with higher income may have a second home they can travel to or choose to find paid accommodations with a higher level of comfort than the free but sparse shelter option that ARC is providing.\n\nI found this information in the American Community Survey (ACS) 5-year data. Here is a [link](https://www.census.gov/data/developers/data-sets/acs-5year.html) to the census page for this data source. The 5-year data is great because it has information at the block-group level, which is a really low level geography. Smaller geographic units are better because they should allow for closer alignment with the arbitrary areas of the evacuation orders than larger units which may have large areas that are not in the region of interest.\n\nThe data dictionary is large and hard to wade through. I found what I was looking for under the variable B19037, which provides what is basically the joint distribution of age and income. However, it is not easy to separate age and income from the way the data is given, so I created the mapping manually since the number of cases is small enough and this is the only data I needed.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# read in the mapping I created manually in a csv file\nB19037_mapping = read.csv(\"census_B19037_definitions.csv\")\n\n# display mapping table\nknitr::kable(head(B19037_mapping))\n```\n\n::: {.cell-output-display}\n|variable   |age_householder |income  |\n|:----------|:---------------|:-------|\n|B19037_003 |0_24            |0_10K   |\n|B19037_004 |0_24            |10K_15K |\n|B19037_005 |0_24            |15K_20K |\n|B19037_006 |0_24            |20K_25K |\n|B19037_007 |0_24            |25K_30K |\n|B19037_008 |0_24            |30K_35K |\n:::\n:::\n\n\nUsing the census API requires the user to have an API key. This is a really easy thing to apply for and should only take a couple minutes to both apply for it and receive it. Here is the page to apply: <https://api.census.gov/data/key_signup.html>\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# store census API key for use in query\n# using tidycensus package functions\nCENSUS_API_KEY = \"YOUR API KEY HERE\"\ncensus_api_key(CENSUS_API_KEY)\n\n# download the data\nacs5_B19307 = get_acs(geography = \"block group\",\n                      variables = B19307_mapping$variable,\n                      year = 2019,\n                      state = \"CA\",\n                      geometry = TRUE)\n```\n:::\n\n\n## Data Transformations\n\nHaving the joint distribution by default is great, but it wouldn't hurt to decompose age and income into their marginal distributions too. I might as well do that now to prepare for later stages where I might want to use them independently.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# compute marginal distribution for age\ndf_age = as.data.frame(acs5_B19307) %>%\n    merge(B19307_mapping, by = \"variable\", all.x = TRUE) %>%\n    mutate(age_householder = paste0(\"age_\", age_householder)) %>%\n    group_by(GEOID, age_householder) %>%\n    summarise(age_count = sum(estimate)) %>%\n    ungroup() %>%\n    pivot_wider(names_from = age_householder, values_from = age_count)\n\n# compute marginal distribution for income\ndf_income = as.data.frame(acs5_B19307) %>%\n    merge(B19307_mapping, by = \"variable\", all.x = TRUE) %>%\n    group_by(GEOID, income) %>%\n    summarise(income_count = sum(estimate)) %>%\n    ungroup() %>%\n    pivot_wider(names_from = income, values_from = income_count)\n\n# having trouble pivoting original data\n# in the presence of the geometry feature\n# so remove the geometry and do it separately here\ndf_joint = as.data.frame(acs5_B19307) %>%\n    select(GEOID, variable, estimate) %>%\n    pivot_wider(names_from = variable, values_from = estimate)\n\n# join marginal distributions into original data\ndf_acs5 = acs5_B19307 %>%\n    select(GEOID, geometry) %>%\n    filter(!duplicated(GEOID)) %>%\n    inner_join(df_age, by = \"GEOID\") %>%\n    inner_join(df_income, by = \"GEOID\") %>%\n    inner_join(df_joint, by = \"GEOID\") %>%\n    mutate(block_group_area = st_area(.))\n```\n:::\n\n\n### Combining the Data\n\nNow that I have data from IPAWS on fire-related evacuation orders in CA, and data on population counts by age and income in CA at the census block group level, I would like to combine that information. This can be done by checking for overlaps between the geometries of the two data sources.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# put IPAWS alert data in a form the sf package likes\n# specifying where to find the geometry information\n# and in what form (WKT)\ndf_alerts = st_as_sf(results_df, wkt = \"areaPolygon\")\nst_crs(df_alerts) = st_crs(df_acs5)\n\n# start by computing just the first case as an example\ndf_intersections = df_alerts[1,] %>%\n    select(identifier, areaPolygon) %>%\n    st_intersection(df_acs5) %>%\n    mutate(intersect_area = st_area(.)) %>%\n    mutate(pct_overlap = as.numeric(intersect_area/block_group_area)) %>%\n    st_drop_geometry()\n\n# plot the census block groups and the\n# evacuation area to show the overlap\nggplot() + \n    geom_sf(data=df_acs5[df_acs5$GEOID %in% df_intersections$GEOID,]) +\n    geom_sf(data=df_alerts[1,],color = \"#F8766D\", fill = \"#F8766D\", alpha = 0.3)\n```\n:::\n\n\n![](evac_example1.jpg)\n\nEven though the number of evacuation orders is small (just a couple hundred) the time it takes to compute all the intersections and areas is several hours.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfor(i in 2:nrow(df_alerts)){\n    # compute for one evacuation order at a time\n    temp = df_alerts[i,] %>%\n        select(identifier, areaPolygon) %>%\n        st_intersection(df_acs5) %>%\n        mutate(intersect_area = st_area(.)) %>%\n        mutate(pct_overlap = as.numeric(intersect_area/block_group_area))  %>%\n        st_drop_geometry()\n    \n    # merge into larger data frame\n    df_intersections = rbind(df_intersections, temp)\n}\n```\n:::\n\n\n### Aggregate by Evacuation Event\n\nThis combined data shows how the area covered by the evacuation order overlaps with the defined census block groups. This data needs to be aggregated at the level of evacuation orders so that one evacuation order has one set of corresponding variables. Here are a couple of simple options.\n\n1.  Weight the census data by the percentage of the area that overlaps with the evacuation order area. If there is more overlap, more of the data from that census block group will be used. This sounds logical, but it assumes that the population of individuals is uniformly distributed throughout that block group, which does not have to be true.\n2.  Give 100% weight to all census block groups that intersect with the evacuation order area. This ensures no individuals are incorrectly excluded if the population of a block group happens to be concentrated in a small area within that block group. However, it may lead to overestimating the relevant population for the evacuation order.\n\nSince the right choice is not immediately obvious, I will just group the data both ways and figure out which way is preferable at a later stage.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# option 1: census features are weighted average\n# based on the intersection of their block group area\n# with the evacuation order area\ndf_intersections_avg = df_intersections %>%\n    select(-GEOID) %>%\n    group_by(identifier) %>%\n    summarise_all(~ sum(.x * pct_overlap)) %>%\n    select(-pct_overlap) %>%\n    inner_join(results_df[,c(\"identifier\",\"sent\",\"areaDesc\")], by = \"identifier\")\n\n# option 2: census features are summed together\n# (given 100% weight) as long as their block group area\n# has any overlap with the evacuation order area\ndf_intersections_sum = df_intersections %>%\n    select(-GEOID, -pct_overlap) %>%\n    group_by(identifier) %>%\n    summarise_all(~ sum(.x)) %>%\n    inner_join(results_df[,c(\"identifier\",\"sent\",\"areaDesc\")], by = \"identifier\")\n```\n:::\n\n\n\n\n## Predicting Shelter Demand\n\nThe next step is to use the data gathered around each evacuation event to predict how many people will seek shelter from ARC. This has to be kept very general because I do not have access to the data required to make meaningful progress beyond this point.\n\n### Get Data on ARC Shelter Demand\n\nARC should have some basic data about the number of people who took shelter at ARC shelters during historical wildfire evacuation events. As a volunteer working on this proof of concept, this data was not made available to me at this stage of the analysis. As such, I have to make up some fake data to use here.\n\nGenerating fake data about shelter demand means none of the following results are meaningful and certain decisions about the analysis cannot be made here. For example, I discussed above two options for how data could be aggregated for unique evacuation events and the way to pick which method to use would be to see which produced the better fitting model. Since I don't have real data, I cannot evaluate which method will be better, so I will arbitrarily pick the second method just to have some data to use here.\n\nI will assume the number of people who seek shelter is drawn from a random Poisson distribution (so that it is always an integer number of people), and is about 10% of the impacted population.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# calculate total population from age variables\nn_events = nrow(df_intersections_sum)\nevent_pop = apply(df_intersections_sum[,grepl(\"age\",colnames(df_intersections_sum))], 1, sum)\n\n# add affected population and random poisson shelter demand\ndf_intersections_sum$event_pop = event_pop\ndf_intersections_sum$shelter_demand = rpois(n_events, lambda = 0.1*event_pop)\n```\n:::\n\n\nThis also skips several other complications that are likely to arise with the actual data, such as figuring out how to match the shelter data with the right evacuation event.\n\n### A Simple Model\n\nMy first instinct when thinking about this problem is to try a Poisson regression. Since the number of people seeking shelter is count data, the Poisson distribution is convenient for modeling it because it will handle cases where the count is zero if that occurs (no one showed up at the shelter) and will enforce non-negativity.\n\nIt may be useful to think about this problem in the context of fractions, as in \"what fraction of the exposed population will seek shelter?\" This is also something easily incorporated within the Poisson regression framework.\n\nThe basic Poisson regression:\n\n$$\n    log(E(Y|x)) = Bx\n$$ Poisson regression modeling counts as a fraction of exposed population:\n\n$$\n    log(\\frac{E(Y|x)}{exposure}) = log(E(Y|x)) - log(exposure) = Bx - log(exposure)\n$$\n\nI will choose the second option just to showcase how the implementation works. I will start with some data transformations, scaling the features by the exposed population so that they are fractions of the total population rather than counts. This normalizes them across observations. I will just use the marginal age variables here. In practice I would want to include the income variables and then also test whether the interaction variables from the joint distribution are useful, but none of that matters with the fake data I have right now. Then I can fit the model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# build modeling dataset\n# start with just the age categories\nmodel_data = df_intersections_sum[,grepl(\"age\",colnames(df_intersections_sum))]\n\n# normalize by the exposed population in each event\nmodel_data = sweep(model_data, 1, 1/event_pop, FUN=\"*\")\n\n# drop the first variable to avoid perfect multicollinearity\n# since they will all add up to 1 otherwise\nmodel_data = model_data[,-1]\n\n# add the target variable and exposure variable\nmodel_data$y = df_intersections_sum$shelter_demand\nmodel_data$exposure = event_pop\n\n# fit the model\nfit = glm(y ~ . - exposure + offset(log(exposure)), data = model_data, family=poisson(link=log))\nsummary(fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nglm(formula = y ~ . - exposure + offset(log(exposure)), family = poisson(link = log), \n    data = model_data)\n\nCoefficients:\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept)  -2.1469     0.1275 -16.844   <2e-16 ***\nage_25_44    -0.1838     0.1637  -1.123    0.261    \nage_45_64    -0.1070     0.1116  -0.959    0.338    \nage_65_inf   -0.1886     0.1356  -1.391    0.164    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 195.42  on 247  degrees of freedom\nResidual deviance: 192.31  on 244  degrees of freedom\nAIC: 2031.1\n\nNumber of Fisher Scoring iterations: 3\n```\n:::\n:::\n\n\nThe only part of the model that looks significant is the intercept, which is correct because I generated shelter demand as a constant 10% of the exposed population. In the real data, I might expect to see exposed populations made up of younger people or having lower incomes significantly affect shelter demand.\n\n### Prediction Intervals\n\nThe Poisson regression I just constructed models expected shelter demand. In other words, the average number of people expected to seek shelter. The estimate produced by the model may over-predict or under-predict. ARC may care more about under-predicting than over-predicting. Depending on the resources available, ARC may prefer to over-predict demand and send more people to open unnecessary shelters as opposed to under-predicting demand and having to turn away people seeking shelter or not have shelter available for them.\n\nOne way to address this within the context of this model is a prediction interval. Unlike a confidence interval which only incorporates uncertainty about model parameter estimates, a prediction interval also incorporates uncertainty about the outcomes. Building a 95% prediction interval means that we would expect that the observed outcome falls within the interval 95% of the time. Using the upper bound of that interval may be a good way to estimate an upper limit of what shelter demand could be for a particular scenario.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# what percentage of model predictions\n# under-estimate shelter demand?\nmean(model_data$y - predict(fit, type = \"response\") > 0)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.5282258\n```\n:::\n\n```{.r .cell-code}\n# bootstrap setup\nset.seed(42)\nn_bootstraps = 1000\nboot_results = matrix(0, nrow = nrow(model_data), ncol = n_bootstraps)\n\n# bootstrap prediction interval\nfor(i in 1:n_bootstraps){\n    # re-sample from modeling data with replacement\n    boot_idx = sample(nrow(model_data), replace = TRUE)\n    df_boot = model_data[boot_idx,]\n    # fit the model with bootstrapped data\n    fit_boot = glm(y ~ . - exposure + offset(log(exposure)), \n                   data = df_boot, family=poisson(link=log))\n    # predictions of actual data using bootstrapped model\n    # distributions from this step would give the confidence interval\n    y_pred_boot = predict(fit_boot, newdata = model_data, type = \"response\")\n    # now need to account for residual variance\n    # given that the data is assumed to be poisson distributed\n    # the prediction is the expected value or lambda\n    # so we can just sample from a poisson distribution with that lambda\n    boot_results[,i] = rpois(nrow(model_data), lambda = y_pred_boot)\n}\n\n# 95% prediction interval\nlower_bound = apply(boot_results, 1, quantile, probs = 0.025)\nupper_bound = apply(boot_results, 1, quantile, probs = 0.975)\n\n# what percentage of the time does\n# the upper bound of the 95% prediction interval\n# under-estimate shelter demand?\nmean(model_data$y - upper_bound > 0)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.01612903\n```\n:::\n:::\n\n\n## Conclusion\n\nI have shown a proposed approach for acquiring data from FEMA and the US Census to support making data-driven estimates of shelter demand during wildfire evacuations in California. While the full analysis is incomplete without the actual shelter data from ARC, I laid out some initial possibilities based on the information available.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}