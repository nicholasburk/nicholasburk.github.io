{
  "hash": "2b8c53ea96f1ab50d700532d4b2c69d8",
  "result": {
    "markdown": "---\ntitle: \"PR-AUC Optimization\"\ndescription: \"Estimating a logistic regression model using a surrogate for the area under the precision-recall curve (PR-AUC) as the loss function\"\ntitle-block-banner-color: white\nimage: \"thumbnail.jpg\"\ndraft: false\n---\n\n\n## Intro/Overview\n\nIn binary classification tasks, the area under the precision-recall curve (PR-AUC) is a popular metric for use cases that deal with imbalanced data. For example, when trying to predict rare events such as credit defaults, hazard occurrences, correct top search results, etc. \n\nA common approach is to estimate a model using logistic regression and then choose from a few candidate models based on their performance on metrics such as PR-AUC. While there is nothing stopping the user from doing this, logistic regression optimizes its coefficients based on minimizing the logistic loss function, not maximizing PR-AUC. If the objective of the problem is to maximize PR-AUC, optimizing the estimation approach to maximize this quantity directly should provide better results than optimizing a completely different metric.\n\nThe trouble with doing this for PR-AUC (and ROC-AUC too for that matter) is that these metrics are defined using indicator functions $1[s_i \\gt s_j]$ which are not smooth and therefore not differentiable. One way around this is to use smooth surrogates to approximate these functions and make them differentiable. We will explore how this can be done as an example.\n\n## Setting Up\n\n### Loading Libraries\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(mlbench) # contains the breast cancer dataset for use as an example\nlibrary(PRROC) # to calculate the area under the precision-recall curve\nlibrary(pROC) # to calculate the area under the receiver operating characteristic curve\nlibrary(ggplot2) # to draw charts\n```\n:::\n\n\n### Loading Data\n\nFor this example we can use any data as long as we have a binary target variable. We will use the breast cancer dataset, which originates from the Breast Cancer Wisconsin (Diagnostic) Data Set, a widely used benchmark in machine learning for medical diagnostics. We want to build a classifier to predict whether the tumor type is benign or malignant. \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load the breast cancer dataset from the mlbench package\ndata(\"BreastCancer\")\n\n# Remove rows with missing values\nBC_clean <- na.omit(BreastCancer)\n\n# Convert ordered factors to character, then to numeric to preserve their order\n# since they are being fed to model.matrix later\nidx_col <- colnames(BC_clean) %in% c(\"Cl.thickness\", \"Cell.size\", \"Cell.shape\", \"Marg.adhesion\", \"Epith.c.size\")\nBC_clean[, idx_col] <- apply(BC_clean[, idx_col], 2, function(x){as.numeric(as.character(x))})\n\ny  <- ifelse(BC_clean$Class == \"malignant\", 1, 0)\nX0 <- model.matrix(Class ~ . -1 -Id, data = BC_clean)\nX  <- cbind(Intercept = 1, X0)      # add intercept\nn  <- nrow(X)\n\nif (length(unique(y)) < 2) stop(\"Need both classes present.\")\n```\n:::\n\n\n### Utility Functions\n\nDefining functions for the sigmoid (to use in our smooth surrogate), calculations for PR-AUC and ROC-AUC, as well as to help with stratified sampling.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsigmoid <- function(z) {1 / (1 + exp(-z))}\n\n# PR-AUC helper (higher score = more positive)\npr_auc <- function(scores, y) {\n  pos <- scores[y == 1]\n  neg <- scores[y == 0]\n  pr.curve(scores.class0 = pos, scores.class1 = neg, curve = FALSE)$auc.integral\n}\n\nroc_auc <- function(scores, y) {\n  as.numeric(pROC::auc(pROC::roc(response = y, predictor = scores, quiet = TRUE)))\n}\n\n# borrowed this from createDataPartition.R in the caret package\n# modified to remove dependency on 'dlply' function from plyr package\ncreateDataPartition <- function (y, times = 1, p = 0.5, list = TRUE, groups = min(5, length(y))){\n  out <- vector(mode = \"list\", times)\n\n  if(length(y) < 2) stop(\"y must have at least 2 data points\")\n\n  if(groups < 2) groups <- 2\n\n  if(is.numeric(y)) {\n    y <- cut(y,\n              unique(quantile(y, probs = seq(0, 1, length.out = groups))),\n              include.lowest = TRUE)\n  } else {\n    xtab <- table(y)\n    if(any(xtab == 0)) {\n      warning(paste(\"Some classes have no records (\",\n                      paste(names(xtab)[xtab  == 0], sep = \"\", collapse = \", \"),\n                      \") and these will be ignored\"))\n      y <- factor(as.character(y))\n    }\n    if(any(xtab == 1)) {\n      warning(paste(\"Some classes have a single record (\",\n                      paste(names(xtab)[xtab  == 1], sep = \"\", collapse = \", \"),\n                      \") and these will be selected for the sample\"))\n    }\n  }\n\n  subsample <- function(dat, p) {\n    if(nrow(dat) == 1) {\n      out <- dat$index\n    } else {\n      num <- ceiling(nrow(dat) * p)\n      out <- sample(dat$index, size = num)\n    }\n    out\n  }\n\n  prettySeq <- function(x) {\n    x <- as.character(x)\n    for(i in seq(along = x)) {\n      if(nchar(x[i]) == 1) {\n        x[i] <- paste(\"Rep\", x[i], sep = \"\")\n      } else {\n        x[i] <- paste(\"Rep\", x[i], sep = \"\")\n      }\n    }\n    x\n  }\n\n  for (j in 1:times) {\n    data_frame <- data.frame(y = y, index = seq(along.with = y))\n    \n    # Split the data frame by the 'y' column\n    split_data <- split(data_frame, data_frame$y)\n    \n    # Apply the subsample function to each split using lapply\n    tmp <- lapply(split_data, subsample, p = p)\n\n    tmp <- sort(as.vector(unlist(tmp)))\n    out[[j]] <- tmp\n  }\n\n  if (!list) {\n    out <- matrix(unlist(out), ncol = times)\n    colnames(out) <- prettySeq(1:ncol(out))\n  } else {\n    names(out) <- prettySeq(out)\n  }\n  out\n}\n```\n:::\n\n\n## Smooth Average Precision\n\nWe want to optimize PR-AUC, which is the area under the precision-recall curve. PR-AUC is defined as follows:\n\n$$\n\\text{PR-AUC} = \\int_0^1 \\text{Precision}(r) dr\n$$\n\nwhere $r$ is recall.\n\nThis is an expectation over all recall levels, i.e. it measures how many positives are captured at varying thresholds relative to false positives.\n\nGiven finite data, Average Precision (AP) is defined as:\n\n$$\n\\text{AP} = \\frac{1}{n_+} \\sum_{k=1}^{n_+} \\text{Precision}(k)\n$$\n\nwhere $n_+$ is the number of positive observations.\n\nThe sum runs over the ranked list, and at each position $k$, you evaluate the precision at the point where a positive is encountered. Intuitively, AP averages the precision achieved each time a new positive is recalled.\n\nPR-AUC integrates precision over recall, i.e. over all possible “fractions of positives recalled.” AP computes precision at discrete recall increments of $\\frac{1}{n_+}$ so AP is basically a Riemann sum estimator of PR-AUC with grid spacing of $\\frac{1}{n_+}$.\n\n$$\n\\text{PR-AUC} \\approx \\frac{1}{n_+} \\sum_{k=1}^{n_+} \\text{Precision}(\\frac{k}{n_+}) = \\text{AP}\n$$\n\n### Gradient Calculation\n\nOur objective function we want to maximize is AP, so we need to find the gradient by taking the derivative with respect to the model parameters. Our model takes the form of a logistic regression, so the model scores $s_i$ are $x_i^TB$.\n\nLet:\n\n$$\n\\begin{aligned}\nC_{ij} &= \\sigma(\\frac{s_j - s_i}{\\tau}) \\\\\nr_i &= 1 + \\sum_j C_{ij} \\\\\nr_i^+ &= 1 + \\sum_{j:y_j=1} C_{ij} \\\\\nprec_i &= \\frac{r_i^+}{r_i} \\\\\nAP &= \\frac{1}{n^+} \\sum_{i:y_i=1} prec_i\n\\end{aligned}\n$$\n\nWe need to apply the chain rule a few times here, but the way we wrote out the terms separately above helps make that easier to see. We will take the derivative with respect to the model score for an example data point, since it's easier to think about the pairwise comparisons that way, and the model parameters are straightforward after that. One helpful first step is to remember that the derivative of the sigmoid function is well known: $\\sigma'(x) = \\sigma(x)(1 - \\sigma(x))$. I'll spell out the derivation here anyway.\n\nSince $\\sigma(x) = \\frac{1}{1 + e^{-x}}$, by the quotient rule:\n\n$$\n\\begin{aligned}\n\\frac{d\\sigma}{dx} &= \\frac{(0)(1 + e^{-x}) - (1)(-e^{-x})}{(1 + e^{-x})^2} \\\\\n&= \\frac{e^{-x}}{(1 + e^{-x})^2} \\\\\n&= \\frac{1}{(1 + e^{-x})} \\frac{e^{-x}}{(1 + e^{-x})} \\\\\n&= \\sigma(x)(1 - \\sigma(x))\n\\end{aligned}\n$$\n\nBy this logic, we will define:\n\n$$\nG_{ij} = C_{ij}(1 - C_{ij}) / \\tau\n$$\n\nwhich is basically just the derivative of the pairwise comparison term $C_{ij}$ with respect to the model score. With that additional definition, we can write the derivatives more simply as follows:\n\n$$\n\\begin{aligned}\n\\frac{dAP}{ds_k} &= \\frac{1}{n^+} \\sum_{i:y_i=1} \\frac{dprec_i}{ds_k} \\\\\n\\frac{dprec_i}{ds_k} &= \\frac{r_i \\frac{dr_i^+}{ds_k} - r_i^+ \\frac{dr_i}{ds_k} }{r_i^2} \\\\\n\\frac{dr_i}{ds_k} &=\n    \\begin{cases}\n    -\\sum_j G_{ij}, & k=i \\\\\n    G_{ik}, & k \\ne i\n    \\end{cases} \\\\\n\\frac{dr_i^+}{ds_k} &=\n    \\begin{cases}\n    -\\sum_{j:y_j=1} G_{ij}, & k=i \\\\\n    G_{ik}1[y_k=1], & k \\ne i\n    \\end{cases} \\\\\n\\end{aligned}\n$$\n\nFinally, remember that $s_i = x_i^TB$ so $\\frac{ds_k}{dB} = x_k$ and so $\\frac{dAP}{dB} = \\frac{dAP}{ds_k} \\frac{ds_k}{dB} = \\frac{dAP}{ds_k}x_k$.\n\nThat all gets plugged together into the formula for calculating the gradient below.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# SmoothAP:\n#  C_ij = sigma((s_j - s_i)/tau), r_i  = 1 + sum_j C_ij\n#  r^+_i = 1 + sum_{j:y_j=1} C_ij, precision_i = r^+_i / r_i\n#  SmoothAP = (1/P) * sum_{i:y_i=1} precision_i\nsmoothap_loss_grad <- function(X, y, beta, tau = 0.05, lambda = 0.0, reg_intercept = FALSE) {\n  n <- nrow(X)\n  s <- as.numeric(X %*% beta)\n  pos_idx <- which(y == 1)\n  P <- length(pos_idx)\n\n  Sdiff <- outer(s, s, function(si, sj) (sj - si) / tau)\n  diag(Sdiff) <- 0\n  C  <- sigmoid(Sdiff)\n  G  <- C * (1 - C) / tau\n  diag(G) <- 0\n\n  r  <- 1 + rowSums(C)\n  rp <- 1 + rowSums(C[ , y == 1, drop = FALSE])\n  prec <- rp / r\n  smoothAP <- mean(prec[pos_idx])\n\n  pos_mask <- as.integer(y == 1)\n  dL_ds <- numeric(n)\n\n  for (i in pos_idx) {\n    Gi <- G[i, ]\n    sumGi_all <- sum(Gi)\n    sumGi_pos <- sum(Gi * pos_mask)\n    base_vec  <- ( r[i] * (Gi * pos_mask) - rp[i] * Gi ) / (r[i]^2)\n    diag_term <- ( r[i] * (-sumGi_pos)   - rp[i] * (-sumGi_all) ) / (r[i]^2)\n    contrib   <- base_vec\n    contrib[i] <- contrib[i] + diag_term\n    dL_ds     <- dL_ds + (1 / P) * contrib\n  }\n\n  grad_beta <- as.numeric(crossprod(X, dL_ds))\n\n  penalty <- 0\n  if (lambda > 0) {\n    if (reg_intercept) {\n      grad_beta <- grad_beta - lambda * beta\n      penalty   <- (lambda / 2) * sum(beta^2)\n    } else {\n      mask <- c(0, rep(1, length(beta) - 1))\n      grad_beta <- grad_beta - lambda * (beta * mask)\n      penalty   <- (lambda / 2) * sum(beta[-1]^2)\n    }\n  }\n  list(loss = smoothAP - penalty, grad = grad_beta, smoothAP = smoothAP)\n}\n```\n:::\n\n\n### Sigmoid for Smoothing\n\nIn the gradient calculation above, we used the term $C_{ij} = \\sigma(\\frac{s_j - s_i}{\\tau})$ which is where the smooth approximation was being made. This replaces what would otherwise have been a non-differentiable indicator function, also known as the Heaviside step function $1[s_j \\gt s_i]$. Here is a plot showing how the sigmoid function we are using approximates this indicator function, where the $\\tau$ parameter introduces more smoothness into the approximation, and hence more error as well. The degree of smoothness that is useful for the estimation process becomes a hyperparameter that needs to be tuned as part of the model fitting process.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create a data frame for the sigmoid function\nx_sigmoid <- rep(seq(-5, 5, length.out = 400), times = 5)\ntau_sigmoid <- rep(c(0.01, 0.05, 0.10, 0.25, 1), each = 400)\ndf_sigmoid <- data.frame(\n  x = x_sigmoid,\n  tau = tau_sigmoid,\n  y = sigmoid(x_sigmoid/tau_sigmoid)\n)\ndf_sigmoid$tau = as.factor(df_sigmoid$tau)\n\n# Create a data frame for the indicator function\nx_indicator <- c(-5, 0, 0, 5)\ny_indicator <- c(0, 0, 1, 1)\ndf_indicator <- data.frame(\n  x = x_indicator,\n  y = y_indicator\n)\n\n# Create the plot\nggplot() +\n  # Add the sigmoid curve\n  geom_line(data = df_sigmoid, aes(x = x, y = y, color = tau)) +\n  # Add the indicator function\n  geom_line(data = df_indicator, aes(x = x, y = y), color = \"black\", linetype = \"dashed\", lwd = 1.2) +\n  # Set the labels and title\n  labs(\n    title = \"Sigmoid Function vs. Indicator Function\",\n    x = \"x\",\n    y = \"f(x/tau)\"\n  ) +\n  theme(legend.position = \"bottom\")\n```\n\n::: {.cell-output-display}\n![](pr_auc_optimization_files/figure-html/sigmoid-approximation-1.png){width=672}\n:::\n:::\n\n\n### Gradient Ascent\n\nWe prepared the function to calculate the gradient above, so next we need to use it to build the optimization function. We are maximizing the objective function, so we are adding the gradient contribution to our parameters at each step of the calculation.\n\nWe will implement this two ways. One is a batch gradient ascent optimization which computes the gradient over the full training data at each step. This is the cleanest way to do it, but problems can arise if the training data becomes very large because this requires us to compute pairwise metrics so the computation grows exponentially as $n^2$. An alternative approach for use with larger datasets is a minibatch method, which is basically doing the same thing, but sampling from the training data at each gradient update step so that the update is still representative of the training data but is not so large anymore.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_smoothap_batch <- function(X, y, tau = 0.05, lambda = 0.01, reg_intercept = FALSE,\n                               lr = 0.5, decay = 0.95, epochs = 2000, tol = 1e-7, verbose = FALSE) {\n  beta <- rep(0, ncol(X))\n  for (ep in 1:epochs) {\n    out <- smoothap_loss_grad(X, y, beta, tau, lambda, reg_intercept)\n    beta_new <- beta + lr * out$grad\n    step_norm <- sqrt(sum((beta_new - beta)^2))\n    beta <- beta_new\n    if (verbose && (ep %% 100 == 0 || ep == 1)) {\n      cat(sprintf(\"Batch ep=%4d | SmoothAP=%.4f | step=%.3e | lr=%.3g\\n\",\n                  ep, out$smoothAP, step_norm, lr))\n    }\n    if (step_norm < tol) {\n      if (verbose) cat(sprintf(\"Batch converged at ep=%d\\n\", ep))\n      break\n    }\n    lr <- lr * decay\n  }\n  beta\n}\n\nfit_smoothap_minibatch <- function(X, y, tau = 0.05, lambda = 0.01, reg_intercept = FALSE,\n                                   lr = 0.1, decay = 0.99, epochs = 4000, batch_size = 16,\n                                   tol = 1e-7, verbose = FALSE) {\n  beta <- rep(0, ncol(X))\n  n <- nrow(X)\n  for (t in 1:epochs) {\n    #idx <- sort(sample.int(n, min(batch_size, n)))\n    idx <- createDataPartition(y, p = min(batch_size, n)/n)[[1]]\n    Xb <- X[idx, , drop = FALSE]; yb <- y[idx]\n    if (!any(yb == 1) || !any(yb == 0)) next\n    out <- smoothap_loss_grad(Xb, yb, beta, tau, lambda, reg_intercept)\n    beta_new <- beta + lr * out$grad\n    step_norm <- sqrt(sum((beta_new - beta)^2))\n    beta <- beta_new\n    if (verbose && (t %% 250 == 0 || t == 1)) {\n      full <- smoothap_loss_grad(X, y, beta, tau, lambda, reg_intercept)\n      cat(sprintf(\"Mini it=%5d | SmoothAP(full)=%.4f | step=%.3e | lr=%.3g\\n\",\n                  t, full$smoothAP, step_norm, lr))\n    }\n    if (step_norm < tol) {\n      if (verbose) cat(sprintf(\"Mini converged at it=%d\\n\", t))\n      break\n    }\n    lr <- lr * decay\n  }\n  beta\n}\n```\n:::\n\n\n## Testing Performance\n\nNow that we have the estimation functions built, we can try tuning the custom model on some example data and then comparing the performance with a traditional logistic regression.\n\n### Data Processing\n\nWe split the data into 60/20/20 train/validation/test. We use performance on the validation set to tune the hyperparameters (tau, learning rate, etc.) for our custom models, then fit all the models on the train and validation data and measure performance on the test set.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create train, validation, and test splits\nset.seed(42)\n\nspl <- createDataPartition(as.factor(y), p = 0.6)\nidx_train <- spl[[1]]\nXtr <- X[idx_train, , drop = FALSE]; ytr <- y[idx_train]\n\ny_temp <- y[-idx_train]\nX_temp <- X[-idx_train, , drop = FALSE]\n\nspl <- createDataPartition(as.factor(y_temp), p = 0.5)\nidx_val <- spl[[1]]\nXva <- X_temp[idx_val, , drop = FALSE]; yva <- y_temp[idx_val]\nXte <- X_temp[-idx_val, , drop = FALSE]; yte <- y_temp[-idx_val]\n```\n:::\n\n\n### Hyperparameter Tuning\n\nWe select a few hyperparameters to test and then use grid search to find the combination that performs best over the validation set. \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create hyperparameter grids\ngrid_batch <- expand.grid(\n  tau    = c(0.01, 0.05, 0.10),\n  lambda = c(0.00, 0.01, 0.10),\n  lr     = c(0.2, 0.1, 0.05),\n  decay  = c(0.99, 0.95, 0.90),\n  KEEP.OUT.ATTRS = FALSE\n)\n\ngrid_mini <- expand.grid(\n  tau    = c(0.01, 0.05, 0.10),\n  lambda = c(0.00, 0.01, 0.10),\n  lr     = c(0.2, 0.1, 0.05),\n  decay  = c(0.99, 0.95, 0.90),\n  batch  = c(16, 32),\n  KEEP.OUT.ATTRS = FALSE\n)\n\n\neval_on_val <- function(beta, Xva, yva) {\n  # Scores are linear for ranking; no outer sigmoid\n  sc_va <- as.numeric(Xva %*% beta)\n  pr_auc(sc_va, yva)\n}\n\n# Batch SmoothAP tuning\nbest_b <- NULL; best_b_prauc <- -Inf\nfor (k in seq_len(nrow(grid_batch))) {\n  g <- grid_batch[k, ]\n  beta_k <- fit_smoothap_batch(Xtr, ytr,\n                               tau = g$tau, lambda = g$lambda,\n                               lr = g$lr, decay = g$decay,\n                               epochs = 1500, tol = 1e-6, verbose = FALSE)\n  score <- eval_on_val(beta_k, Xva, yva)\n  if (score > best_b_prauc) { best_b_prauc <- score; best_b <- list(params = g, beta = beta_k) }\n}\ncat(sprintf(\"Best batch SmoothAP val PR-AUC = %.4f with params: %s\\n\",\n            best_b_prauc, paste(names(best_b$params), unlist(best_b$params), sep=\"=\", collapse=\", \")))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nBest batch SmoothAP val PR-AUC = 0.9848 with params: tau=0.1, lambda=0.1, lr=0.2, decay=0.99\n```\n:::\n\n```{.r .cell-code}\n# Mini-batch SmoothAP tuning\nbest_m <- NULL; best_m_prauc <- -Inf\nfor (k in seq_len(nrow(grid_mini))) {\n  g <- grid_mini[k, ]\n  beta_k <- fit_smoothap_minibatch(Xtr, ytr,\n                                   tau = g$tau, lambda = g$lambda,\n                                   lr = g$lr, decay = g$decay,\n                                   epochs = 2500, batch_size = g$batch,\n                                   tol = 1e-6, verbose = FALSE)\n  score <- eval_on_val(beta_k, Xva, yva)\n  if (score > best_m_prauc) { best_m_prauc <- score; best_m <- list(params = g, beta = beta_k) }\n}\ncat(sprintf(\"Best mini SmoothAP val PR-AUC = %.4f with params: %s\\n\",\n            best_m_prauc, paste(names(best_m$params), unlist(best_m$params), sep=\"=\", collapse=\", \")))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nBest mini SmoothAP val PR-AUC = 0.9870 with params: tau=0.1, lambda=0, lr=0.1, decay=0.95, batch=16\n```\n:::\n:::\n\n\nIt looks like a faster learning rate and less learning rate decay is best here. The optimal choice for tau is less than 1 but also greater than 0.01 so some smoothness is useful. Since several of these values are at the edge of the grid we could probably find better hyperparameter combinations if we expanded the grid and continued testing, but since this is just a toy example we will stop here.\n\n### Test Set Performance\n\nNow we apply the best hyperparameters we found in the tuning step above and fit the models on the training and validation data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Re-fit models on Train+Val data\nXtrva <- rbind(Xtr, Xva)\nytrva <- c(ytr, yva)\n\nbeta_batch <- fit_smoothap_batch(Xtrva, ytrva,\n                                 tau   = best_b$params$tau,\n                                 lambda= best_b$params$lambda,\n                                 lr    = best_b$params$lr,\n                                 decay = best_b$params$decay,\n                                 epochs= 3000, tol = 1e-7, verbose = FALSE)\n\nbeta_mini  <- fit_smoothap_minibatch(Xtrva, ytrva,\n                                     tau   = best_m$params$tau,\n                                     lambda= best_m$params$lambda,\n                                     lr    = best_m$params$lr,\n                                     decay = best_m$params$decay,\n                                     epochs= 4000, batch_size = best_m$params$batch,\n                                     tol = 1e-7, verbose = FALSE)\n\n# Baseline logistic (trained on train+val)\nglm_fit <- glm(ytrva ~ ., data = data.frame(ytrva = ytrva, Xtrva[ , -1, drop = FALSE]),\n               family = binomial())\n\n# Get model scores\nsc_batch <- as.numeric(Xte %*% beta_batch)\nsc_mini  <- as.numeric(Xte %*% beta_mini)\nsc_glm   <- as.numeric(predict(glm_fit, newdata = data.frame(Xte[ , -1, drop = FALSE]),\n                               type = \"link\"))\n\n# Performance Metrics on the test data\nres_test <- data.frame(\n  method = c(\"Logistic (MLE)\", \"SmoothAP (batch)\", \"SmoothAP (mini)\"),\n  PR_AUC = c(pr_auc(sc_glm, yte), pr_auc(sc_batch, yte), pr_auc(sc_mini, yte)),\n  ROC_AUC = c(roc_auc(sc_glm, yte), roc_auc(sc_batch, yte), roc_auc(sc_mini, yte))\n)\nknitr::kable(res_test)\n```\n\n::: {.cell-output-display}\n|method           |    PR_AUC|   ROC_AUC|\n|:----------------|---------:|---------:|\n|Logistic (MLE)   | 0.9746975| 0.9792070|\n|SmoothAP (batch) | 0.9927899| 0.9963733|\n|SmoothAP (mini)  | 0.9934036| 0.9966151|\n:::\n\n```{.r .cell-code}\n# Stratified bootstrap CIs for PR-AUC or ROC-AUC\nboot_ci <- function(scores, y, B = 1000, metric = c(\"pr\", \"roc\")) {\n  metric <- match.arg(metric)\n  n <- length(y)\n  pos_idx <- which(y == 1)\n  neg_idx <- which(y == 0)\n  npos <- length(pos_idx)\n  nneg <- length(neg_idx)\n\n  vals <- numeric(B)\n  for (b in seq_len(B)) {\n    # sample positives and negatives separately (with replacement)\n    samp_pos <- sample(pos_idx, npos, replace = TRUE)\n    samp_neg <- sample(neg_idx, nneg, replace = TRUE)\n    idx <- c(samp_pos, samp_neg)\n\n    scb <- scores[idx]\n    yb  <- y[idx]\n\n    vals[b] <- if (metric == \"pr\") pr_auc(scb, yb) else roc_auc(scb, yb)\n  }\n  stats::quantile(vals, c(0.025, 0.975))\n}\n\nB <- 1000  # reduce if runtime is a concern\n\nci_tbl <- function(name, scores, y) {\n  c(PR_L = boot_ci(scores, y, B, \"pr\")[1],\n    PR_U = boot_ci(scores, y, B, \"pr\")[2],\n    ROC_L= boot_ci(scores, y, B, \"roc\")[1],\n    ROC_U= boot_ci(scores, y, B, \"roc\")[2])\n}\n\ncis <- rbind(\n  c(method = \"Logistic (MLE)\",  ci_tbl(\"glm\",   sc_glm,   yte)),\n  c(method = \"SmoothAP (batch)\",ci_tbl(\"batch\", sc_batch, yte)),\n  c(method = \"SmoothAP (mini)\", ci_tbl(\"mini\",  sc_mini,  yte))\n)\ncis <- as.data.frame(cis, stringsAsFactors = FALSE)\ncis[, -1] <- lapply(cis[, -1], as.numeric)\n\ncat(\"\\nTest-set bootstrap 95% CIs\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nTest-set bootstrap 95% CIs\n```\n:::\n\n```{.r .cell-code}\nknitr::kable(cis)\n```\n\n::: {.cell-output-display}\n|method           | PR_L.2.5%| PR_U.97.5%| ROC_L.2.5%| ROC_U.97.5%|\n|:----------------|---------:|----------:|----------:|-----------:|\n|Logistic (MLE)   | 0.9381130|          1|  0.9429340|   0.9995164|\n|SmoothAP (batch) | 0.9787212|          1|  0.9896035|   1.0000000|\n|SmoothAP (mini)  | 0.9806356|          1|  0.9895974|   1.0000000|\n:::\n:::\n\n\nWe can see that this custom algorithm was able to achieve improved performance on PR-AUC as compared to traditional logistic regression, both when looking at the point estimates for performance as well as confidence intervals for that metric over the test set. The two estimation methods (batch and mini-batch) achieve basically identical performance, which we expect since the mini-batch method is just a way to extend the batch method to be feasible for use with larger datasets.\n\nIn addition to better performance on PR-AUC, this custom algorithm also happens to do better on ROC-AUC, which is a related rank-order performance metric.\n\n## Conclusion\n\nIn this analysis we implement a custom algorithm for estimating a logistic regression using a surrogate for PR-AUC as the objective function in place of the logistic loss function. We see that we are able to achieve improved performance as measured by PR-AUC when we optimize for it directly while estimating the model.\n",
    "supporting": [
      "pr_auc_optimization_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}