---
title: "LLM Transformer Implemented in Base R"
description: "An exercise in understanding the mechanics of the transformer architechture used in Large Language Models (LLMs)"
title-block-banner-color: white
image: "thumbnail.jpg"
draft: false
---

## Intro/Overview

One of the core components in AI models like ChatGPT is a language model. These language models are usually focused on predicting the next token in a sequence, and are used to generate language. For example, if the tokens are characters then given the sequence "orang" the model should predict the next character is "e". One of the major innovations for these models is the attention mechanism introduced by Vaswani, Ashish, et al.(2017) in their paper "Attention is all you need". This model architecture is called a transformer, and is what is used in the GPT (Generative Pre-trained Transformer) models in ChatGPT.

![](fig1.jpg){fig-align="center"}

In the following work, I will be implementing the transformer architecture from scratch in base R. The figure above has both encoder and decoder blocks as it is the architecture used for translation tasks, but I will only be implementing the decoder block on the right part of the figure without the encoder block embedding section, since that is not necessary for the next token prediction task.

My work heavily references [this YouTube video](https://www.youtube.com/watch?v=kCc8FmEb1nY) from Andrej Karpathy, as well as his related [minGPT github project](https://github.com/karpathy/minGPT). Karpathy is well known for developing and teaching the first deep learning course at Stanford, CS 231n: Convolutional Neural Networks for Visual Recognition. His materials break down the core mechanics of the transformer model architecture using the pytorch library in Python. My implementation will differ by being built in R (which I am more familiar with than Python) as well as not relying on any libraries and implementing the backpropogation from scratch. This will make it slower and less efficient than Karpathy's implementation which relies on the pytorch library to handle the backpropogation, but will help improve my understanding of the full process.

Full disclosure, around 80% of the content in the code blocks was constructed with assistance from an AI model: Google's Gemini 2.5 flash. I have implemented feed forward neural network models before, but this example is more complex than the proof of concepts I usually work on and Gemini was very helpful in figuring out how to structure the various functions and pass parameter information back and forth between them.

## Setting Up

### Data Preparation

Since the intention is to follow Karpathy's example it makes the most sense to use the same dataset, which is a text file containing the complete works of William Shakespeare. The original file can be found on the [Project Gutenberg website](https://www.gutenberg.org/), but the file Karpathy uses is much cleaner so it is easier to use [his file](https://github.com/karpathy/ng-video-lecture/blob/master/input.txt) directly.

```{r}
#| label: data-preparation

# text corpus
text_corpus = readLines("input.txt")
text_corpus = paste(text_corpus, collapse = "\n")

# Create a vocabulary of unique characters
chars = sort(unique(strsplit(text_corpus, "")[[1]]))
vocab_size = length(chars)

# Create a mapping from character to integer (stoi: string_to_int)
stoi = setNames(1:vocab_size, chars)

# Create a mapping from integer to character (itos: int_to_string)
itos = setNames(chars, 1:vocab_size)

# Helper functions for encoding and decoding
encode = function(s) {
  sapply(strsplit(s, "")[[1]], function(char) stoi[[char]])
}

decode = function(l) {
  paste(sapply(l, function(idx) itos[[as.character(idx)]]), collapse = "")
}

cat("Vocabulary size:", vocab_size, "\n")
cat("Character to index mapping (first 5):", paste(names(head(stoi, 5)), head(stoi, 5), sep = ":", collapse = ", "), "\n")
cat("Example encoding of 'hello':", encode("hello"), "\n")
cat("Example decoding of [1 2 3 4 5]:", decode(c(1, 2, 3, 4, 5)), "\n")
```

## Hyperparameters and Model Architecture

Usually in a transformer model, one layer of multi-headed attention will be followed by one feed forward layer, and that will constitute one transformer block. Then there will be multiple transformer blocks in sequence that make up the model. In this example, only a single transformer block will be used for simplicity, since keeping track of all the parameters for the gradient update is already complex, even for only one block.

```{r}
#| label: hyperparameters

block_size = 8    # how many characters we consider for prediction
n_embd = 16       # embedding dimension, also model dimension (d_model)
n_head = 4        # number of attention heads
head_size = n_embd / n_head # dimension of each attention head
learning_rate = 0.01
n_iter = 100000     # number of training iterations (more needed for real performance)
dropout_rate = 0.10 # Dropout probability (e.g., 0.10 means 10% of neurons dropped)
gradient_clip_threshold = 1.0 # Gradient clipping threshold
# Batch size is 1 for simplicity in this from-scratch example.

```

### Training and Validation Split

The data is split into 90% training data and 10% validation data. One important note is that the first 90% of the file is used as training while the last 10% is used for validation, there is no random sampling in creating this partition. 

This is a deliberate choice due to the autoregressive nature of this model, if we used simple random sampling to make this split, we could lose the independence between training and validation by training on very similar data to what would exist in validation. As a simple example, let's pretend we have a context window of 5 characters and the word "incredible" is in the dataset. We could end up with a case like this:

- input: "incr", target: "e", split: training
- input: "ncre", target: "d", split: validation
- input: "cred", target: "i", split: training

The problem in this case is that the model has already been trained on most of that string already, so when it sees it in validation the performance will look similar to performance on the training set because the data is similar, and not because the model is actually generalizing well.

```{r}
#| label: data-split

# Create training examples (X, y)
data_encoded = encode(text_corpus)

X_data = matrix(NA, nrow = length(data_encoded) - block_size, ncol = block_size)
y_data = matrix(NA, nrow = length(data_encoded) - block_size, ncol = block_size)

for (i in 1:(length(data_encoded) - block_size)) {
  X_data[i, ] = data_encoded[i:(i + block_size - 1)]
  y_data[i, ] = data_encoded[(i + 1):(i + block_size)]
}

# create 90%/10% train/validation split
num_samples = nrow(X_data)
train_indices = 1:floor(0.9 * num_samples)
val_indices = setdiff(1:num_samples, train_indices)

# debugging, try to intentionally overfit
# train_indices = (1:20)*20
# val_indices = (1:20)*20

X_train = X_data[train_indices, ]
y_train = y_data[train_indices, ]
X_val = X_data[val_indices, ]
y_val = y_data[val_indices, ]

cat("Shape of X_train:", dim(X_train), "\n")
cat("Shape of y_train:", dim(y_train), "\n")
cat("Shape of X_val:", dim(X_val), "\n")
cat("Shape of y_val:", dim(y_val), "\n")

```

### Core Utility Functions

These are helper functions that handle things like the softmax operation, neuron activation, dropout, etc. Most operations are being applied to matrix objects and we want the output to also be a matrix. There are a lot of scenarios where base R functions want to simplify the data type of the output and may try to return a vector or something else when the input was a matrix, so the matrix() function is used heavily here to ensure the output is still a matrix object.

#### Layer Normalization

I struggled a lot with the implementation of layer normalization, so it is worth adding some additional explanation here around some aspects I found confusing.

Let's assume the input data to LayerNorm is $X \in R^{N \times D}$, where each row $X_i \in R^D$ is a sample. LayerNorm operates independently on each sample, so for an example of the forward pass we drop the batch index $i$ while focusing on a single vector $x \in R^D$.

__Forward Pass:__

$$
\begin{aligned}
\mu &= \frac{1}{D} \sum_{j=1}^D x_j \\
\sigma^2 &= \frac{1}{D} \sum_{j=1}^D (x_j - \mu)^2 \\
\hat{x_j} &= \frac{x_j - \mu}{\sqrt{\sigma^2 + \epsilon}} \\
y_j &= \gamma_j \hat{x_j} + \beta_j
\end{aligned}
$$

The $y_j$ elements are the output of LayerNorm. An important observation is that the third line above shows the normalization, which is happening over the rows of $X$. Note how the first two lines calculating $\mu$ and $\sigma^2$ are over the dimension of the columns $D$, so each row has its own mean and variance and is being normalized. However, the affine transformation in the fourth line shows parameters $\gamma_j$ and $\beta_j$ with the index $j$ over the column space, so this last transformation is a scale and shift over the columns.

I also had trouble understanding the backward pass for LayerNorm. The gradients for $\gamma$ and $\beta$ are fairly straightforward since they are just an affine transformation at the end, but passing the gradient back through to the input data was tricky, and this difference between transformations over rows vs columns complicates things somewhat.

__Backward Pass:__

We want $\frac{dL}{dx_j}$, the gradient for the input row data.

Using the chain rule:

$$
\frac{dL}{dx_j} = \sum_k \frac{dL}{d\hat{x_k}} \frac{d\hat{x_k}}{dx_j}
$$

where $k$ is a summation index (over columns) we are introducing to iterate over the columns, while $j$ is the specific column index of the individual element within the row vector $x$ that we are differentiating with respect to.

Computing $\frac{d \hat{x_k}}{dx_j}$:

recall that:

$$
\begin{aligned}
\mu &= \frac{1}{D} \sum_{k=1}^D x_k \\
\sigma^2 &= \frac{1}{D} \sum_{k=1}^D (x_k - \mu)^2 \\
\hat{x_k} &= \frac{x_k - \mu}{\sqrt{\sigma^2 + \epsilon}}
\end{aligned}
$$

This has two dependencies, one directly through $x_k$ as each element of $x_k$ is $x_j$ when $k=j$, and also indirectly through $\mu$ and $\sigma^2$ which are functions of $x_j$.

For the mean $\mu$:

$$
\frac{d\mu}{dx_j} = \frac{1}{D}
$$

For the variance $\sigma^2$:

$$
\begin{aligned}
\frac{d\sigma^2}{dx_j} &= \frac{1}{D} \sum_{k=1}^D \frac{d}{dx_j} (x_k - \mu)^2 \\
&= \frac{1}{D} \sum_{k=1}^D 2(x_k - \mu) \frac{d}{dx_j} (x_k - \mu) \\
&= \frac{2}{D} \sum_{k=1}^D (x_k - \mu) (\delta_{kj} - \frac{1}{D})
\end{aligned}
$$

Note that $\frac{dx_k}{dx_j} = \delta_{kj}$ (1 if k=j, 0 otherwise)

Now we have to think of that summation in 2 parts. First, when k=j, then $\delta_{kj} = 1$ and we have:

$$
(x_k - \mu) (\delta_{kj} - \frac{1}{D}) = (x_j - \mu) (1 - \frac{1}{D})
$$

Then for the other terms where $k \ne j$:

$$
\sum_{k \ne j} (x_k - \mu) (\delta_{kj} - \frac{1}{D}) = - \frac{1}{D} \sum_{k \ne j} (x_k - \mu)
$$

However, this second term can be simplified because summing over a vector minus its mean is zero.

$$
\sum_{k=1}^D (x_k - \mu) = 0 \\
\sum_{k \ne j} (x_k - \mu) = -(x_j - \mu)
$$

So the second term becomes

$$
- \frac{1}{D} * -(x_j - \mu) = \frac{1}{D} (x_j - \mu)
$$
then putting both parts back together gives us

$$
\begin{aligned}
\frac{d\sigma^2}{dx_j} &= \frac{2}{D}((x_j - \mu) (1 - \frac{1}{D}) + \frac{1}{D} (x_j - \mu)) \\
&= \frac{2}{D}(x_j - \mu)
\end{aligned}
$$

Now we can use the results for $\frac{d\mu}{dx_j}$ and $\frac{d\sigma^2}{dx_j}$ to calculate $\frac{d\hat{x_k}}{dx_j}$. Using the quotient rule:

$$
\begin{aligned}
\frac{d\hat{x_k}}{dx_j} &= (\sqrt{\sigma^2 + \epsilon} \frac{d}{dx_j} [(x_k - \mu)] - (x_k - \mu) \frac{d}{dx_j} [\sqrt{\sigma^2 + \epsilon}]) (\sigma^2 + \epsilon)^{-1} \\
&= (\sigma^2 + \epsilon)^{-1/2}(\delta_{kj} - \frac{1}{D}) - (x_k - \mu) \frac{1}{2}(\sigma^2 + \epsilon)^{-3/2} \frac{d\sigma^2}{dx_j} \\
&= (\sigma^2 + \epsilon)^{-1/2}(\delta_{kj} - \frac{1}{D}) - (x_k - \mu) \frac{1}{2}(\sigma^2 + \epsilon)^{-3/2} \frac{2}{D}(x_j - \mu) \\
&= \frac{1}{\sqrt{\sigma^2 + \epsilon}} [\delta_{kj} - \frac{1}{D} - \frac{(x_k - \mu)(x_j - \mu)}{(\sigma^2 + \epsilon)D}]
\end{aligned}
$$

Finally, we can plug this back into the first equation for the gradient we are after. To simplify the notation slightly, we will say $\frac{dL}{d\hat{x_k}} = \hat{g_k}$

$$
\begin{aligned}
\frac{dL}{dx_j} &= \sum_k \frac{dL}{d\hat{x_k}} \frac{d\hat{x_k}}{dx_j} \\
&= \sum_k \hat{g_k} \frac{d\hat{x_k}}{dx_j} \\
&= \sum_k \hat{g_k} \frac{1}{\sqrt{\sigma^2 + \epsilon}} [\delta_{kj} - \frac{1}{D} - \frac{(x_k - \mu)(x_j - \mu)}{(\sigma^2 + \epsilon)D}] \\
&= \frac{1}{\sqrt{\sigma^2 + \epsilon}} (\hat{g_j} - \frac{1}{D} \sum_k \hat{g_k} - \frac{(x_j - \mu)}{(\sigma^2 + \epsilon)D} \sum_k \hat{g_k} (x_k - \mu)
\end{aligned}
$$

If you look at the layer norm backward function implemented below, you can see that the above formula is what is used to calculate the gradient d_X.

```{r}
#| label: core-utilities

# Softmax activation function (row-wise for matrices)
softmax_matrix_rows = function(X) {
  t(apply(X, 1, function(row) {
    exp_row = exp(row - max(row)) # Numerical stability
    return(exp_row / sum(exp_row))
  })) # wrapped with t() to transpose back to original shape
}

# Layer Normalization
layer_norm_forward = function(X, gamma, beta, epsilon = 1e-5) {
  mean_X = apply(X, 1, mean)
  var_X = apply(X, 1, var)

  X_norm = (X - matrix(mean_X, nrow = nrow(X), ncol = ncol(X), byrow = FALSE)) /
           sqrt(matrix(var_X + epsilon, nrow = nrow(X), ncol = ncol(X), byrow = FALSE))

  out = X_norm * matrix(gamma, nrow = nrow(X), ncol = ncol(X), byrow = TRUE) +
        matrix(beta, nrow = nrow(X), ncol = ncol(X), byrow = TRUE)

  return(list(out = out, X_norm = X_norm, mean_X = mean_X, var_X = var_X, X = X)) # Store X for backward
}

layer_norm_backward = function(dout, X, X_norm, mean_X, var_X, gamma, epsilon = 1e-5) {
  N = nrow(X) # Number of samples (batch size)
  D = ncol(X) # Number of features (embedding dimension)

  # Gradients for gamma and beta
  dgamma = colSums(dout * X_norm)
  dbeta = colSums(dout)

  # Gradient of X_norm
  d_X_norm = dout * matrix(gamma, nrow = N, ncol = D, byrow = TRUE)

  std_inv = 1 / sqrt(var_X + epsilon)  # (N,)
  std_inv_mat = matrix(std_inv, nrow = N, ncol = D)

  # Sum over features (columns) for each row
  d_X_norm_sum = rowSums(d_X_norm)                   # shape (N,)
  d_X_norm_dot_Xnorm = rowSums(d_X_norm * X_norm)    # shape (N,)

  term1 = d_X_norm
  term2 = matrix(d_X_norm_sum / D, nrow = N, ncol = D)
  term3 = X_norm * matrix(d_X_norm_dot_Xnorm / D, nrow = N, ncol = D)

  d_X = (term1 - term2 - term3) * std_inv_mat

  return(list(d_X = d_X, dgamma = dgamma, dbeta = dbeta))
}

# ReLU activation function
relu = function(x) {
  matrix(pmax(0, x), nrow = nrow(x), ncol = ncol(x))
}

relu_grad = function(x) {
  (x > 0) * 1
}

# Dropout layer
dropout_forward = function(x, dropout_rate, is_training) {
  if (is_training) {
    mask = matrix(runif(nrow(x)*ncol(x)), nrow = nrow(x), ncol = ncol(x)) > dropout_rate
    out = x * mask / (1 - dropout_rate) # Scale up during training
    return(list(out = out, mask = mask))
  } else {
    return(list(out = x, mask = NULL)) # No dropout during inference
  }
}

dropout_backward = function(dout, mask, dropout_rate) {
  # Apply the same mask and scaling to the gradient
  return(dout * mask / (1 - dropout_rate))
}

# He Initialization function for weights
he_init_weights = function(fan_in, fan_out, ReLU_activation = FALSE) {
  # Only account for halving the variance if ReLU actiavtion is being applied
  if(ReLU_activation){
    std_dev = sqrt(2 / fan_in)
  }else{
    std_dev = sqrt(1 / fan_in)
  }
    
  weights = matrix(rnorm(fan_in * fan_out, mean = 0, sd = std_dev),
                   nrow = fan_in, ncol = fan_out)
  return(weights)
}

# Gradient Clipping function (by global norm)
clip_gradients_by_norm = function(gradients_list, clip_threshold) {
  # Flatten all gradients into a single vector to calculate the norm
  all_grad_elements = c()
  for (grad_name in names(gradients_list)) {
    all_grad_elements = c(all_grad_elements, as.vector(gradients_list[[grad_name]]))
  }

  global_norm = sqrt(sum(all_grad_elements^2))

  # If the global norm exceeds the threshold, scale all gradients
  if (global_norm > clip_threshold) {
    scale_factor = clip_threshold / global_norm
    clipped_gradients = lapply(gradients_list, function(grad_matrix) {
      grad_matrix * scale_factor
    })
    return(clipped_gradients)
  } else {
    # No clipping needed
    return(gradients_list)
  }
}

```

### Model Parameters Initialization

Many of the parameters will be initialized using He initialization. He initialization (also known as Kaiming initialization) is a weight initialization technique widely used in deep neural networks, especially when using Rectified Linear Unit (ReLU) as the activation function which we are doing here. The weights are determined like this:

$$
W = N(0, \sigma^2) \text{ where } \sigma = \sqrt{\frac{2}{n_{in}}}
$$

The basic idea is to choose starting weights that won't immediately cause problems with vanishing or exploding gradients. We want to keep the variance of activations and gradients consistent as we move forward and backward through the layers in the network. Consider the simple feed forward layer as follows:

$$
\begin{aligned}
&y = Wx + b \\
&a = f(y)
\end{aligned}
$$

where:

- x is the input vector to the layer.
- W is the weight matrix of the layer.
- b is the bias vector.
- y is the linear output before activation.
- f is the activation function (e.g., ReLU).
- a is the activated output of the layer.

For simplicity, let's assume the biases b are initialized to zero (which is common and is what we are doing here) and that the elements of x and W are independent and have zero mean.

ReLU is defined as f(y)=max(0,y). This means that for y<0, the output is 0, and for y>=0, the output is y. When initialized, roughly half of the inputs to a ReLU neuron will be negative (resulting in zero output) and half will be positive (resulting in the input value). This effectively halves the variance compared to a linear activation function.

We want the variance of the output a to be roughly equal to the variance of the input x.

Consider the variance of y=Wx:

$$ 
var(y) = var(\sum_{i=1}^{n_{in}} W_i x_i)
$$

Assuming $W_i$ and $x_i$ are independent and have zero mean:

$$
\begin{aligned}
var(y) &= \sum_{i=1}^{n_{in}} var(W_i x_i) \\
&= \sum_{i=1}^{n_{in}} E[(W_i x_i)^2] - E[(W_i x_i)]^2 \\
&= \sum_{i=1}^{n_{in}} E[W_i^2] E[x_i^2] - (E[W_i] E[x_i])^2 \\
&= \sum_{i=1}^{n_{in}} (var(W_i) + E[W_i]^2) (var(x_i) + E[x_i]^2) - (E[W_i] E[x_i])^2 \\
&= \sum_{i=1}^{n_{in}} (E[W_i]^2 var(x_i) + E[x_i]^2 var(W_i) + var(W_i)var(x_i))
\end{aligned}
$$

Since we assume $E[W_i]=0$ and $E[x_i]=0$:

$$
    var(y) = \sum_{i=1}^{n_{in}} var(W_i)var(x_i)
$$
Assuming all $W_i$ and $x_i$ are identically distributed:

$$
    var(y) = n_{in} var(W)var(x)
$$

Now, for ReLU, a=max(0,y). If y has zero mean and is symmetric around zero, then roughly half of the values of y will be positive and half will be negative. The negative values become zero, effectively reducing the variance by half. So, for the output after ReLU:

$$
\begin{aligned}
var(a) &\approx \frac{1}{2} var(y) \\
&\approx \frac{1}{2} n_{in} var(W)var(x)
\end{aligned}
$$

Remember the objective: We want the variance of the output a to be roughly equal to the variance of the input x. So we substitute $var(x)$ for $var(a)$ in this last equation, solving for the variance in weights $W$ that achieve the desired outcome.

$$
    var(x) \approx \frac{1}{2} n_{in} var(W)var(x)
$$

and therefore:

$$
\begin{aligned}
\frac{1}{2} n_{in} var(W) &= 1 \\
var(W_i) &= \frac{2}{n_{in}}
\end{aligned}
$$

Most layers don't have activation functions applied, so we only double the variance in the He initialization for the layer where the ReLU activation is used. In other cases, the same logic applies, just without the need to double the variance.

```{r}
#| label: parameter-init

set.seed(42) # For reproducibility

# Character Embeddings - typically not He-initialized, using uniform
C = matrix(runif(vocab_size * n_embd, -1, 1), nrow = vocab_size, ncol = n_embd)

# Positional Embeddings - typically not He-initialized (often fixed or small random)
P_emb = matrix(runif(block_size * n_embd, -1, 1), nrow = block_size, ncol = n_embd)

# Multi-Head Attention Parameters (using He Initialization)
# For Wq, Wk, Wv, fan_in is n_embd
Wq = he_init_weights(n_embd, n_head * head_size)
Wk = he_init_weights(n_embd, n_head * head_size)
Wv = he_init_weights(n_embd, n_head * head_size)
# For Wo, fan_in is n_head * head_size
Wo = he_init_weights(n_head * head_size, n_embd)

# LayerNorm for Attention - gamma initialized to 1s, beta to 0s
ln1_gamma = rep(1, n_embd)
ln1_beta = rep(0, n_embd)

# Feed-Forward Network Parameters (using He Initialization)
# For W_ff1, fan_in is n_embd
W_ff1 = he_init_weights(n_embd, 4 * n_embd, ReLU_activation = TRUE)
b_ff1 = rep(0, 4 * n_embd) # Biases are kept at 0
# For W_ff2, fan_in is 4 * n_embd
W_ff2 = he_init_weights(4 * n_embd, n_embd)
b_ff2 = rep(0, n_embd) # Biases are kept at 0

# LayerNorm for FF - gamma initialized to 1s, beta to 0s
ln2_gamma = rep(1, n_embd)
ln2_beta = rep(0, n_embd)

# Final linear layer to logits (using He Initialization)
# For W_final, fan_in is n_embd
W_final = he_init_weights(n_embd, vocab_size)
b_final = rep(0, vocab_size) # Biases are kept at 0

# Store all parameters in a list for easier management
params = list(
  C = C, P_emb = P_emb,
  Wq = Wq, Wk = Wk, Wv = Wv, Wo = Wo,
  ln1_gamma = ln1_gamma, ln1_beta = ln1_beta,
  W_ff1 = W_ff1, b_ff1 = b_ff1, W_ff2 = W_ff2, b_ff2 = b_ff2,
  ln2_gamma = ln2_gamma, ln2_beta = ln2_beta,
  W_final = W_final, b_final = b_final
)

cat("Parameters initialized.\n")

```

## Transformer Block Implementation Functions

### Multi-Head Self-Attention

The self-attention mechanism helps the model learn weights associated with prior tokens in the sequence. It could actually look forward as well if allowed to do so, like when translation is the desired function, but usually for next token prediction the attention mechanism is restricted to only look backwards at previous tokens. This is handled through the "is_causal" parameter in the function below, which will set the weights for any future token positions to zero during the softmax step.

The multi-headed piece of the mechanism is basically because multiple "heads" of the attention mechanism are run in parallel, and then the output is joined together in a final linear layer at the end. This lets the model learn more than one way to pay attention to previous tokens, if that is beneficial for the prediction.

This mechanism is summarized in figure 2 of Vaswani, Ashish, et al.(2017), reproduced here:

![](fig2.jpg){fig-align="center"}

The key equation from that paper which we are implementing here is:

$$
\text{Attention}(Q, K, V) = \text{softmax}(\frac{QK^T}{\sqrt{d_k}}) V
$$

The softmax function produces weights which are what determine how much attention is paid to previous tokens, while the V matrix contains the values emitted by those previous tokens.

```{r}
#| label: multi-headed-attention

multi_head_attention_forward = function(x, Wq, Wk, Wv, Wo, n_head, head_size, ln_gamma, ln_beta, dropout_rate, is_training, is_causal = TRUE) {
  x_in = x

  # Pre-LN:
  ln_out_x = layer_norm_forward(x, ln_gamma, ln_beta)
  x = ln_out_x$out

  Q_all = x %*% Wq
  K_all = x %*% Wk
  V_all = x %*% Wv

  outputs_per_head = lapply(1:n_head, function(h) {
    start_col = (h - 1) * head_size + 1
    end_col = h * head_size
    Q_h = Q_all[, start_col:end_col]
    K_h = K_all[, start_col:end_col]
    V_h = V_all[, start_col:end_col]

    scores = (Q_h %*% t(K_h)) / sqrt(head_size)

    if (is_causal) {
      mask_causal = upper.tri(matrix(0, nrow(scores), ncol(scores)), diag = FALSE)
      scores[mask_causal] = -Inf
    }

    attention_weights = softmax_matrix_rows(scores)
    # Dropout after attention weights (before multiplying by V)
    attn_weights_dropout = dropout_forward(attention_weights, dropout_rate, is_training)
    attention_weights_dropped = attn_weights_dropout$out

    attended_values = attention_weights_dropped %*% V_h
    return(list(attended_values = attended_values,
                attention_weights = attention_weights, # Original for backward
                attention_weights_dropped_mask = attn_weights_dropout$mask,
                scores = scores, Q_h = Q_h, K_h = K_h, V_h = V_h))
  })

  concat_heads = do.call(cbind, lapply(outputs_per_head, function(o) o$attended_values))
  attn_out = concat_heads %*% Wo

  # Dropout after attention block output (before residual)
  attn_out_dropout = dropout_forward(attn_out, dropout_rate, is_training)
  attn_out_dropped = attn_out_dropout$out

  out = x_in + attn_out_dropped

  return(list(
    out = out,
    attn_out_dropped = attn_out_dropped,
    attn_out_dropout_mask = attn_out_dropout$mask,
    concat_heads = concat_heads,
    outputs_per_head = outputs_per_head,
    x_for_qkvo = x, # Input to Q, K, V linear layers (post-LN)
    ln_cache_x = ln_out_x
  ))
}

multi_head_attention_backward = function(dout, x_in, attn_out_dropped, attn_out_dropout_mask, concat_heads,
                                       outputs_per_head, x_for_qkvo, ln_cache_x, Wq, Wk, Wv, Wo, n_head, head_size, ln_gamma, ln_beta, dropout_rate, is_causal = TRUE) {

  # Backward through dropout on attn_out
  d_attn_out = dropout_backward(dout, attn_out_dropout_mask, dropout_rate)

  d_Wo = t(concat_heads) %*% d_attn_out
  d_concat_heads = d_attn_out %*% t(Wo)

  d_Q_all = matrix(0, nrow = nrow(x_for_qkvo), ncol = ncol(Wq))
  d_K_all = matrix(0, nrow = nrow(x_for_qkvo), ncol = ncol(Wk))
  d_V_all = matrix(0, nrow = nrow(x_for_qkvo), ncol = ncol(Wv))

  for (h in 1:n_head) {
    head_cache = outputs_per_head[[h]]
    d_attended_values = d_concat_heads[, ((h - 1) * head_size + 1):(h * head_size)]

    # Backward through dropout on attention_weights
    d_attention_weights_dropped = d_attended_values %*% t(head_cache$V_h)
    d_attention_weights = dropout_backward(d_attention_weights_dropped, head_cache$attention_weights_dropped_mask, dropout_rate)

    d_V_h = t(head_cache$attention_weights_dropped) %*% d_attended_values # Use dropped weights for V_h grad

    # Backward through softmax (simplified Jacobian-vector product)
    d_scores = d_attention_weights * head_cache$attention_weights # Element-wise multiply for part of softmax grad
    d_scores = d_scores - rowSums(d_scores * head_cache$attention_weights) * head_cache$attention_weights # Sum over columns and apply second part

    if (is_causal) {
      mask_causal = upper.tri(matrix(0, nrow(d_scores), ncol(d_scores)), diag = FALSE)
      d_scores[mask_causal] = 0
    }

    d_Q_h = (d_scores %*% head_cache$K_h) / sqrt(head_size)
    d_K_h = (t(d_scores) %*% head_cache$Q_h) / sqrt(head_size)

    d_Q_all[, ((h - 1) * head_size + 1):(h * head_size)] = d_Q_all[, ((h - 1) * head_size + 1):(h * head_size)] + d_Q_h
    d_K_all[, ((h - 1) * head_size + 1):(h * head_size)] = d_K_all[, ((h - 1) * head_size + 1):(h * head_size)] + d_K_h
    d_V_all[, ((h - 1) * head_size + 1):(h * head_size)] = d_V_all[, ((h - 1) * head_size + 1):(h * head_size)] + d_V_h
  }

  d_Wq = t(x_for_qkvo) %*% d_Q_all
  d_Wk = t(x_for_qkvo) %*% d_K_all
  d_Wv = t(x_for_qkvo) %*% d_V_all

  d_x_from_qkvo = (d_Q_all %*% t(Wq)) + (d_K_all %*% t(Wk)) + (d_V_all %*% t(Wv))

  ln_grads_x = layer_norm_backward(d_x_from_qkvo, ln_cache_x$X, ln_cache_x$X_norm,
                                   ln_cache_x$mean_X, ln_cache_x$var_X, ln_gamma, epsilon = 1e-5)
  d_ln1_gamma = ln_grads_x$dgamma
  d_ln1_beta = ln_grads_x$dbeta
  d_x = dout + ln_grads_x$d_X

  return(list(
    d_x = d_x, d_Wq = d_Wq, d_Wk = d_Wk, d_Wv = d_Wv, d_Wo = d_Wo,
    d_ln1_gamma = d_ln1_gamma, d_ln1_beta = d_ln1_beta
  ))
}

```

### Position-wise Feed-Forward Network

This is a fully connected feed forward neural network with a single hidden layer. An activation function (ReLU in this case) is applied to the hidden layer. The hidden layer is 4 times the size of the input and output layers in this implementation, but there is no strong reason that needs to be the case. This image shows a common visualization for how this type of network looks.

![](fig3.png){fig-align="center"}

$$
\begin{aligned}
&a = f(W_1x + b_1) \\
&y = W_2a + b_2
\end{aligned}
$$

where:

- x is the input vector to the layer.
- W1 and W2 are the weight matrices into and out of the hidden layer.
- b1 and b2 are the bias vectors.
- f is the activation function (e.g., ReLU).
- a is the activated output of the hidden layer.
- y is the linear output after activation.

In the visualization above, the rows of x are the nodes on the left, they become the nodes a in the center after the first transformation, and then become the output y after the second transformation.

Given that the transformations are all linear and the activation function ReLU(x) = max(x, 0) is simple to work with, the implementation is fairly straightforward.

```{r}
#| label: feed-forward-neural-network

feed_forward_forward = function(x, W_ff1, b_ff1, W_ff2, b_ff2, ln_gamma, ln_beta, dropout_rate, is_training) {
  x_in = x

  # Pre-LN:
  ln_out_x = layer_norm_forward(x, ln_gamma, ln_beta)
  x = ln_out_x$out

  # First linear layer + ReLU
  hidden = x %*% W_ff1 + matrix(b_ff1, nrow = nrow(x), ncol = ncol(W_ff1), byrow = TRUE)
  hidden_activated = relu(hidden)

  # Dropout after hidden activation
  hidden_dropout = dropout_forward(hidden_activated, dropout_rate, is_training)
  hidden_dropped = hidden_dropout$out

  # Second linear layer
  ff_out = hidden_dropped %*% W_ff2 + matrix(b_ff2, nrow = nrow(hidden_dropped), ncol = ncol(W_ff2), byrow = TRUE)

  # Dropout after FF block output (before residual)
  ff_out_dropout = dropout_forward(ff_out, dropout_rate, is_training)
  ff_out_dropped = ff_out_dropout$out

  out = x_in + ff_out_dropped

  return(list(
    out = out,
    ff_out_dropped = ff_out_dropped,
    ff_out_dropout_mask = ff_out_dropout$mask,
    hidden = hidden,
    hidden_activated = hidden_activated,
    hidden_dropout_mask = hidden_dropout$mask,
    ln_cache_x = ln_out_x
  ))
}

feed_forward_backward = function(dout, x_in, ff_out_dropped, ff_out_dropout_mask, hidden, hidden_activated, hidden_dropout_mask,
                                 ln_cache_x, W_ff1, b_ff1, W_ff2, b_ff2, ln_gamma, ln_beta, dropout_rate) {

  # Backward through dropout on ff_out
  d_ff_out = dropout_backward(dout, ff_out_dropout_mask, dropout_rate)

  d_W_ff2 = t(hidden_activated) %*% d_ff_out # Use original hidden_activated here for grad of W_ff2
  d_b_ff2 = colSums(d_ff_out)

  d_hidden_dropped = d_ff_out %*% t(W_ff2)
  # Backward through dropout on hidden
  d_hidden_activated = dropout_backward(d_hidden_dropped, hidden_dropout_mask, dropout_rate)

  d_hidden = d_hidden_activated * relu_grad(hidden)

  d_W_ff1 = t(ln_cache_x$out) %*% d_hidden
  d_b_ff1 = colSums(d_hidden)

  d_x_from_ff = d_hidden %*% t(W_ff1)

  ln_grads_x = layer_norm_backward(d_x_from_ff, ln_cache_x$X, ln_cache_x$X_norm,
                                   ln_cache_x$mean_X, ln_cache_x$var_X, ln_gamma, epsilon = 1e-5)
  d_ln2_gamma = ln_grads_x$dgamma
  d_ln2_beta = ln_grads_x$dbeta
  d_x = dout + ln_grads_x$d_X

  return(list(
    d_x = d_x, d_W_ff1 = d_W_ff1, d_b_ff1 = d_b_ff1,
    d_W_ff2 = d_W_ff2, d_b_ff2 = d_b_ff2,
    d_ln2_gamma = d_ln2_gamma, d_ln2_beta = d_ln2_beta
  ))
}

```

### Transformer Model Forward Pass and Loss

This function puts together all the pieces we've built previously to get a single forward pass for the full model. It starts by getting the token embeddings for the input data (from the C matrix parameter) and adding the position embedding parameter data. This feeds into the single transformer block for this model (one layer of multi-headed self-attention, followed by a feed forward neural network with a single hidden layer). Then finally one last linear layer which converts the inputs into probabilities over the vocabulary space.

```{r}
#| label: transformer-forward

transformer_forward = function(X_batch, y_batch, params, dropout_rate, is_training) {
  # 1. Input and Positional Embeddings
  x_emb = t(sapply(1:block_size, function(j) params$C[X_batch[1, j], ]))
  x = x_emb + params$P_emb

  # Dropout after combined embeddings
  embed_dropout_out = dropout_forward(x, dropout_rate, is_training)
  x = embed_dropout_out$out

  # Transformer Block
  attn_out_cache = multi_head_attention_forward(x, params$Wq, params$Wk, params$Wv, params$Wo,
                                                n_head, head_size, params$ln1_gamma, params$ln1_beta,
                                                dropout_rate, is_training)
  x = attn_out_cache$out

  ff_out_cache = feed_forward_forward(x, params$W_ff1, params$b_ff1, params$W_ff2, params$b_ff2,
                                     params$ln2_gamma, params$ln2_beta, dropout_rate, is_training)
  x = ff_out_cache$out

  # Final linear layer to get logits for all tokens in the sequence
  logits = x %*% params$W_final + matrix(params$b_final, nrow = block_size, ncol = vocab_size, byrow = TRUE)

  # Softmax to get probabilities
  probs = softmax_matrix_rows(logits)

  # Select the probability of the true token for each entry in the flattened batch
  y_batch_flat = as.vector(t(y_batch))
  indices = cbind(1:length(y_batch_flat), y_batch_flat)
  correct_probs = probs[indices]
  
  # Compute Negative Log Likelihood / Cross-Entropy
  token_losses = -log(correct_probs)
  loss = mean(token_losses)

  return(list(
    logits = logits, probs = probs, loss = loss,
    x_emb = x_emb,
    embed_dropout_mask = embed_dropout_out$mask,
    attn_cache = attn_out_cache,
    ff_cache = ff_out_cache,
    last_x_output = x
  ))
}

```

### Transformer Model Backward Pass

This function puts together all the pieces we've built previously to get a single backward pass for the full model, producing gradients for all the parameters. It works backwards from the final output of probabilities, computing gradients for each of the model parameters and the associated data, and passing the gradient for the input data back to each prior step along the series of model transformations.

```{r}
#| label: transformer-backward

transformer_backward = function(cache, X_batch, y_batch, params, dropout_rate) {
  grads = list(
    d_C = matrix(0, nrow = nrow(params$C), ncol = ncol(params$C)),
    d_P_emb = matrix(0, nrow = nrow(params$P_emb), ncol = ncol(params$P_emb)),
    d_Wq = matrix(0, nrow = nrow(params$Wq), ncol = ncol(params$Wq)),
    d_Wk = matrix(0, nrow = nrow(params$Wk), ncol = ncol(params$Wk)),
    d_Wv = matrix(0, nrow = nrow(params$Wv), ncol = ncol(params$Wv)),
    d_Wo = matrix(0, nrow = nrow(params$Wo), ncol = ncol(params$Wo)),
    d_ln1_gamma = rep(0, length(params$ln1_gamma)),
    d_ln1_beta = rep(0, length(params$ln1_beta)),
    d_W_ff1 = matrix(0, nrow = nrow(params$W_ff1), ncol = ncol(params$W_ff1)),
    d_b_ff1 = rep(0, length(params$b_ff1)),
    d_W_ff2 = matrix(0, nrow = nrow(params$W_ff2), ncol = ncol(params$W_ff2)),
    d_b_ff2 = rep(0, length(params$b_ff2)),
    d_ln2_gamma = rep(0, length(params$ln2_gamma)),
    d_ln2_beta = rep(0, length(params$ln2_beta)),
    d_W_final = matrix(0, nrow = nrow(params$W_final), ncol = ncol(params$W_final)),
    d_b_final = rep(0, length(params$b_final))
  )

  # Transpose y_batch to get elements row by row, then flatten
  y_batch_flat = as.vector(t(y_batch))
  
  # Create one-hot targets for the flattened y_batch
  one_hot_targets_flat = matrix(0, nrow = length(y_batch_flat), ncol = vocab_size)
  one_hot_targets_flat[cbind(1:length(y_batch_flat), y_batch_flat)] = 1
  
  d_logits = cache$probs - one_hot_targets_flat

  d_W_final = t(cache$last_x_output) %*% d_logits
  d_b_final = colSums(d_logits)
  
  grads$d_W_final = grads$d_W_final + d_W_final
  grads$d_b_final = grads$d_b_final + d_b_final

  # d_last_token_output = d_logits %*% t(params$W_final)
  # 
  # d_x_from_logits = matrix(0, nrow = block_size, ncol = n_embd)
  # d_x_from_logits[block_size, ] = d_last_token_output[1, ]
  
  d_x_from_logits = d_logits %*% t(params$W_final)

  ff_grads = feed_forward_backward(d_x_from_logits, cache$attn_cache$out, cache$ff_cache$ff_out_dropped,
                                   cache$ff_cache$ff_out_dropout_mask, cache$ff_cache$hidden,
                                   cache$ff_cache$hidden_activated, cache$ff_cache$hidden_dropout_mask,
                                   cache$ff_cache$ln_cache_x, params$W_ff1, params$b_ff1,
                                   params$W_ff2, params$b_ff2, params$ln2_gamma, params$ln2_beta, dropout_rate)
  d_x_from_ff = ff_grads$d_x
  grads$d_W_ff1 = grads$d_W_ff1 + ff_grads$d_W_ff1
  grads$d_b_ff1 = grads$d_b_ff1 + ff_grads$d_b_ff1
  grads$d_W_ff2 = grads$d_W_ff2 + ff_grads$d_W_ff2
  grads$d_b_ff2 = grads$d_b_ff2 + ff_grads$d_b_ff2
  grads$d_ln2_gamma = grads$d_ln2_gamma + ff_grads$d_ln2_gamma
  grads$d_ln2_beta = grads$d_ln2_beta + ff_grads$d_ln2_beta

  attn_grads = multi_head_attention_backward(d_x_from_ff, cache$x_emb + params$P_emb,
                                             cache$attn_cache$attn_out_dropped, cache$attn_cache$attn_out_dropout_mask,
                                             cache$attn_cache$concat_heads, cache$attn_cache$outputs_per_head,
                                             cache$attn_cache$x_for_qkvo, cache$attn_cache$ln_cache_x,
                                             params$Wq, params$Wk, params$Wv, params$Wo, n_head, head_size,
                                             params$ln1_gamma, params$ln1_beta, dropout_rate)

  d_x_from_attn = attn_grads$d_x
  grads$d_Wq = grads$d_Wq + attn_grads$d_Wq
  grads$d_Wk = grads$d_Wk + attn_grads$d_Wk
  grads$d_Wv = grads$d_Wv + attn_grads$d_Wv
  grads$d_Wo = grads$d_Wo + attn_grads$d_Wo
  grads$d_ln1_gamma = grads$d_ln1_gamma + attn_grads$d_ln1_gamma
  grads$d_ln1_beta = grads$d_ln1_beta + attn_grads$d_ln1_beta

  # Backward through embedding dropout
  d_x_from_attn_and_pos = dropout_backward(d_x_from_attn, cache$embed_dropout_mask, dropout_rate)

  # Gradient for Positional Embeddings
  grads$d_P_emb = grads$d_P_emb + d_x_from_attn_and_pos

  # Gradient for Character Embeddings (C)
  for (i in 1:block_size) {
    char_idx = X_batch[1, i]
    grads$d_C[char_idx, ] = grads$d_C[char_idx, ] + d_x_from_attn_and_pos[i, ]
  }

  return(grads)
}

```

## Training Loop

This is where we train the model by picking one example sequence of tokens at a time (batch size is 1 in this implementation), calculating the loss, and updating the model parameters with their calculated gradients. Gradient clipping is applied here as a preventative measure to address the potential for the exploding gradient problem. Training and validation loss are printed out after every 10000 training examples. Training loss is very noisy since it is calculated on only one training example at a time, as compared to validation loss which is calculated over the full validation dataset.

```{r}
#| label: training-loop

# Training loop
for (iter in 1:n_iter) {
  # --- Training Step ---
  # Sample a random training example
  idx_train = sample(1:nrow(X_train), 1)
  X_batch_train = matrix(X_train[idx_train, ], nrow = 1)
  y_batch_train = matrix(y_train[idx_train, ], nrow = 1)

  # Forward pass (training mode)
  cache_train = transformer_forward(X_batch_train, y_batch_train, params, dropout_rate, is_training = TRUE)
  loss_train = cache_train$loss

  # Backward pass
  grads = transformer_backward(cache_train, X_batch_train, y_batch_train, params, dropout_rate)

  # Apply Gradient Clipping
  clipped_grads = clip_gradients_by_norm(grads, gradient_clip_threshold)
  # clipped_grads = grads # or not

  # Update parameters using clipped gradients
  for (p_name in names(params)) {
    # Check if a gradient for this parameter exists in clipped_grads
    grad_name = paste0("d_", p_name)
    if (!is.null(clipped_grads[[grad_name]])) {
      params[[p_name]] = params[[p_name]] - learning_rate * clipped_grads[[grad_name]]
    } else {
        # This case should ideally not be hit if `grads` contains all corresponding `d_` parameters
        # but included for robustness if `params` has entries not tracked by `grads`
        cat("Warning: No gradient found for parameter:", p_name, "\n")
    }
  }

  # --- Validation Step ---
  if (iter %% 10000 == 0) {
    # Calculate validation loss (inference mode, no dropout)
    val_losses = c()
    if (nrow(X_val) > 0) { # Check if validation set is not empty
      for (val_idx in 1:nrow(X_val)) {
        X_batch_val = matrix(X_val[val_idx, ], nrow = 1)
        y_batch_val = matrix(y_val[val_idx, ], nrow = 1)
        cache_val = transformer_forward(X_batch_val, y_batch_val, params, dropout_rate, is_training = FALSE)
        val_losses = c(val_losses, cache_val$loss)
      }
      avg_val_loss = mean(val_losses)
      cat("Iteration:", iter, " Training Loss:", round(loss_train, 4), " Validation Loss:", round(avg_val_loss, 4), "\n")
    } else {
      cat("Iteration:", iter, " Training Loss:", round(loss_train, 4), " (No validation data)\n")
    }
  }
}

cat("\nTraining complete. Final Training Loss:", round(loss_train, 4), "\n")
if (nrow(X_val) > 0) {
  cat("Final Average Validation Loss:", round(avg_val_loss, 4), "\n")
}

```

## Generating New Text

The text generation function will run in inference mode (is_training = FALSE) to ensure no dropout is applied. It can be fed a starting sequence, or will pad with blank spaces if the sequence is shorter than expected or empty. It generates output similar to what it is trained on, so even if the starting sequence is very different from the training content, the output will still look like the training content. In this case, the model was trained on Shakespeare, so the output will look like Shakespeare even if the generation is seeded with text that looks very different from Shakespeare.

```{r}
#| label: text-generation

generate_text = function(current_params, start_string, num_characters_to_generate) {
  generated_sequence_indices = encode(start_string)

  if (length(generated_sequence_indices) < block_size) {
    padded_start = c(rep(stoi[[" "]], block_size - length(generated_sequence_indices)), generated_sequence_indices)
    generated_sequence_indices = padded_start
    cat("Padded start string to block_size with spaces: '", decode(padded_start), "'\n", sep = "")
  }

  for (i in 1:num_characters_to_generate) {
    context_indices = tail(generated_sequence_indices, block_size)
    X_predict = matrix(context_indices, nrow = 1)

    # Simplified forward pass for inference (is_training = FALSE)
    x_emb_infer = t(sapply(1:block_size, function(j) current_params$C[X_predict[1, j], ]))
    x_infer = x_emb_infer + current_params$P_emb

    # No dropout applied during inference (dropout_forward handles this)
    x_infer = dropout_forward(x_infer, dropout_rate, is_training = FALSE)$out

    attn_out_infer = multi_head_attention_forward(x_infer, current_params$Wq, current_params$Wk, current_params$Wv, current_params$Wo,
                                                 n_head, head_size, current_params$ln1_gamma, current_params$ln1_beta,
                                                 dropout_rate, is_training = FALSE)$out
    x_infer = attn_out_infer

    ff_out_infer = feed_forward_forward(x_infer, current_params$W_ff1, current_params$b_ff1, current_params$W_ff2, current_params$b_ff2,
                                       current_params$ln2_gamma, current_params$ln2_beta,
                                       dropout_rate, is_training = FALSE)$out
    x_infer = ff_out_infer

    # Final linear layer to get logits for the *last* token in the sequence
    last_token_output_infer = matrix(x_infer[block_size, ], nrow = 1)
    logits_infer = last_token_output_infer %*% current_params$W_final + matrix(current_params$b_final, nrow = 1, ncol = vocab_size)

    probs_infer = softmax_matrix_rows(logits_infer)

    next_char_idx = sample.int(vocab_size, 1, prob = probs_infer)

    generated_sequence_indices = c(generated_sequence_indices, next_char_idx)
  }
  return(decode(generated_sequence_indices))
}

# Example usage: Generate new characters starting with a given string
start_seq = "Charizard is my favorite Pokemon."
num_to_generate = 1000

generated_text = generate_text(params, start_seq, num_to_generate)
cat("\nGenerated text starting with '", start_seq, "':\n", generated_text, "\n", sep = "")

```

We can see that while the language is not correct, the model is producing text structured very similarly to the input Shakespeare text. The validation loss achieved here is worse than what Karpathy achieves in his implementation, but this is due to the choice to simplify several aspects of the model (batch size of 1, only a single transformer block instead of 3 or 4 in sequence) to make it easier to implement everything from scratch and keep the matrices 2-dimensional for easier manual inspection in RStudio's global environment.

There are a number of improvements than could be made to this implementation. The most obvious ones are having a batch size greater than 1, and including multiple transformer blocks in sequence instead of just 1. Optimizing the weighting scheme in the gradient update to decay the learning rate or use some other algorithm like the Adam optimizer would also be a good choice. The reason this output doesn't look as impressive as ChatGPT comes down to the complexity of the model. In this implementation, the count of final model parameters is `r sum(sapply(params, length))`, which is very small in comparison to the count of 1.5 billion parameters in GPT-2 or 175 billion parameters in GPT-3.

## Attention Plot

After training the model, there are a number of interesting visualizations we can create. One is an attention plot, where we pick an input sequence and show how the attention layer creates weights to pay attention to prior tokens over that sequence. We can examine the input sequence "methinks" which is a word that exists commonly in the input Shakespeare text. At this point, we will call a couple of libraries just for the plotting functionality.

```{r}
#| label: attention-plot

library(tidyr)
library(ggplot2)
library(gridExtra)

attn_sequence = encode("methinks")
X_predict = matrix(attn_sequence, nrow = 1)

# Token and positional embedding
x_emb_infer = t(sapply(1:block_size, function(j) params$C[X_predict[1, j], ]))
x_infer = x_emb_infer + params$P_emb

# Layer normalization
ln_out_x = layer_norm_forward(x_infer, params$ln1_gamma, params$ln1_beta)
x = ln_out_x$out

# Attention block
Q_all = x %*% Wq
K_all = x %*% Wk

plots_per_head = lapply(1:n_head, function(h) {
  start_col = (h - 1) * head_size + 1
  end_col = h * head_size
  Q_h = Q_all[, start_col:end_col]
  K_h = K_all[, start_col:end_col]

  scores = (Q_h %*% t(K_h)) / sqrt(head_size)

  mask_causal = upper.tri(matrix(0, nrow(scores), ncol(scores)), diag = FALSE)
  scores[mask_causal] = -Inf

  attention_weights = softmax_matrix_rows(scores)
  
  # make plot
  df = as.data.frame(attention_weights)
  colnames(df) = c("m","e","t","h","i","n","k","s")
  df$row = c("m","e","t","h","i","n","k","s")
  df = tidyr::pivot_longer(df, -row, names_to = "col", values_to = "val")
  df$row = factor(df$row, levels = rev(c("m","e","t","h","i","n","k","s")))
  df$col = factor(df$col, levels = c("m","e","t","h","i","n","k","s"))
  p = ggplot(df, aes(x = col, y = row, fill = val)) +
      geom_tile() +
      scale_fill_viridis_c() +
      labs(x = "prior tokens in sequence", y = "input token",
           fill = "weights", title = paste0("Attention Head #", h)) +
      theme(plot.title = element_text(size = 11))
  
  # Return plot object to list output
  return(p)
  
})

# Display plots in 2x2 grid
grid.arrange(grobs = plots_per_head, ncol = 2)

# Save plot for thumbnail
g = arrangeGrob(grobs = plots_per_head, ncol = 2)
ggsave(file = "thumbnail.jpg", g, width = 6, height = 6/1.618)

```

Each of the four attention heads generates different weights during the self-attention process. The first token always gets a 100% weight since there are no other tokens at that point, but for later tokens we can see that the model has learned to pay attention (give higher weights to) tokens that are further back in the sequence, and which prior tokens are highly weighted differs from one attention head to the next, so they are providing meaningfully distinct information.

## Conclusion

While implementing everything from scratch without relying on any packages adds significant complexity to this model, it offers a detailed understanding of the core transformer architecture. The process of implementing some of this core functionality led to a deeper understanding of how and why certain calculations are what they are (See derivations in earlier sections regarding He initialization and Layer Normalization). This example serves as an educational tool for delving into the fundamental mechanisms of attention, normalization, and regularization within deep learning models, all without external dependencies.


