---
title: "Impact Evaluation"
description: "A summary of some topics in the field of impact evaluation from my volunteer work with the American Red Cross"
title-block-banner-color: white
image: "thumbnail.jpg"
draft: false
---

## Intro/Overview

One of the volunteer projects I worked on for the American Red Cross (ARC) was to help develop a cost benefit analysis for one of their programs. The International Federation of the Red Cross and Red Crescent Societies (IFRC) has a nice summary dashboard they maintain with high level information about some of their specific interventions: [https://go.ifrc.org/](https://go.ifrc.org/). 

One of the programs that IFRC supports is early action or anticipatory action. Unlike a responsive program that would wait for a disaster to occur and then provide aid afterwards, an anticipatory action program aims to provide preventative aid before the disaster occurs. This can be effective for certain types of disasters where we can reasonably predict the event in advance.

For example, we might predict a coming flood using data about river levels, weather, upstream water flow, etc. If we wait until after the flood occurs, the flooding could spread water-borne diseases among the affected population and a relief effort might involve providing medicine or other forms of healthcare. Alternatively, an anticipatory action might be to provide chlorine tablets or other water treatment options before the flood occurs so that water-borne disease does not become prevalent after the flood. If effective, this type of preventative action can allow an organization to help more people with the available budget, since these types of preventative actions are often more cost efficient than treating problems after they become more serious.

## Setting Up

### Loading Libraries

```{r}
#| label: loading-libraries
#| warning: false
#| output: false

library(lmtest) # to adjust model using clustered error covariance
library(sandwich) # to compute clustered error covariance
library(dplyr) # for data manipulation
library(tidyr) # for data manipulation
library(ggplot2) # to make plots
```

## Generating the Data

I will generate my own fake data to use as an example for explaining some general ideas. Continuing with the flood example, let us say that we want to evaluate the effectiveness of an anticipatory action (intervention) that aimed to reduce healthcare expenditures by providing chlorine tablets immediately before a flood event to help reduce the incidence of water-borne disease. Based on the available budget, the treatment can only be provided to part of the affected population. Some time after the event, we survey the affected population to see what their actual healthcare expenditures were. This is the data that we use to evaluate the impact of our program.

```{r}
#| label: generate-example-data
#| output: false

# set seed for consistency
set.seed(42)
df = data.frame()

# assume total population of 1000 households
n_households = 1000

# each household has 1-7 individuals
hh_size = sample.int(7, n_households, replace = TRUE)

# generate data for each individual
for(i in 1:n_households){
    hh = data.frame(
        hh_id = i, # household ID
        hh_size = hh_size[i], # household size
        age = sample.int(65, hh_size[i], replace=TRUE), # age from 1-65
        female = ifelse(runif(hh_size[i]) < 0.5, 1, 0)
    )
    df = rbind(df, hh)
}

# only 100 households receive the treatment
# and treatment is not purely random
# households that are larger, have more young or elderly,
# or are more female are given some preference
treatment_select = df %>% 
    group_by(hh_id, hh_size) %>%
    summarise(pct_young_old = mean((age <= 12) | (age >= 50)),
              pct_F = mean(female)) %>%
    mutate(treatment_prob = hh_size + 2*pct_young_old + 2*pct_F)

hh_id_treatment = sample(treatment_select$hh_id, 100, 
                         prob = treatment_select$treatment_prob)

df$treatment = ifelse(df$hh_id %in% hh_id_treatment, 1, 0)

# simulate actual healthcare costs
# hh_size increases costs because of increased exposure opportunity
# age young or old increases costs due to weaker immune systems
# treatment is effective and reduces costs
df$hc_cost = 50 + 5*df$hh_size + 20*((df$age <= 12) | (df$age >= 50)) - 20*df$treatment

# add cluster error and individual error
df_cluster_error = data.frame(hh_id = 1:n_households, 
                              e_household = rnorm(n_households, sd = 5))

df = df %>%
    inner_join(df_cluster_error, by = "hh_id") %>%
    mutate(e_individual = rnorm(nrow(df), sd = 5)) %>%
    mutate(hc_cost = hc_cost + e_household + e_individual)
```

## Average Treatment Effect

The metric we want to estimate is the average treatment effect (ATE). The ATE is the expected effect of the treatment on the target outcome. In our example, this would be the dollar amount by which the treatment reduces healthcare expenses for each individual, on average. Estimating this impact is important for evaluating an intervention because it tells us how effective the intervention was at achieving the desired outcome, and hopefully can be generalized to other events and inform decisions about whether to use this same intervention in future scenarios.

### Basic Calculation

The definition for ATE is the average difference in outcomes for treated vs non-treated individuals. More formally:

$$
    ATE = E[y_1 - y_0]
$$
Or, writing this in terms of an estimate from a sample:

$$
    \widehat{ATE} = \frac{1}{N} \sum_i (y_1(i) - y_0(i))
$$

The problem in both of these definitions is that we never observe both $y_1(i)$ and $y_0(i)$, because any one individual either receives the treatment or does not, so we cannot observe what happened to that same individual under both scenarios. So instead, the practical solution is to examine the difference in means between the treated and untreated groups, which should be a good estimate of the ATE under certain assumptions.

$$
    E[Y|X = 1] - E[Y|X = 0]
$$
```{r}
#| label: ATE-simple-manual

# calculate simple ATE by hand
EY1 = mean(df$hc_cost[df$treatment == 1])
EY0 = mean(df$hc_cost[df$treatment == 0])
ATE = EY1 - EY0
ATE
```

### Linear Regression

A simple regression yields the same estimate for the ATE, since using a binary indicator for the treatment is effectively the same as just taking the mean for each group. This is a useful approach if you want to do anything beyond getting this single point estimate.

```{r}
#| label: ATE-simple-regression

# estimate via simple linear model
fit = lm(hc_cost ~ treatment, data = df)
summary(fit)

# 95% confidence interval for ATE
confint(fit, "treatment", level = 0.95)

# same thing using the standard errors directly
coef(fit)[2] + qt(0.025, df=fit$df.residual) * summary(fit)$coefficients[2,2]
coef(fit)[2] + qt(0.975, df=fit$df.residual) * summary(fit)$coefficients[2,2]
```

The estimated coefficient on the treatment variable matches the manual calculation, as expected. The estimated ATE of about -17 means that receiving the treatment reduces healthcare expenditures by an average of $17 per person. However, this estimate does not quite match the true effect of -20 which is the effect I used to generate the data, and the 95% confidence interval does not include the true effect either. This is because of some additional complications I added while generating the data which will need to be addressed.

## Additional Considerations

The simple estimate for ATE above is useful to look at, but there are usually more factors that need to be considered in this type of analysis. These will differ substantially based on the situation. Here we will examine a few that are relevant to this example.

### Covariate Adjustment

A confounding factor is a variable that influences both the dependent variable and independent variable. To estimate the effect of X on Y, we must suppress the effects of confounding variables that influence both X and Y. We say that X and Y are confounded by some other variable Z whenever Z causally influences both X and Y.

![](confounding.png)

In this example, there are variables such as household size and age (Z) which influence both the treatment effect (X) and healthcare expenditures (Y). They influence treatment because treatment was not assigned randomly; the assignment was weighted to prefer larger households and households with young and elderly members. They have a direct effect on healthcare costs based on how those costs were simulated in this example.

We can account for these factors by explicitly estimating their effects in the model. Conditioning our estimate on these additional variables is sometimes called covariate adjustment. Another way to think about this is that the simple version of the model suffers from omitted variable bias and the previous estimate of the ATE was incorporating effects which were actually attributable to these confounding factors.

```{r}
#| label: ATE-covariate-adjustment

# add indicator for young/elderly age group
df$age_young_old = ifelse((df$age <= 12) | (df$age >= 50), 1, 0)

# estimate via simple linear model
fit = lm(hc_cost ~ treatment + hh_size + age_young_old, data = df)
summary(fit)

# 95% confidence interval for ATE
confint(fit, "treatment", level = 0.95)
```

After adjusting for these confounding factors, our ATE estimate is much closer to the true value and the 95% confidence interval includes the true value.

### Inverse Probability Treatment Weights

Another method for addressing this concern around confounding is through the use of propensity scores, or inverse probability treatment weights. This is a popular method for observational studies. Unlike a randomized controlled trial (RCT) where an experiment is designed in advance to have treatment and control groups balanced across other covariates of interest, an observational study has no control over the treatment assignment and will often have treatment and control groups which are not balanced across other covariates. This technique is essentially re-weighting the observational data to make it look more it came from a balanced RCT design, which is important for estimating ATE since the calculation assumes we are comparing similar individuals.

This method starts by building a model to predict a propensity score, which is the probability that the treatment was assigned to an individual. Observations are then re-weighted by dividing by this probability in order to achieve better balance.

```{r}
#| label: ATE-propensity-score

# estimate propensity score
fit_ps = glm(treatment ~ hh_size + age_young_old + female, data = df, family = binomial())
ehat = predict(fit_ps, type = "response")
df$ipw = df$treatment/ehat + (1-df$treatment)/(1-ehat)

# estimate ATE using weighted linear model
fit = lm(hc_cost ~ treatment, data = df, weights = ipw)
summary(fit)

# 95% confidence interval for ATE
confint(fit, "treatment", level = 0.95)
```

This is an improvement over the un-adjusted estimate. One additional diagnostic we can examine here is the extent to which this re-weighting improved covariate balance. This is often done by comparing Standardized Mean Differences (SMD) for the covariates of interest before and after adjustment using these weights. A common rule of thumb for good balance is a SMD threshold of 0.1, although this rule is somewhat arbitrary.

```{r}
#| label: covariate-balance

# calculate SMD for covariates of interest
cov_bal_smd = df %>%
    group_by(treatment) %>%
    summarise(hh_size_mean = mean(hh_size),
              hh_size_wmean = weighted.mean(hh_size, ipw),
              hh_size_sd = sd(hh_size),
              age_mean = mean(age_young_old),
              age_wmean = weighted.mean(age_young_old, ipw),
              female_mean = mean(female),
              female_wmean = weighted.mean(female, ipw)) %>%
    summarise(hh_size_unadjusted = abs(diff(hh_size_mean)) / sqrt(sum(hh_size_sd^2)/2),
              hh_size_adjusted = abs(diff(hh_size_wmean)) / sqrt(sum(hh_size_sd^2)/2),
              age_unadjusted = abs(diff(age_mean)),
              age_adjusted = abs(diff(age_wmean)),
              female_unadjusted = abs(diff(female_mean)),
              female_adjusted = abs(diff(female_wmean))) %>%
    pivot_longer(everything(), values_to = "SMD") %>%
    mutate(adjusted = ifelse(grepl("unadjusted", name), "unadjusted", "adjusted")) %>%
    mutate(covariate = sub("_[^_]*$", "", name))

# make love plot
cov_bal_smd %>%
    mutate(adjusted = as.factor(adjusted)) %>%
    ggplot(aes(x = SMD, y = covariate, color = adjusted, shape = adjusted)) +
    geom_vline(xintercept = 0) +
    geom_vline(xintercept = 0.1, lty = "dashed") +
    geom_point(size = 3) +
    labs(x = "Absolute Standardized Mean Differences", y = "Covariate") +
    theme(legend.title = element_blank())
```

We can see that covariate balance as measured by SMD improves after re-weighting the data with the inverse probability weights. The balance for household size is under the threshold of 0.1 after re-weighting, and the balance for the other covariates has also improved although they were already under the 0.1 threshold.

### Clustered Standard Errors

There are certain situations where we might expect to observe clustering in our data. In this fake example, we have created clustering in the errors by adding an error component at the household level as well as the individual level. The rationale is that people in the same household will tend to have similar experiences. For instance, if one person from a household gets sick they are likely to pass that sickness to the other members of the household.

This clustering is present in the treatment assignment as well. The treatment is assigned to an entire household, not to specific individuals. This type of study design is similar to a cluster randomized trial where treatment is assigned at a more aggregate level even though outcomes are measured at an individual level. There are a number of reasons why this design might be desirable. In this fake example, if we had a family of two parents and two young children and randomly decided to only give the treatment to the parents, it would not be reasonable to expect them to follow the study design and treat only their own drinking water and let their children drink dirty water. They would likely give their treatment to their children, causing spillover effects which would reduce the power of our estimates.

When clustering exists in the data it means that the errors are not all independent since there is dependence within each cluster. Observations within the same cluster (household) are similar to each other. This means the standard errors estimated by the model are too small and we need to adjust them to make sure our estimates are not overconfident. The adjustment will increase the standard errors, resulting in wider confidence intervals for our ATE estimate.

```{r}
#| label: clustered-standard-errors

# first model using covariate adjustment
fit1 = lm(hc_cost ~ treatment + hh_size + age_young_old, data = df)
fit1_coef_cl = coeftest(fit1, vcov = vcovCL, cluster = ~hh_id)
fit1_coef_cl

# 95% confidence interval for ATE
df_clust_ci = data.frame(
    model = "covariate adjustment",
    clustered_std_err = "no",
    ci_95_lower = coef(fit1)[2] + qt(0.025, df=fit1$df.residual) * summary(fit1)$coefficients[2,2],
    ci_95_upper = coef(fit1)[2] + qt(0.975, df=fit1$df.residual) * summary(fit1)$coefficients[2,2]
)
df_clust_ci = rbind(df_clust_ci, data.frame(
    model = "covariate adjustment",
    clustered_std_err = "yes",
    ci_95_lower = coef(fit1)[2] + qt(0.025, df=fit1$df.residual) * fit1_coef_cl[2,2],
    ci_95_upper = coef(fit1)[2] + qt(0.975, df=fit1$df.residual) * fit1_coef_cl[2,2]
))

# second model using inverse probability weights
fit2 = lm(hc_cost ~ treatment, data = df, weights = ipw)
fit2_coef_cl = coeftest(fit2, vcov = vcovCL, cluster = ~hh_id)
fit2_coef_cl

# 95% confidence interval for ATE
df_clust_ci = rbind(df_clust_ci, data.frame(
    model = "inverse probability weights",
    clustered_std_err = "no",
    ci_95_lower = coef(fit2)[2] + qt(0.025, df=fit2$df.residual) * summary(fit2)$coefficients[2,2],
    ci_95_upper = coef(fit2)[2] + qt(0.975, df=fit2$df.residual) * summary(fit2)$coefficients[2,2]
))
df_clust_ci = rbind(df_clust_ci, data.frame(
    model = "inverse probability weights",
    clustered_std_err = "yes",
    ci_95_lower = coef(fit2)[2] + qt(0.025, df=fit2$df.residual) * fit2_coef_cl[2,2],
    ci_95_upper = coef(fit2)[2] + qt(0.975, df=fit2$df.residual) * fit2_coef_cl[2,2]
))

# display table
rownames(df_clust_ci) = NULL
knitr::kable(df_clust_ci)
```

As expected, accounting for the clustering in the errors increases the estimate for the standard error of ATE and consequently widens the bounds of the associated 95% confidence interval. It makes our estimate less precise, but it is a more accurate representation of what our confidence in our estimate should be.

## Conclusion

We discussed estimating ATE to evaluate the impact of an intervention and covered a few examples of the problems that can arise in this type of analysis. Of course, there are a lot of other problems that can arise and techniques available to address them which I have not covered here. The World Bank provides a general description of more of the common techniques for impact evaluations in their publication [here](https://www.worldbank.org/en/programs/sief-trust-fund/publication/impact-evaluation-in-practice) which the interested reader can download for free. Also, the use case I examined was focused on using observational data, but another approach would be to try to run the analysis concurrently with the intervention and have an RCT study design. The [Abdul Latif Jameel Poverty Action Lab (J-PAL)](https://www.povertyactionlab.org/research-resources?view=toc) has a lot of good resources on the subject, as well as impact evaluations in general. 
