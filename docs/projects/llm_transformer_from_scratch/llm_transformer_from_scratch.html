<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="description" content="An exercise in understanding the mechanics of the transformer architechture used in Large Language Models (LLMs)">

<title>Nicholas Burk - LLM Transformer Implemented in Base R</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Nicholas Burk</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../index.html" rel="" target="">
 <span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../projects.html" rel="" target="">
 <span class="menu-text">Projects</span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#introoverview" id="toc-introoverview" class="nav-link active" data-scroll-target="#introoverview">Intro/Overview</a></li>
  <li><a href="#setting-up" id="toc-setting-up" class="nav-link" data-scroll-target="#setting-up">Setting Up</a>
  <ul class="collapse">
  <li><a href="#data-preparation" id="toc-data-preparation" class="nav-link" data-scroll-target="#data-preparation">Data Preparation</a></li>
  </ul></li>
  <li><a href="#hyperparameters-and-model-architecture" id="toc-hyperparameters-and-model-architecture" class="nav-link" data-scroll-target="#hyperparameters-and-model-architecture">Hyperparameters and Model Architecture</a>
  <ul class="collapse">
  <li><a href="#training-and-validation-split" id="toc-training-and-validation-split" class="nav-link" data-scroll-target="#training-and-validation-split">Training and Validation Split</a></li>
  <li><a href="#core-utility-functions" id="toc-core-utility-functions" class="nav-link" data-scroll-target="#core-utility-functions">Core Utility Functions</a></li>
  <li><a href="#model-parameters-initialization" id="toc-model-parameters-initialization" class="nav-link" data-scroll-target="#model-parameters-initialization">Model Parameters Initialization</a></li>
  </ul></li>
  <li><a href="#transformer-block-implementation-functions" id="toc-transformer-block-implementation-functions" class="nav-link" data-scroll-target="#transformer-block-implementation-functions">Transformer Block Implementation Functions</a>
  <ul class="collapse">
  <li><a href="#multi-head-self-attention" id="toc-multi-head-self-attention" class="nav-link" data-scroll-target="#multi-head-self-attention">Multi-Head Self-Attention</a></li>
  <li><a href="#position-wise-feed-forward-network" id="toc-position-wise-feed-forward-network" class="nav-link" data-scroll-target="#position-wise-feed-forward-network">Position-wise Feed-Forward Network</a></li>
  <li><a href="#transformer-model-forward-pass-and-loss" id="toc-transformer-model-forward-pass-and-loss" class="nav-link" data-scroll-target="#transformer-model-forward-pass-and-loss">Transformer Model Forward Pass and Loss</a></li>
  <li><a href="#transformer-model-backward-pass" id="toc-transformer-model-backward-pass" class="nav-link" data-scroll-target="#transformer-model-backward-pass">Transformer Model Backward Pass</a></li>
  </ul></li>
  <li><a href="#training-loop" id="toc-training-loop" class="nav-link" data-scroll-target="#training-loop">Training Loop</a></li>
  <li><a href="#generating-new-text" id="toc-generating-new-text" class="nav-link" data-scroll-target="#generating-new-text">Generating New Text</a></li>
  <li><a href="#attention-plot" id="toc-attention-plot" class="nav-link" data-scroll-target="#attention-plot">Attention Plot</a></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion">Conclusion</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">LLM Transformer Implemented in Base R</h1>
</div>

<div>
  <div class="description">
    An exercise in understanding the mechanics of the transformer architechture used in Large Language Models (LLMs)
  </div>
</div>


<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<section id="introoverview" class="level2">
<h2 class="anchored" data-anchor-id="introoverview">Intro/Overview</h2>
<p>One of the core components in AI models like ChatGPT is a language model. These language models are usually focused on predicting the next token in a sequence, and are used to generate language. For example, if the tokens are characters then given the sequence “orang” the model should predict the next character is “e”. One of the major innovations for these models is the attention mechanism introduced by Vaswani, Ashish, et al.(2017) in their paper “Attention is all you need”. This model architecture is called a transformer, and is what is used in the GPT (Generative Pre-trained Transformer) models in ChatGPT.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="fig1.jpg" class="img-fluid figure-img"></p>
</figure>
</div>
<p>In the following work, I will be implementing the transformer architecture from scratch in base R. The figure above has both encoder and decoder blocks as it is the architecture used for translation tasks, but I will only be implementing the decoder block on the right part of the figure without the encoder block embedding section, since that is not necessary for the next token prediction task.</p>
<p>My work heavily references <a href="https://www.youtube.com/watch?v=kCc8FmEb1nY">this YouTube video</a> from Andrej Karpathy, as well as his related <a href="https://github.com/karpathy/minGPT">minGPT github project</a>. Karpathy is well known for developing and teaching the first deep learning course at Stanford, CS 231n: Convolutional Neural Networks for Visual Recognition. His materials break down the core mechanics of the transformer model architecture using the pytorch library in Python. My implementation will differ by being built in R (which I am more familiar with than Python) as well as not relying on any libraries and implementing the backpropogation from scratch. This will make it slower and less efficient than Karpathy’s implementation which relies on the pytorch library to handle the backpropogation, but will help improve my understanding of the full process.</p>
<p>Full disclosure, around 80% of the content in the code blocks was constructed with assistance from an AI model: Google’s Gemini 2.5 flash. I have implemented feed forward neural network models before, but this example is more complex than the proof of concepts I usually work on and Gemini was very helpful in figuring out how to structure the various functions and pass parameter information back and forth between them.</p>
</section>
<section id="setting-up" class="level2">
<h2 class="anchored" data-anchor-id="setting-up">Setting Up</h2>
<section id="data-preparation" class="level3">
<h3 class="anchored" data-anchor-id="data-preparation">Data Preparation</h3>
<p>Since the intention is to follow Karpathy’s example it makes the most sense to use the same dataset, which is a text file containing the complete works of William Shakespeare. The original file can be found on the <a href="https://www.gutenberg.org/">Project Gutenberg website</a>, but the file Karpathy uses is much cleaner so it is easier to use <a href="https://github.com/karpathy/ng-video-lecture/blob/master/input.txt">his file</a> directly.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># text corpus</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>text_corpus <span class="ot">=</span> <span class="fu">readLines</span>(<span class="st">"input.txt"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Warning in readLines("input.txt"): incomplete final line found on 'input.txt'</code></pre>
</div>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>text_corpus <span class="ot">=</span> <span class="fu">paste</span>(text_corpus, <span class="at">collapse =</span> <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a vocabulary of unique characters</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>chars <span class="ot">=</span> <span class="fu">sort</span>(<span class="fu">unique</span>(<span class="fu">strsplit</span>(text_corpus, <span class="st">""</span>)[[<span class="dv">1</span>]]))</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>vocab_size <span class="ot">=</span> <span class="fu">length</span>(chars)</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a mapping from character to integer (stoi: string_to_int)</span></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>stoi <span class="ot">=</span> <span class="fu">setNames</span>(<span class="dv">1</span><span class="sc">:</span>vocab_size, chars)</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a mapping from integer to character (itos: int_to_string)</span></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>itos <span class="ot">=</span> <span class="fu">setNames</span>(chars, <span class="dv">1</span><span class="sc">:</span>vocab_size)</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Helper functions for encoding and decoding</span></span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>encode <span class="ot">=</span> <span class="cf">function</span>(s) {</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>  <span class="fu">sapply</span>(<span class="fu">strsplit</span>(s, <span class="st">""</span>)[[<span class="dv">1</span>]], <span class="cf">function</span>(char) stoi[[char]])</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>decode <span class="ot">=</span> <span class="cf">function</span>(l) {</span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a>  <span class="fu">paste</span>(<span class="fu">sapply</span>(l, <span class="cf">function</span>(idx) itos[[<span class="fu">as.character</span>(idx)]]), <span class="at">collapse =</span> <span class="st">""</span>)</span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"Vocabulary size:"</span>, vocab_size, <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Vocabulary size: 65 </code></pre>
</div>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"Character to index mapping (first 5):"</span>, <span class="fu">paste</span>(<span class="fu">names</span>(<span class="fu">head</span>(stoi, <span class="dv">5</span>)), <span class="fu">head</span>(stoi, <span class="dv">5</span>), <span class="at">sep =</span> <span class="st">":"</span>, <span class="at">collapse =</span> <span class="st">", "</span>), <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Character to index mapping (first 5): ':1, -:2,  :3, 
:4, !:5 </code></pre>
</div>
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"Example encoding of 'hello':"</span>, <span class="fu">encode</span>(<span class="st">"hello"</span>), <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Example encoding of 'hello': 28 22 36 36 42 </code></pre>
</div>
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"Example decoding of [1 2 3 4 5]:"</span>, <span class="fu">decode</span>(<span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">5</span>)), <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Example decoding of [1 2 3 4 5]: '- 
! </code></pre>
</div>
</div>
</section>
</section>
<section id="hyperparameters-and-model-architecture" class="level2">
<h2 class="anchored" data-anchor-id="hyperparameters-and-model-architecture">Hyperparameters and Model Architecture</h2>
<p>Usually in a transformer model, one layer of multi-headed attention will be followed by one feed forward layer, and that will constitute one transformer block. Then there will be multiple transformer blocks in sequence that make up the model. In this example, only a single transformer block will be used for simplicity, since keeping track of all the parameters for the gradient update is already complex, even for only one block.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>block_size <span class="ot">=</span> <span class="dv">8</span>    <span class="co"># how many characters we consider for prediction</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>n_embd <span class="ot">=</span> <span class="dv">16</span>       <span class="co"># embedding dimension, also model dimension (d_model)</span></span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>n_head <span class="ot">=</span> <span class="dv">4</span>        <span class="co"># number of attention heads</span></span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>head_size <span class="ot">=</span> n_embd <span class="sc">/</span> n_head <span class="co"># dimension of each attention head</span></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>learning_rate <span class="ot">=</span> <span class="fl">0.01</span></span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>n_iter <span class="ot">=</span> <span class="dv">100000</span>     <span class="co"># number of training iterations (more needed for real performance)</span></span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>dropout_rate <span class="ot">=</span> <span class="fl">0.10</span> <span class="co"># Dropout probability (e.g., 0.10 means 10% of neurons dropped)</span></span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>gradient_clip_threshold <span class="ot">=</span> <span class="fl">1.0</span> <span class="co"># Gradient clipping threshold</span></span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Batch size is 1 for simplicity in this from-scratch example.</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<section id="training-and-validation-split" class="level3">
<h3 class="anchored" data-anchor-id="training-and-validation-split">Training and Validation Split</h3>
<p>The data is split into 90% training data and 10% validation data. One important note is that the first 90% of the file is used as training while the last 10% is used for validation, there is no random sampling in creating this partition.</p>
<p>This is a deliberate choice due to the autoregressive nature of this model, if we used simple random sampling to make this split, we could lose the independence between training and validation by training on very similar data to what would exist in validation. As a simple example, let’s pretend we have a context window of 5 characters and the word “incredible” is in the dataset. We could end up with a case like this:</p>
<ul>
<li>input: “incr”, target: “e”, split: training</li>
<li>input: “ncre”, target: “d”, split: validation</li>
<li>input: “cred”, target: “i”, split: training</li>
</ul>
<p>The problem in this case is that the model has already been trained on most of that string already, so when it sees it in validation the performance will look similar to performance on the training set because the data is similar, and not because the model is actually generalizing well.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create training examples (X, y)</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>data_encoded <span class="ot">=</span> <span class="fu">encode</span>(text_corpus)</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>X_data <span class="ot">=</span> <span class="fu">matrix</span>(<span class="cn">NA</span>, <span class="at">nrow =</span> <span class="fu">length</span>(data_encoded) <span class="sc">-</span> block_size, <span class="at">ncol =</span> block_size)</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>y_data <span class="ot">=</span> <span class="fu">matrix</span>(<span class="cn">NA</span>, <span class="at">nrow =</span> <span class="fu">length</span>(data_encoded) <span class="sc">-</span> block_size, <span class="at">ncol =</span> block_size)</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>(<span class="fu">length</span>(data_encoded) <span class="sc">-</span> block_size)) {</span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>  X_data[i, ] <span class="ot">=</span> data_encoded[i<span class="sc">:</span>(i <span class="sc">+</span> block_size <span class="sc">-</span> <span class="dv">1</span>)]</span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a>  y_data[i, ] <span class="ot">=</span> data_encoded[(i <span class="sc">+</span> <span class="dv">1</span>)<span class="sc">:</span>(i <span class="sc">+</span> block_size)]</span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a><span class="co"># create 90%/10% train/validation split</span></span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a>num_samples <span class="ot">=</span> <span class="fu">nrow</span>(X_data)</span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a>train_indices <span class="ot">=</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">floor</span>(<span class="fl">0.9</span> <span class="sc">*</span> num_samples)</span>
<span id="cb12-15"><a href="#cb12-15" aria-hidden="true" tabindex="-1"></a>val_indices <span class="ot">=</span> <span class="fu">setdiff</span>(<span class="dv">1</span><span class="sc">:</span>num_samples, train_indices)</span>
<span id="cb12-16"><a href="#cb12-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-17"><a href="#cb12-17" aria-hidden="true" tabindex="-1"></a><span class="co"># debugging, try to intentionally overfit</span></span>
<span id="cb12-18"><a href="#cb12-18" aria-hidden="true" tabindex="-1"></a><span class="co"># train_indices = (1:20)*20</span></span>
<span id="cb12-19"><a href="#cb12-19" aria-hidden="true" tabindex="-1"></a><span class="co"># val_indices = (1:20)*20</span></span>
<span id="cb12-20"><a href="#cb12-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-21"><a href="#cb12-21" aria-hidden="true" tabindex="-1"></a>X_train <span class="ot">=</span> X_data[train_indices, ]</span>
<span id="cb12-22"><a href="#cb12-22" aria-hidden="true" tabindex="-1"></a>y_train <span class="ot">=</span> y_data[train_indices, ]</span>
<span id="cb12-23"><a href="#cb12-23" aria-hidden="true" tabindex="-1"></a>X_val <span class="ot">=</span> X_data[val_indices, ]</span>
<span id="cb12-24"><a href="#cb12-24" aria-hidden="true" tabindex="-1"></a>y_val <span class="ot">=</span> y_data[val_indices, ]</span>
<span id="cb12-25"><a href="#cb12-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-26"><a href="#cb12-26" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"Shape of X_train:"</span>, <span class="fu">dim</span>(X_train), <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Shape of X_train: 1003846 8 </code></pre>
</div>
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"Shape of y_train:"</span>, <span class="fu">dim</span>(y_train), <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Shape of y_train: 1003846 8 </code></pre>
</div>
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"Shape of X_val:"</span>, <span class="fu">dim</span>(X_val), <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Shape of X_val: 111539 8 </code></pre>
</div>
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"Shape of y_val:"</span>, <span class="fu">dim</span>(y_val), <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Shape of y_val: 111539 8 </code></pre>
</div>
</div>
</section>
<section id="core-utility-functions" class="level3">
<h3 class="anchored" data-anchor-id="core-utility-functions">Core Utility Functions</h3>
<p>These are helper functions that handle things like the softmax operation, neuron activation, dropout, etc. Most operations are being applied to matrix objects and we want the output to also be a matrix. There are a lot of scenarios where base R functions want to simplify the data type of the output and may try to return a vector or something else when the input was a matrix, so the matrix() function is used heavily here to ensure the output is still a matrix object.</p>
<section id="layer-normalization" class="level4">
<h4 class="anchored" data-anchor-id="layer-normalization">Layer Normalization</h4>
<p>I struggled a lot with the implementation of layer normalization, so it is worth adding some additional explanation here around some aspects I found confusing.</p>
<p>Let’s assume the input data to LayerNorm is <span class="math inline">\(X \in R^{N \times D}\)</span>, where each row <span class="math inline">\(X_i \in R^D\)</span> is a sample. LayerNorm operates independently on each sample, so for an example of the forward pass we drop the batch index <span class="math inline">\(i\)</span> while focusing on a single vector <span class="math inline">\(x \in R^D\)</span>.</p>
<p><strong>Forward Pass:</strong></p>
<p><span class="math display">\[
\begin{aligned}
\mu &amp;= \frac{1}{D} \sum_{j=1}^D x_j \\
\sigma^2 &amp;= \frac{1}{D} \sum_{j=1}^D (x_j - \mu)^2 \\
\hat{x_j} &amp;= \frac{x_j - \mu}{\sqrt{\sigma^2 + \epsilon}} \\
y_j &amp;= \gamma_j \hat{x_j} + \beta_j
\end{aligned}
\]</span></p>
<p>The <span class="math inline">\(y_j\)</span> elements are the output of LayerNorm. An important observation is that the third line above shows the normalization, which is happening over the rows of <span class="math inline">\(X\)</span>. Note how the first two lines calculating <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma^2\)</span> are over the dimension of the columns <span class="math inline">\(D\)</span>, so each row has its own mean and variance and is being normalized. However, the affine transformation in the fourth line shows parameters <span class="math inline">\(\gamma_j\)</span> and <span class="math inline">\(\beta_j\)</span> with the index <span class="math inline">\(j\)</span> over the column space, so this last transformation is a scale and shift over the columns.</p>
<p>I also had trouble understanding the backward pass for LayerNorm. The gradients for <span class="math inline">\(\gamma\)</span> and <span class="math inline">\(\beta\)</span> are fairly straightforward since they are just an affine transformation at the end, but passing the gradient back through to the input data was tricky, and this difference between transformations over rows vs columns complicates things somewhat.</p>
<p><strong>Backward Pass:</strong></p>
<p>We want <span class="math inline">\(\frac{dL}{dx_j}\)</span>, the gradient for the input row data.</p>
<p>Using the chain rule:</p>
<p><span class="math display">\[
\frac{dL}{dx_j} = \sum_k \frac{dL}{d\hat{x_k}} \frac{d\hat{x_k}}{dx_j}
\]</span></p>
<p>where <span class="math inline">\(k\)</span> is a summation index (over columns) we are introducing to iterate over the columns, while <span class="math inline">\(j\)</span> is the specific column index of the individual element within the row vector <span class="math inline">\(x\)</span> that we are differentiating with respect to.</p>
<p>Computing <span class="math inline">\(\frac{d \hat{x_k}}{dx_j}\)</span>:</p>
<p>recall that:</p>
<p><span class="math display">\[
\begin{aligned}
\mu &amp;= \frac{1}{D} \sum_{k=1}^D x_k \\
\sigma^2 &amp;= \frac{1}{D} \sum_{k=1}^D (x_k - \mu)^2 \\
\hat{x_k} &amp;= \frac{x_k - \mu}{\sqrt{\sigma^2 + \epsilon}}
\end{aligned}
\]</span></p>
<p>This has two dependencies, one directly through <span class="math inline">\(x_k\)</span> as each element of <span class="math inline">\(x_k\)</span> is <span class="math inline">\(x_j\)</span> when <span class="math inline">\(k=j\)</span>, and also indirectly through <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma^2\)</span> which are functions of <span class="math inline">\(x_j\)</span>.</p>
<p>For the mean <span class="math inline">\(\mu\)</span>:</p>
<p><span class="math display">\[
\frac{d\mu}{dx_j} = \frac{1}{D}
\]</span></p>
<p>For the variance <span class="math inline">\(\sigma^2\)</span>:</p>
<p><span class="math display">\[
\begin{aligned}
\frac{d\sigma^2}{dx_j} &amp;= \frac{1}{D} \sum_{k=1}^D \frac{d}{dx_j} (x_k - \mu)^2 \\
&amp;= \frac{1}{D} \sum_{k=1}^D 2(x_k - \mu) \frac{d}{dx_j} (x_k - \mu) \\
&amp;= \frac{2}{D} \sum_{k=1}^D (x_k - \mu) (\delta_{kj} - \frac{1}{D})
\end{aligned}
\]</span></p>
<p>Note that <span class="math inline">\(\frac{dx_k}{dx_j} = \delta_{kj}\)</span> (1 if k=j, 0 otherwise)</p>
<p>Now we have to think of that summation in 2 parts. First, when k=j, then <span class="math inline">\(\delta_{kj} = 1\)</span> and we have:</p>
<p><span class="math display">\[
(x_k - \mu) (\delta_{kj} - \frac{1}{D}) = (x_j - \mu) (1 - \frac{1}{D})
\]</span></p>
<p>Then for the other terms where <span class="math inline">\(k \ne j\)</span>:</p>
<p><span class="math display">\[
\sum_{k \ne j} (x_k - \mu) (\delta_{kj} - \frac{1}{D}) = - \frac{1}{D} \sum_{k \ne j} (x_k - \mu)
\]</span></p>
<p>However, this second term can be simplified because summing over a vector minus its mean is zero.</p>
<p><span class="math display">\[
\sum_{k=1}^D (x_k - \mu) = 0 \\
\sum_{k \ne j} (x_k - \mu) = -(x_j - \mu)
\]</span></p>
<p>So the second term becomes</p>
<p><span class="math display">\[
- \frac{1}{D} * -(x_j - \mu) = \frac{1}{D} (x_j - \mu)
\]</span> then putting both parts back together gives us</p>
<p><span class="math display">\[
\begin{aligned}
\frac{d\sigma^2}{dx_j} &amp;= \frac{2}{D}((x_j - \mu) (1 - \frac{1}{D}) + \frac{1}{D} (x_j - \mu)) \\
&amp;= \frac{2}{D}(x_j - \mu)
\end{aligned}
\]</span></p>
<p>Now we can use the results for <span class="math inline">\(\frac{d\mu}{dx_j}\)</span> and <span class="math inline">\(\frac{d\sigma^2}{dx_j}\)</span> to calculate <span class="math inline">\(\frac{d\hat{x_k}}{dx_j}\)</span>. Using the quotient rule:</p>
<p><span class="math display">\[
\begin{aligned}
\frac{d\hat{x_k}}{dx_j} &amp;= (\sqrt{\sigma^2 + \epsilon} \frac{d}{dx_j} [(x_k - \mu)] - (x_k - \mu) \frac{d}{dx_j} [\sqrt{\sigma^2 + \epsilon}]) (\sigma^2 + \epsilon)^{-1} \\
&amp;= (\sigma^2 + \epsilon)^{-1/2}(\delta_{kj} - \frac{1}{D}) - (x_k - \mu) \frac{1}{2}(\sigma^2 + \epsilon)^{-3/2} \frac{d\sigma^2}{dx_j} \\
&amp;= (\sigma^2 + \epsilon)^{-1/2}(\delta_{kj} - \frac{1}{D}) - (x_k - \mu) \frac{1}{2}(\sigma^2 + \epsilon)^{-3/2} \frac{2}{D}(x_j - \mu) \\
&amp;= \frac{1}{\sqrt{\sigma^2 + \epsilon}} [\delta_{kj} - \frac{1}{D} - \frac{(x_k - \mu)(x_j - \mu)}{(\sigma^2 + \epsilon)D}]
\end{aligned}
\]</span></p>
<p>Finally, we can plug this back into the first equation for the gradient we are after. To simplify the notation slightly, we will say <span class="math inline">\(\frac{dL}{d\hat{x_k}} = \hat{g_k}\)</span></p>
<p><span class="math display">\[
\begin{aligned}
\frac{dL}{dx_j} &amp;= \sum_k \frac{dL}{d\hat{x_k}} \frac{d\hat{x_k}}{dx_j} \\
&amp;= \sum_k \hat{g_k} \frac{d\hat{x_k}}{dx_j} \\
&amp;= \sum_k \hat{g_k} \frac{1}{\sqrt{\sigma^2 + \epsilon}} [\delta_{kj} - \frac{1}{D} - \frac{(x_k - \mu)(x_j - \mu)}{(\sigma^2 + \epsilon)D}] \\
&amp;= \frac{1}{\sqrt{\sigma^2 + \epsilon}} (\hat{g_j} - \frac{1}{D} \sum_k \hat{g_k} - \frac{(x_j - \mu)}{(\sigma^2 + \epsilon)D} \sum_k \hat{g_k} (x_k - \mu)
\end{aligned}
\]</span></p>
<p>If you look at the layer norm backward function implemented below, you can see that the above formula is what is used to calculate the gradient d_X.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Softmax activation function (row-wise for matrices)</span></span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>softmax_matrix_rows <span class="ot">=</span> <span class="cf">function</span>(X) {</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">t</span>(<span class="fu">apply</span>(X, <span class="dv">1</span>, <span class="cf">function</span>(row) {</span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>    exp_row <span class="ot">=</span> <span class="fu">exp</span>(row <span class="sc">-</span> <span class="fu">max</span>(row)) <span class="co"># Numerical stability</span></span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a>    <span class="fu">return</span>(exp_row <span class="sc">/</span> <span class="fu">sum</span>(exp_row))</span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a>  })) <span class="co"># wrapped with t() to transpose back to original shape</span></span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Layer Normalization</span></span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a>layer_norm_forward <span class="ot">=</span> <span class="cf">function</span>(X, gamma, beta, <span class="at">epsilon =</span> <span class="fl">1e-5</span>) {</span>
<span id="cb20-11"><a href="#cb20-11" aria-hidden="true" tabindex="-1"></a>  mean_X <span class="ot">=</span> <span class="fu">apply</span>(X, <span class="dv">1</span>, mean)</span>
<span id="cb20-12"><a href="#cb20-12" aria-hidden="true" tabindex="-1"></a>  var_X <span class="ot">=</span> <span class="fu">apply</span>(X, <span class="dv">1</span>, var)</span>
<span id="cb20-13"><a href="#cb20-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-14"><a href="#cb20-14" aria-hidden="true" tabindex="-1"></a>  X_norm <span class="ot">=</span> (X <span class="sc">-</span> <span class="fu">matrix</span>(mean_X, <span class="at">nrow =</span> <span class="fu">nrow</span>(X), <span class="at">ncol =</span> <span class="fu">ncol</span>(X), <span class="at">byrow =</span> <span class="cn">FALSE</span>)) <span class="sc">/</span></span>
<span id="cb20-15"><a href="#cb20-15" aria-hidden="true" tabindex="-1"></a>           <span class="fu">sqrt</span>(<span class="fu">matrix</span>(var_X <span class="sc">+</span> epsilon, <span class="at">nrow =</span> <span class="fu">nrow</span>(X), <span class="at">ncol =</span> <span class="fu">ncol</span>(X), <span class="at">byrow =</span> <span class="cn">FALSE</span>))</span>
<span id="cb20-16"><a href="#cb20-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-17"><a href="#cb20-17" aria-hidden="true" tabindex="-1"></a>  out <span class="ot">=</span> X_norm <span class="sc">*</span> <span class="fu">matrix</span>(gamma, <span class="at">nrow =</span> <span class="fu">nrow</span>(X), <span class="at">ncol =</span> <span class="fu">ncol</span>(X), <span class="at">byrow =</span> <span class="cn">TRUE</span>) <span class="sc">+</span></span>
<span id="cb20-18"><a href="#cb20-18" aria-hidden="true" tabindex="-1"></a>        <span class="fu">matrix</span>(beta, <span class="at">nrow =</span> <span class="fu">nrow</span>(X), <span class="at">ncol =</span> <span class="fu">ncol</span>(X), <span class="at">byrow =</span> <span class="cn">TRUE</span>)</span>
<span id="cb20-19"><a href="#cb20-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-20"><a href="#cb20-20" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(<span class="fu">list</span>(<span class="at">out =</span> out, <span class="at">X_norm =</span> X_norm, <span class="at">mean_X =</span> mean_X, <span class="at">var_X =</span> var_X, <span class="at">X =</span> X)) <span class="co"># Store X for backward</span></span>
<span id="cb20-21"><a href="#cb20-21" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb20-22"><a href="#cb20-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-23"><a href="#cb20-23" aria-hidden="true" tabindex="-1"></a>layer_norm_backward <span class="ot">=</span> <span class="cf">function</span>(dout, X, X_norm, mean_X, var_X, gamma, <span class="at">epsilon =</span> <span class="fl">1e-5</span>) {</span>
<span id="cb20-24"><a href="#cb20-24" aria-hidden="true" tabindex="-1"></a>  N <span class="ot">=</span> <span class="fu">nrow</span>(X) <span class="co"># Number of samples (batch size)</span></span>
<span id="cb20-25"><a href="#cb20-25" aria-hidden="true" tabindex="-1"></a>  D <span class="ot">=</span> <span class="fu">ncol</span>(X) <span class="co"># Number of features (embedding dimension)</span></span>
<span id="cb20-26"><a href="#cb20-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-27"><a href="#cb20-27" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Gradients for gamma and beta</span></span>
<span id="cb20-28"><a href="#cb20-28" aria-hidden="true" tabindex="-1"></a>  dgamma <span class="ot">=</span> <span class="fu">colSums</span>(dout <span class="sc">*</span> X_norm)</span>
<span id="cb20-29"><a href="#cb20-29" aria-hidden="true" tabindex="-1"></a>  dbeta <span class="ot">=</span> <span class="fu">colSums</span>(dout)</span>
<span id="cb20-30"><a href="#cb20-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-31"><a href="#cb20-31" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Gradient of X_norm</span></span>
<span id="cb20-32"><a href="#cb20-32" aria-hidden="true" tabindex="-1"></a>  d_X_norm <span class="ot">=</span> dout <span class="sc">*</span> <span class="fu">matrix</span>(gamma, <span class="at">nrow =</span> N, <span class="at">ncol =</span> D, <span class="at">byrow =</span> <span class="cn">TRUE</span>)</span>
<span id="cb20-33"><a href="#cb20-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-34"><a href="#cb20-34" aria-hidden="true" tabindex="-1"></a>  std_inv <span class="ot">=</span> <span class="dv">1</span> <span class="sc">/</span> <span class="fu">sqrt</span>(var_X <span class="sc">+</span> epsilon)  <span class="co"># (N,)</span></span>
<span id="cb20-35"><a href="#cb20-35" aria-hidden="true" tabindex="-1"></a>  std_inv_mat <span class="ot">=</span> <span class="fu">matrix</span>(std_inv, <span class="at">nrow =</span> N, <span class="at">ncol =</span> D)</span>
<span id="cb20-36"><a href="#cb20-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-37"><a href="#cb20-37" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Sum over features (columns) for each row</span></span>
<span id="cb20-38"><a href="#cb20-38" aria-hidden="true" tabindex="-1"></a>  d_X_norm_sum <span class="ot">=</span> <span class="fu">rowSums</span>(d_X_norm)                   <span class="co"># shape (N,)</span></span>
<span id="cb20-39"><a href="#cb20-39" aria-hidden="true" tabindex="-1"></a>  d_X_norm_dot_Xnorm <span class="ot">=</span> <span class="fu">rowSums</span>(d_X_norm <span class="sc">*</span> X_norm)    <span class="co"># shape (N,)</span></span>
<span id="cb20-40"><a href="#cb20-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-41"><a href="#cb20-41" aria-hidden="true" tabindex="-1"></a>  term1 <span class="ot">=</span> d_X_norm</span>
<span id="cb20-42"><a href="#cb20-42" aria-hidden="true" tabindex="-1"></a>  term2 <span class="ot">=</span> <span class="fu">matrix</span>(d_X_norm_sum <span class="sc">/</span> D, <span class="at">nrow =</span> N, <span class="at">ncol =</span> D)</span>
<span id="cb20-43"><a href="#cb20-43" aria-hidden="true" tabindex="-1"></a>  term3 <span class="ot">=</span> X_norm <span class="sc">*</span> <span class="fu">matrix</span>(d_X_norm_dot_Xnorm <span class="sc">/</span> D, <span class="at">nrow =</span> N, <span class="at">ncol =</span> D)</span>
<span id="cb20-44"><a href="#cb20-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-45"><a href="#cb20-45" aria-hidden="true" tabindex="-1"></a>  d_X <span class="ot">=</span> (term1 <span class="sc">-</span> term2 <span class="sc">-</span> term3) <span class="sc">*</span> std_inv_mat</span>
<span id="cb20-46"><a href="#cb20-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-47"><a href="#cb20-47" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(<span class="fu">list</span>(<span class="at">d_X =</span> d_X, <span class="at">dgamma =</span> dgamma, <span class="at">dbeta =</span> dbeta))</span>
<span id="cb20-48"><a href="#cb20-48" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb20-49"><a href="#cb20-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-50"><a href="#cb20-50" aria-hidden="true" tabindex="-1"></a><span class="co"># ReLU activation function</span></span>
<span id="cb20-51"><a href="#cb20-51" aria-hidden="true" tabindex="-1"></a>relu <span class="ot">=</span> <span class="cf">function</span>(x) {</span>
<span id="cb20-52"><a href="#cb20-52" aria-hidden="true" tabindex="-1"></a>  <span class="fu">matrix</span>(<span class="fu">pmax</span>(<span class="dv">0</span>, x), <span class="at">nrow =</span> <span class="fu">nrow</span>(x), <span class="at">ncol =</span> <span class="fu">ncol</span>(x))</span>
<span id="cb20-53"><a href="#cb20-53" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb20-54"><a href="#cb20-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-55"><a href="#cb20-55" aria-hidden="true" tabindex="-1"></a>relu_grad <span class="ot">=</span> <span class="cf">function</span>(x) {</span>
<span id="cb20-56"><a href="#cb20-56" aria-hidden="true" tabindex="-1"></a>  (x <span class="sc">&gt;</span> <span class="dv">0</span>) <span class="sc">*</span> <span class="dv">1</span></span>
<span id="cb20-57"><a href="#cb20-57" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb20-58"><a href="#cb20-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-59"><a href="#cb20-59" aria-hidden="true" tabindex="-1"></a><span class="co"># Dropout layer</span></span>
<span id="cb20-60"><a href="#cb20-60" aria-hidden="true" tabindex="-1"></a>dropout_forward <span class="ot">=</span> <span class="cf">function</span>(x, dropout_rate, is_training) {</span>
<span id="cb20-61"><a href="#cb20-61" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> (is_training) {</span>
<span id="cb20-62"><a href="#cb20-62" aria-hidden="true" tabindex="-1"></a>    mask <span class="ot">=</span> <span class="fu">matrix</span>(<span class="fu">runif</span>(<span class="fu">nrow</span>(x)<span class="sc">*</span><span class="fu">ncol</span>(x)), <span class="at">nrow =</span> <span class="fu">nrow</span>(x), <span class="at">ncol =</span> <span class="fu">ncol</span>(x)) <span class="sc">&gt;</span> dropout_rate</span>
<span id="cb20-63"><a href="#cb20-63" aria-hidden="true" tabindex="-1"></a>    out <span class="ot">=</span> x <span class="sc">*</span> mask <span class="sc">/</span> (<span class="dv">1</span> <span class="sc">-</span> dropout_rate) <span class="co"># Scale up during training</span></span>
<span id="cb20-64"><a href="#cb20-64" aria-hidden="true" tabindex="-1"></a>    <span class="fu">return</span>(<span class="fu">list</span>(<span class="at">out =</span> out, <span class="at">mask =</span> mask))</span>
<span id="cb20-65"><a href="#cb20-65" aria-hidden="true" tabindex="-1"></a>  } <span class="cf">else</span> {</span>
<span id="cb20-66"><a href="#cb20-66" aria-hidden="true" tabindex="-1"></a>    <span class="fu">return</span>(<span class="fu">list</span>(<span class="at">out =</span> x, <span class="at">mask =</span> <span class="cn">NULL</span>)) <span class="co"># No dropout during inference</span></span>
<span id="cb20-67"><a href="#cb20-67" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb20-68"><a href="#cb20-68" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb20-69"><a href="#cb20-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-70"><a href="#cb20-70" aria-hidden="true" tabindex="-1"></a>dropout_backward <span class="ot">=</span> <span class="cf">function</span>(dout, mask, dropout_rate) {</span>
<span id="cb20-71"><a href="#cb20-71" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Apply the same mask and scaling to the gradient</span></span>
<span id="cb20-72"><a href="#cb20-72" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(dout <span class="sc">*</span> mask <span class="sc">/</span> (<span class="dv">1</span> <span class="sc">-</span> dropout_rate))</span>
<span id="cb20-73"><a href="#cb20-73" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb20-74"><a href="#cb20-74" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-75"><a href="#cb20-75" aria-hidden="true" tabindex="-1"></a><span class="co"># He Initialization function for weights</span></span>
<span id="cb20-76"><a href="#cb20-76" aria-hidden="true" tabindex="-1"></a>he_init_weights <span class="ot">=</span> <span class="cf">function</span>(fan_in, fan_out, <span class="at">ReLU_activation =</span> <span class="cn">FALSE</span>) {</span>
<span id="cb20-77"><a href="#cb20-77" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Only account for halving the variance if ReLU actiavtion is being applied</span></span>
<span id="cb20-78"><a href="#cb20-78" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span>(ReLU_activation){</span>
<span id="cb20-79"><a href="#cb20-79" aria-hidden="true" tabindex="-1"></a>    std_dev <span class="ot">=</span> <span class="fu">sqrt</span>(<span class="dv">2</span> <span class="sc">/</span> fan_in)</span>
<span id="cb20-80"><a href="#cb20-80" aria-hidden="true" tabindex="-1"></a>  }<span class="cf">else</span>{</span>
<span id="cb20-81"><a href="#cb20-81" aria-hidden="true" tabindex="-1"></a>    std_dev <span class="ot">=</span> <span class="fu">sqrt</span>(<span class="dv">1</span> <span class="sc">/</span> fan_in)</span>
<span id="cb20-82"><a href="#cb20-82" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb20-83"><a href="#cb20-83" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb20-84"><a href="#cb20-84" aria-hidden="true" tabindex="-1"></a>  weights <span class="ot">=</span> <span class="fu">matrix</span>(<span class="fu">rnorm</span>(fan_in <span class="sc">*</span> fan_out, <span class="at">mean =</span> <span class="dv">0</span>, <span class="at">sd =</span> std_dev),</span>
<span id="cb20-85"><a href="#cb20-85" aria-hidden="true" tabindex="-1"></a>                   <span class="at">nrow =</span> fan_in, <span class="at">ncol =</span> fan_out)</span>
<span id="cb20-86"><a href="#cb20-86" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(weights)</span>
<span id="cb20-87"><a href="#cb20-87" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb20-88"><a href="#cb20-88" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-89"><a href="#cb20-89" aria-hidden="true" tabindex="-1"></a><span class="co"># Gradient Clipping function (by global norm)</span></span>
<span id="cb20-90"><a href="#cb20-90" aria-hidden="true" tabindex="-1"></a>clip_gradients_by_norm <span class="ot">=</span> <span class="cf">function</span>(gradients_list, clip_threshold) {</span>
<span id="cb20-91"><a href="#cb20-91" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Flatten all gradients into a single vector to calculate the norm</span></span>
<span id="cb20-92"><a href="#cb20-92" aria-hidden="true" tabindex="-1"></a>  all_grad_elements <span class="ot">=</span> <span class="fu">c</span>()</span>
<span id="cb20-93"><a href="#cb20-93" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> (grad_name <span class="cf">in</span> <span class="fu">names</span>(gradients_list)) {</span>
<span id="cb20-94"><a href="#cb20-94" aria-hidden="true" tabindex="-1"></a>    all_grad_elements <span class="ot">=</span> <span class="fu">c</span>(all_grad_elements, <span class="fu">as.vector</span>(gradients_list[[grad_name]]))</span>
<span id="cb20-95"><a href="#cb20-95" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb20-96"><a href="#cb20-96" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-97"><a href="#cb20-97" aria-hidden="true" tabindex="-1"></a>  global_norm <span class="ot">=</span> <span class="fu">sqrt</span>(<span class="fu">sum</span>(all_grad_elements<span class="sc">^</span><span class="dv">2</span>))</span>
<span id="cb20-98"><a href="#cb20-98" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-99"><a href="#cb20-99" aria-hidden="true" tabindex="-1"></a>  <span class="co"># If the global norm exceeds the threshold, scale all gradients</span></span>
<span id="cb20-100"><a href="#cb20-100" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> (global_norm <span class="sc">&gt;</span> clip_threshold) {</span>
<span id="cb20-101"><a href="#cb20-101" aria-hidden="true" tabindex="-1"></a>    scale_factor <span class="ot">=</span> clip_threshold <span class="sc">/</span> global_norm</span>
<span id="cb20-102"><a href="#cb20-102" aria-hidden="true" tabindex="-1"></a>    clipped_gradients <span class="ot">=</span> <span class="fu">lapply</span>(gradients_list, <span class="cf">function</span>(grad_matrix) {</span>
<span id="cb20-103"><a href="#cb20-103" aria-hidden="true" tabindex="-1"></a>      grad_matrix <span class="sc">*</span> scale_factor</span>
<span id="cb20-104"><a href="#cb20-104" aria-hidden="true" tabindex="-1"></a>    })</span>
<span id="cb20-105"><a href="#cb20-105" aria-hidden="true" tabindex="-1"></a>    <span class="fu">return</span>(clipped_gradients)</span>
<span id="cb20-106"><a href="#cb20-106" aria-hidden="true" tabindex="-1"></a>  } <span class="cf">else</span> {</span>
<span id="cb20-107"><a href="#cb20-107" aria-hidden="true" tabindex="-1"></a>    <span class="co"># No clipping needed</span></span>
<span id="cb20-108"><a href="#cb20-108" aria-hidden="true" tabindex="-1"></a>    <span class="fu">return</span>(gradients_list)</span>
<span id="cb20-109"><a href="#cb20-109" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb20-110"><a href="#cb20-110" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
</section>
<section id="model-parameters-initialization" class="level3">
<h3 class="anchored" data-anchor-id="model-parameters-initialization">Model Parameters Initialization</h3>
<p>Many of the parameters will be initialized using He initialization. He initialization (also known as Kaiming initialization) is a weight initialization technique widely used in deep neural networks, especially when using Rectified Linear Unit (ReLU) as the activation function which we are doing here. The weights are determined like this:</p>
<p><span class="math display">\[
W = N(0, \sigma^2) \text{ where } \sigma = \sqrt{\frac{2}{n_{in}}}
\]</span></p>
<p>The basic idea is to choose starting weights that won’t immediately cause problems with vanishing or exploding gradients. We want to keep the variance of activations and gradients consistent as we move forward and backward through the layers in the network. Consider the simple feed forward layer as follows:</p>
<p><span class="math display">\[
\begin{aligned}
&amp;y = Wx + b \\
&amp;a = f(y)
\end{aligned}
\]</span></p>
<p>where:</p>
<ul>
<li>x is the input vector to the layer.</li>
<li>W is the weight matrix of the layer.</li>
<li>b is the bias vector.</li>
<li>y is the linear output before activation.</li>
<li>f is the activation function (e.g., ReLU).</li>
<li>a is the activated output of the layer.</li>
</ul>
<p>For simplicity, let’s assume the biases b are initialized to zero (which is common and is what we are doing here) and that the elements of x and W are independent and have zero mean.</p>
<p>ReLU is defined as f(y)=max(0,y). This means that for y&lt;0, the output is 0, and for y&gt;=0, the output is y. When initialized, roughly half of the inputs to a ReLU neuron will be negative (resulting in zero output) and half will be positive (resulting in the input value). This effectively halves the variance compared to a linear activation function.</p>
<p>We want the variance of the output a to be roughly equal to the variance of the input x.</p>
<p>Consider the variance of y=Wx:</p>
<p><span class="math display">\[
var(y) = var(\sum_{i=1}^{n_{in}} W_i x_i)
\]</span></p>
<p>Assuming <span class="math inline">\(W_i\)</span> and <span class="math inline">\(x_i\)</span> are independent and have zero mean:</p>
<p><span class="math display">\[
\begin{aligned}
var(y) &amp;= \sum_{i=1}^{n_{in}} var(W_i x_i) \\
&amp;= \sum_{i=1}^{n_{in}} E[(W_i x_i)^2] - E[(W_i x_i)]^2 \\
&amp;= \sum_{i=1}^{n_{in}} E[W_i^2] E[x_i^2] - (E[W_i] E[x_i])^2 \\
&amp;= \sum_{i=1}^{n_{in}} (var(W_i) + E[W_i]^2) (var(x_i) + E[x_i]^2) - (E[W_i] E[x_i])^2 \\
&amp;= \sum_{i=1}^{n_{in}} (E[W_i]^2 var(x_i) + E[x_i]^2 var(W_i) + var(W_i)var(x_i))
\end{aligned}
\]</span></p>
<p>Since we assume <span class="math inline">\(E[W_i]=0\)</span> and <span class="math inline">\(E[x_i]=0\)</span>:</p>
<p><span class="math display">\[
    var(y) = \sum_{i=1}^{n_{in}} var(W_i)var(x_i)
\]</span> Assuming all <span class="math inline">\(W_i\)</span> and <span class="math inline">\(x_i\)</span> are identically distributed:</p>
<p><span class="math display">\[
    var(y) = n_{in} var(W)var(x)
\]</span></p>
<p>Now, for ReLU, a=max(0,y). If y has zero mean and is symmetric around zero, then roughly half of the values of y will be positive and half will be negative. The negative values become zero, effectively reducing the variance by half. So, for the output after ReLU:</p>
<p><span class="math display">\[
\begin{aligned}
var(a) &amp;\approx \frac{1}{2} var(y) \\
&amp;\approx \frac{1}{2} n_{in} var(W)var(x)
\end{aligned}
\]</span></p>
<p>Remember the objective: We want the variance of the output a to be roughly equal to the variance of the input x. So we substitute <span class="math inline">\(var(x)\)</span> for <span class="math inline">\(var(a)\)</span> in this last equation, solving for the variance in weights <span class="math inline">\(W\)</span> that achieve the desired outcome.</p>
<p><span class="math display">\[
    var(x) \approx \frac{1}{2} n_{in} var(W)var(x)
\]</span></p>
<p>and therefore:</p>
<p><span class="math display">\[
\begin{aligned}
\frac{1}{2} n_{in} var(W) &amp;= 1 \\
var(W_i) &amp;= \frac{2}{n_{in}}
\end{aligned}
\]</span></p>
<p>Most layers don’t have activation functions applied, so we only double the variance in the He initialization for the layer where the ReLU activation is used. In other cases, the same logic applies, just without the need to double the variance.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">42</span>) <span class="co"># For reproducibility</span></span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Character Embeddings - typically not He-initialized, using uniform</span></span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a>C <span class="ot">=</span> <span class="fu">matrix</span>(<span class="fu">runif</span>(vocab_size <span class="sc">*</span> n_embd, <span class="sc">-</span><span class="dv">1</span>, <span class="dv">1</span>), <span class="at">nrow =</span> vocab_size, <span class="at">ncol =</span> n_embd)</span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Positional Embeddings - typically not He-initialized (often fixed or small random)</span></span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a>P_emb <span class="ot">=</span> <span class="fu">matrix</span>(<span class="fu">runif</span>(block_size <span class="sc">*</span> n_embd, <span class="sc">-</span><span class="dv">1</span>, <span class="dv">1</span>), <span class="at">nrow =</span> block_size, <span class="at">ncol =</span> n_embd)</span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Multi-Head Attention Parameters (using He Initialization)</span></span>
<span id="cb21-10"><a href="#cb21-10" aria-hidden="true" tabindex="-1"></a><span class="co"># For Wq, Wk, Wv, fan_in is n_embd</span></span>
<span id="cb21-11"><a href="#cb21-11" aria-hidden="true" tabindex="-1"></a>Wq <span class="ot">=</span> <span class="fu">he_init_weights</span>(n_embd, n_head <span class="sc">*</span> head_size)</span>
<span id="cb21-12"><a href="#cb21-12" aria-hidden="true" tabindex="-1"></a>Wk <span class="ot">=</span> <span class="fu">he_init_weights</span>(n_embd, n_head <span class="sc">*</span> head_size)</span>
<span id="cb21-13"><a href="#cb21-13" aria-hidden="true" tabindex="-1"></a>Wv <span class="ot">=</span> <span class="fu">he_init_weights</span>(n_embd, n_head <span class="sc">*</span> head_size)</span>
<span id="cb21-14"><a href="#cb21-14" aria-hidden="true" tabindex="-1"></a><span class="co"># For Wo, fan_in is n_head * head_size</span></span>
<span id="cb21-15"><a href="#cb21-15" aria-hidden="true" tabindex="-1"></a>Wo <span class="ot">=</span> <span class="fu">he_init_weights</span>(n_head <span class="sc">*</span> head_size, n_embd)</span>
<span id="cb21-16"><a href="#cb21-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-17"><a href="#cb21-17" aria-hidden="true" tabindex="-1"></a><span class="co"># LayerNorm for Attention - gamma initialized to 1s, beta to 0s</span></span>
<span id="cb21-18"><a href="#cb21-18" aria-hidden="true" tabindex="-1"></a>ln1_gamma <span class="ot">=</span> <span class="fu">rep</span>(<span class="dv">1</span>, n_embd)</span>
<span id="cb21-19"><a href="#cb21-19" aria-hidden="true" tabindex="-1"></a>ln1_beta <span class="ot">=</span> <span class="fu">rep</span>(<span class="dv">0</span>, n_embd)</span>
<span id="cb21-20"><a href="#cb21-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-21"><a href="#cb21-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Feed-Forward Network Parameters (using He Initialization)</span></span>
<span id="cb21-22"><a href="#cb21-22" aria-hidden="true" tabindex="-1"></a><span class="co"># For W_ff1, fan_in is n_embd</span></span>
<span id="cb21-23"><a href="#cb21-23" aria-hidden="true" tabindex="-1"></a>W_ff1 <span class="ot">=</span> <span class="fu">he_init_weights</span>(n_embd, <span class="dv">4</span> <span class="sc">*</span> n_embd, <span class="at">ReLU_activation =</span> <span class="cn">TRUE</span>)</span>
<span id="cb21-24"><a href="#cb21-24" aria-hidden="true" tabindex="-1"></a>b_ff1 <span class="ot">=</span> <span class="fu">rep</span>(<span class="dv">0</span>, <span class="dv">4</span> <span class="sc">*</span> n_embd) <span class="co"># Biases are kept at 0</span></span>
<span id="cb21-25"><a href="#cb21-25" aria-hidden="true" tabindex="-1"></a><span class="co"># For W_ff2, fan_in is 4 * n_embd</span></span>
<span id="cb21-26"><a href="#cb21-26" aria-hidden="true" tabindex="-1"></a>W_ff2 <span class="ot">=</span> <span class="fu">he_init_weights</span>(<span class="dv">4</span> <span class="sc">*</span> n_embd, n_embd)</span>
<span id="cb21-27"><a href="#cb21-27" aria-hidden="true" tabindex="-1"></a>b_ff2 <span class="ot">=</span> <span class="fu">rep</span>(<span class="dv">0</span>, n_embd) <span class="co"># Biases are kept at 0</span></span>
<span id="cb21-28"><a href="#cb21-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-29"><a href="#cb21-29" aria-hidden="true" tabindex="-1"></a><span class="co"># LayerNorm for FF - gamma initialized to 1s, beta to 0s</span></span>
<span id="cb21-30"><a href="#cb21-30" aria-hidden="true" tabindex="-1"></a>ln2_gamma <span class="ot">=</span> <span class="fu">rep</span>(<span class="dv">1</span>, n_embd)</span>
<span id="cb21-31"><a href="#cb21-31" aria-hidden="true" tabindex="-1"></a>ln2_beta <span class="ot">=</span> <span class="fu">rep</span>(<span class="dv">0</span>, n_embd)</span>
<span id="cb21-32"><a href="#cb21-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-33"><a href="#cb21-33" aria-hidden="true" tabindex="-1"></a><span class="co"># Final linear layer to logits (using He Initialization)</span></span>
<span id="cb21-34"><a href="#cb21-34" aria-hidden="true" tabindex="-1"></a><span class="co"># For W_final, fan_in is n_embd</span></span>
<span id="cb21-35"><a href="#cb21-35" aria-hidden="true" tabindex="-1"></a>W_final <span class="ot">=</span> <span class="fu">he_init_weights</span>(n_embd, vocab_size)</span>
<span id="cb21-36"><a href="#cb21-36" aria-hidden="true" tabindex="-1"></a>b_final <span class="ot">=</span> <span class="fu">rep</span>(<span class="dv">0</span>, vocab_size) <span class="co"># Biases are kept at 0</span></span>
<span id="cb21-37"><a href="#cb21-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-38"><a href="#cb21-38" aria-hidden="true" tabindex="-1"></a><span class="co"># Store all parameters in a list for easier management</span></span>
<span id="cb21-39"><a href="#cb21-39" aria-hidden="true" tabindex="-1"></a>params <span class="ot">=</span> <span class="fu">list</span>(</span>
<span id="cb21-40"><a href="#cb21-40" aria-hidden="true" tabindex="-1"></a>  <span class="at">C =</span> C, <span class="at">P_emb =</span> P_emb,</span>
<span id="cb21-41"><a href="#cb21-41" aria-hidden="true" tabindex="-1"></a>  <span class="at">Wq =</span> Wq, <span class="at">Wk =</span> Wk, <span class="at">Wv =</span> Wv, <span class="at">Wo =</span> Wo,</span>
<span id="cb21-42"><a href="#cb21-42" aria-hidden="true" tabindex="-1"></a>  <span class="at">ln1_gamma =</span> ln1_gamma, <span class="at">ln1_beta =</span> ln1_beta,</span>
<span id="cb21-43"><a href="#cb21-43" aria-hidden="true" tabindex="-1"></a>  <span class="at">W_ff1 =</span> W_ff1, <span class="at">b_ff1 =</span> b_ff1, <span class="at">W_ff2 =</span> W_ff2, <span class="at">b_ff2 =</span> b_ff2,</span>
<span id="cb21-44"><a href="#cb21-44" aria-hidden="true" tabindex="-1"></a>  <span class="at">ln2_gamma =</span> ln2_gamma, <span class="at">ln2_beta =</span> ln2_beta,</span>
<span id="cb21-45"><a href="#cb21-45" aria-hidden="true" tabindex="-1"></a>  <span class="at">W_final =</span> W_final, <span class="at">b_final =</span> b_final</span>
<span id="cb21-46"><a href="#cb21-46" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb21-47"><a href="#cb21-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-48"><a href="#cb21-48" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"Parameters initialized.</span><span class="sc">\n</span><span class="st">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Parameters initialized.</code></pre>
</div>
</div>
</section>
</section>
<section id="transformer-block-implementation-functions" class="level2">
<h2 class="anchored" data-anchor-id="transformer-block-implementation-functions">Transformer Block Implementation Functions</h2>
<section id="multi-head-self-attention" class="level3">
<h3 class="anchored" data-anchor-id="multi-head-self-attention">Multi-Head Self-Attention</h3>
<p>The self-attention mechanism helps the model learn weights associated with prior tokens in the sequence. It could actually look forward as well if allowed to do so, like when translation is the desired function, but usually for next token prediction the attention mechanism is restricted to only look backwards at previous tokens. This is handled through the “is_causal” parameter in the function below, which will set the weights for any future token positions to zero during the softmax step.</p>
<p>The multi-headed piece of the mechanism is basically because multiple “heads” of the attention mechanism are run in parallel, and then the output is joined together in a final linear layer at the end. This lets the model learn more than one way to pay attention to previous tokens, if that is beneficial for the prediction.</p>
<p>This mechanism is summarized in figure 2 of Vaswani, Ashish, et al.(2017), reproduced here:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="fig2.jpg" class="img-fluid figure-img"></p>
</figure>
</div>
<p>The key equation from that paper which we are implementing here is:</p>
<p><span class="math display">\[
\text{Attention}(Q, K, V) = \text{softmax}(\frac{QK^T}{\sqrt{d_k}}) V
\]</span></p>
<p>The softmax function produces weights which are what determine how much attention is paid to previous tokens, while the V matrix contains the values emitted by those previous tokens.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>multi_head_attention_forward <span class="ot">=</span> <span class="cf">function</span>(x, Wq, Wk, Wv, Wo, n_head, head_size, ln_gamma, ln_beta, dropout_rate, is_training, <span class="at">is_causal =</span> <span class="cn">TRUE</span>) {</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>  x_in <span class="ot">=</span> x</span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Pre-LN:</span></span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a>  ln_out_x <span class="ot">=</span> <span class="fu">layer_norm_forward</span>(x, ln_gamma, ln_beta)</span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a>  x <span class="ot">=</span> ln_out_x<span class="sc">$</span>out</span>
<span id="cb23-7"><a href="#cb23-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-8"><a href="#cb23-8" aria-hidden="true" tabindex="-1"></a>  Q_all <span class="ot">=</span> x <span class="sc">%*%</span> Wq</span>
<span id="cb23-9"><a href="#cb23-9" aria-hidden="true" tabindex="-1"></a>  K_all <span class="ot">=</span> x <span class="sc">%*%</span> Wk</span>
<span id="cb23-10"><a href="#cb23-10" aria-hidden="true" tabindex="-1"></a>  V_all <span class="ot">=</span> x <span class="sc">%*%</span> Wv</span>
<span id="cb23-11"><a href="#cb23-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-12"><a href="#cb23-12" aria-hidden="true" tabindex="-1"></a>  outputs_per_head <span class="ot">=</span> <span class="fu">lapply</span>(<span class="dv">1</span><span class="sc">:</span>n_head, <span class="cf">function</span>(h) {</span>
<span id="cb23-13"><a href="#cb23-13" aria-hidden="true" tabindex="-1"></a>    start_col <span class="ot">=</span> (h <span class="sc">-</span> <span class="dv">1</span>) <span class="sc">*</span> head_size <span class="sc">+</span> <span class="dv">1</span></span>
<span id="cb23-14"><a href="#cb23-14" aria-hidden="true" tabindex="-1"></a>    end_col <span class="ot">=</span> h <span class="sc">*</span> head_size</span>
<span id="cb23-15"><a href="#cb23-15" aria-hidden="true" tabindex="-1"></a>    Q_h <span class="ot">=</span> Q_all[, start_col<span class="sc">:</span>end_col]</span>
<span id="cb23-16"><a href="#cb23-16" aria-hidden="true" tabindex="-1"></a>    K_h <span class="ot">=</span> K_all[, start_col<span class="sc">:</span>end_col]</span>
<span id="cb23-17"><a href="#cb23-17" aria-hidden="true" tabindex="-1"></a>    V_h <span class="ot">=</span> V_all[, start_col<span class="sc">:</span>end_col]</span>
<span id="cb23-18"><a href="#cb23-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-19"><a href="#cb23-19" aria-hidden="true" tabindex="-1"></a>    scores <span class="ot">=</span> (Q_h <span class="sc">%*%</span> <span class="fu">t</span>(K_h)) <span class="sc">/</span> <span class="fu">sqrt</span>(head_size)</span>
<span id="cb23-20"><a href="#cb23-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-21"><a href="#cb23-21" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> (is_causal) {</span>
<span id="cb23-22"><a href="#cb23-22" aria-hidden="true" tabindex="-1"></a>      mask_causal <span class="ot">=</span> <span class="fu">upper.tri</span>(<span class="fu">matrix</span>(<span class="dv">0</span>, <span class="fu">nrow</span>(scores), <span class="fu">ncol</span>(scores)), <span class="at">diag =</span> <span class="cn">FALSE</span>)</span>
<span id="cb23-23"><a href="#cb23-23" aria-hidden="true" tabindex="-1"></a>      scores[mask_causal] <span class="ot">=</span> <span class="sc">-</span><span class="cn">Inf</span></span>
<span id="cb23-24"><a href="#cb23-24" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb23-25"><a href="#cb23-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-26"><a href="#cb23-26" aria-hidden="true" tabindex="-1"></a>    attention_weights <span class="ot">=</span> <span class="fu">softmax_matrix_rows</span>(scores)</span>
<span id="cb23-27"><a href="#cb23-27" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Dropout after attention weights (before multiplying by V)</span></span>
<span id="cb23-28"><a href="#cb23-28" aria-hidden="true" tabindex="-1"></a>    attn_weights_dropout <span class="ot">=</span> <span class="fu">dropout_forward</span>(attention_weights, dropout_rate, is_training)</span>
<span id="cb23-29"><a href="#cb23-29" aria-hidden="true" tabindex="-1"></a>    attention_weights_dropped <span class="ot">=</span> attn_weights_dropout<span class="sc">$</span>out</span>
<span id="cb23-30"><a href="#cb23-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-31"><a href="#cb23-31" aria-hidden="true" tabindex="-1"></a>    attended_values <span class="ot">=</span> attention_weights_dropped <span class="sc">%*%</span> V_h</span>
<span id="cb23-32"><a href="#cb23-32" aria-hidden="true" tabindex="-1"></a>    <span class="fu">return</span>(<span class="fu">list</span>(<span class="at">attended_values =</span> attended_values,</span>
<span id="cb23-33"><a href="#cb23-33" aria-hidden="true" tabindex="-1"></a>                <span class="at">attention_weights =</span> attention_weights, <span class="co"># Original for backward</span></span>
<span id="cb23-34"><a href="#cb23-34" aria-hidden="true" tabindex="-1"></a>                <span class="at">attention_weights_dropped_mask =</span> attn_weights_dropout<span class="sc">$</span>mask,</span>
<span id="cb23-35"><a href="#cb23-35" aria-hidden="true" tabindex="-1"></a>                <span class="at">scores =</span> scores, <span class="at">Q_h =</span> Q_h, <span class="at">K_h =</span> K_h, <span class="at">V_h =</span> V_h))</span>
<span id="cb23-36"><a href="#cb23-36" aria-hidden="true" tabindex="-1"></a>  })</span>
<span id="cb23-37"><a href="#cb23-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-38"><a href="#cb23-38" aria-hidden="true" tabindex="-1"></a>  concat_heads <span class="ot">=</span> <span class="fu">do.call</span>(cbind, <span class="fu">lapply</span>(outputs_per_head, <span class="cf">function</span>(o) o<span class="sc">$</span>attended_values))</span>
<span id="cb23-39"><a href="#cb23-39" aria-hidden="true" tabindex="-1"></a>  attn_out <span class="ot">=</span> concat_heads <span class="sc">%*%</span> Wo</span>
<span id="cb23-40"><a href="#cb23-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-41"><a href="#cb23-41" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Dropout after attention block output (before residual)</span></span>
<span id="cb23-42"><a href="#cb23-42" aria-hidden="true" tabindex="-1"></a>  attn_out_dropout <span class="ot">=</span> <span class="fu">dropout_forward</span>(attn_out, dropout_rate, is_training)</span>
<span id="cb23-43"><a href="#cb23-43" aria-hidden="true" tabindex="-1"></a>  attn_out_dropped <span class="ot">=</span> attn_out_dropout<span class="sc">$</span>out</span>
<span id="cb23-44"><a href="#cb23-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-45"><a href="#cb23-45" aria-hidden="true" tabindex="-1"></a>  out <span class="ot">=</span> x_in <span class="sc">+</span> attn_out_dropped</span>
<span id="cb23-46"><a href="#cb23-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-47"><a href="#cb23-47" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(<span class="fu">list</span>(</span>
<span id="cb23-48"><a href="#cb23-48" aria-hidden="true" tabindex="-1"></a>    <span class="at">out =</span> out,</span>
<span id="cb23-49"><a href="#cb23-49" aria-hidden="true" tabindex="-1"></a>    <span class="at">attn_out_dropped =</span> attn_out_dropped,</span>
<span id="cb23-50"><a href="#cb23-50" aria-hidden="true" tabindex="-1"></a>    <span class="at">attn_out_dropout_mask =</span> attn_out_dropout<span class="sc">$</span>mask,</span>
<span id="cb23-51"><a href="#cb23-51" aria-hidden="true" tabindex="-1"></a>    <span class="at">concat_heads =</span> concat_heads,</span>
<span id="cb23-52"><a href="#cb23-52" aria-hidden="true" tabindex="-1"></a>    <span class="at">outputs_per_head =</span> outputs_per_head,</span>
<span id="cb23-53"><a href="#cb23-53" aria-hidden="true" tabindex="-1"></a>    <span class="at">x_for_qkvo =</span> x, <span class="co"># Input to Q, K, V linear layers (post-LN)</span></span>
<span id="cb23-54"><a href="#cb23-54" aria-hidden="true" tabindex="-1"></a>    <span class="at">ln_cache_x =</span> ln_out_x</span>
<span id="cb23-55"><a href="#cb23-55" aria-hidden="true" tabindex="-1"></a>  ))</span>
<span id="cb23-56"><a href="#cb23-56" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb23-57"><a href="#cb23-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-58"><a href="#cb23-58" aria-hidden="true" tabindex="-1"></a>multi_head_attention_backward <span class="ot">=</span> <span class="cf">function</span>(dout, x_in, attn_out_dropped, attn_out_dropout_mask, concat_heads,</span>
<span id="cb23-59"><a href="#cb23-59" aria-hidden="true" tabindex="-1"></a>                                       outputs_per_head, x_for_qkvo, ln_cache_x, Wq, Wk, Wv, Wo, n_head, head_size, ln_gamma, ln_beta, dropout_rate, <span class="at">is_causal =</span> <span class="cn">TRUE</span>) {</span>
<span id="cb23-60"><a href="#cb23-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-61"><a href="#cb23-61" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Backward through dropout on attn_out</span></span>
<span id="cb23-62"><a href="#cb23-62" aria-hidden="true" tabindex="-1"></a>  d_attn_out <span class="ot">=</span> <span class="fu">dropout_backward</span>(dout, attn_out_dropout_mask, dropout_rate)</span>
<span id="cb23-63"><a href="#cb23-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-64"><a href="#cb23-64" aria-hidden="true" tabindex="-1"></a>  d_Wo <span class="ot">=</span> <span class="fu">t</span>(concat_heads) <span class="sc">%*%</span> d_attn_out</span>
<span id="cb23-65"><a href="#cb23-65" aria-hidden="true" tabindex="-1"></a>  d_concat_heads <span class="ot">=</span> d_attn_out <span class="sc">%*%</span> <span class="fu">t</span>(Wo)</span>
<span id="cb23-66"><a href="#cb23-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-67"><a href="#cb23-67" aria-hidden="true" tabindex="-1"></a>  d_Q_all <span class="ot">=</span> <span class="fu">matrix</span>(<span class="dv">0</span>, <span class="at">nrow =</span> <span class="fu">nrow</span>(x_for_qkvo), <span class="at">ncol =</span> <span class="fu">ncol</span>(Wq))</span>
<span id="cb23-68"><a href="#cb23-68" aria-hidden="true" tabindex="-1"></a>  d_K_all <span class="ot">=</span> <span class="fu">matrix</span>(<span class="dv">0</span>, <span class="at">nrow =</span> <span class="fu">nrow</span>(x_for_qkvo), <span class="at">ncol =</span> <span class="fu">ncol</span>(Wk))</span>
<span id="cb23-69"><a href="#cb23-69" aria-hidden="true" tabindex="-1"></a>  d_V_all <span class="ot">=</span> <span class="fu">matrix</span>(<span class="dv">0</span>, <span class="at">nrow =</span> <span class="fu">nrow</span>(x_for_qkvo), <span class="at">ncol =</span> <span class="fu">ncol</span>(Wv))</span>
<span id="cb23-70"><a href="#cb23-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-71"><a href="#cb23-71" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> (h <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>n_head) {</span>
<span id="cb23-72"><a href="#cb23-72" aria-hidden="true" tabindex="-1"></a>    head_cache <span class="ot">=</span> outputs_per_head[[h]]</span>
<span id="cb23-73"><a href="#cb23-73" aria-hidden="true" tabindex="-1"></a>    d_attended_values <span class="ot">=</span> d_concat_heads[, ((h <span class="sc">-</span> <span class="dv">1</span>) <span class="sc">*</span> head_size <span class="sc">+</span> <span class="dv">1</span>)<span class="sc">:</span>(h <span class="sc">*</span> head_size)]</span>
<span id="cb23-74"><a href="#cb23-74" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-75"><a href="#cb23-75" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Backward through dropout on attention_weights</span></span>
<span id="cb23-76"><a href="#cb23-76" aria-hidden="true" tabindex="-1"></a>    d_attention_weights_dropped <span class="ot">=</span> d_attended_values <span class="sc">%*%</span> <span class="fu">t</span>(head_cache<span class="sc">$</span>V_h)</span>
<span id="cb23-77"><a href="#cb23-77" aria-hidden="true" tabindex="-1"></a>    d_attention_weights <span class="ot">=</span> <span class="fu">dropout_backward</span>(d_attention_weights_dropped, head_cache<span class="sc">$</span>attention_weights_dropped_mask, dropout_rate)</span>
<span id="cb23-78"><a href="#cb23-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-79"><a href="#cb23-79" aria-hidden="true" tabindex="-1"></a>    d_V_h <span class="ot">=</span> <span class="fu">t</span>(head_cache<span class="sc">$</span>attention_weights_dropped) <span class="sc">%*%</span> d_attended_values <span class="co"># Use dropped weights for V_h grad</span></span>
<span id="cb23-80"><a href="#cb23-80" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-81"><a href="#cb23-81" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Backward through softmax (simplified Jacobian-vector product)</span></span>
<span id="cb23-82"><a href="#cb23-82" aria-hidden="true" tabindex="-1"></a>    d_scores <span class="ot">=</span> d_attention_weights <span class="sc">*</span> head_cache<span class="sc">$</span>attention_weights <span class="co"># Element-wise multiply for part of softmax grad</span></span>
<span id="cb23-83"><a href="#cb23-83" aria-hidden="true" tabindex="-1"></a>    d_scores <span class="ot">=</span> d_scores <span class="sc">-</span> <span class="fu">rowSums</span>(d_scores <span class="sc">*</span> head_cache<span class="sc">$</span>attention_weights) <span class="sc">*</span> head_cache<span class="sc">$</span>attention_weights <span class="co"># Sum over columns and apply second part</span></span>
<span id="cb23-84"><a href="#cb23-84" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-85"><a href="#cb23-85" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> (is_causal) {</span>
<span id="cb23-86"><a href="#cb23-86" aria-hidden="true" tabindex="-1"></a>      mask_causal <span class="ot">=</span> <span class="fu">upper.tri</span>(<span class="fu">matrix</span>(<span class="dv">0</span>, <span class="fu">nrow</span>(d_scores), <span class="fu">ncol</span>(d_scores)), <span class="at">diag =</span> <span class="cn">FALSE</span>)</span>
<span id="cb23-87"><a href="#cb23-87" aria-hidden="true" tabindex="-1"></a>      d_scores[mask_causal] <span class="ot">=</span> <span class="dv">0</span></span>
<span id="cb23-88"><a href="#cb23-88" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb23-89"><a href="#cb23-89" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-90"><a href="#cb23-90" aria-hidden="true" tabindex="-1"></a>    d_Q_h <span class="ot">=</span> (d_scores <span class="sc">%*%</span> head_cache<span class="sc">$</span>K_h) <span class="sc">/</span> <span class="fu">sqrt</span>(head_size)</span>
<span id="cb23-91"><a href="#cb23-91" aria-hidden="true" tabindex="-1"></a>    d_K_h <span class="ot">=</span> (<span class="fu">t</span>(d_scores) <span class="sc">%*%</span> head_cache<span class="sc">$</span>Q_h) <span class="sc">/</span> <span class="fu">sqrt</span>(head_size)</span>
<span id="cb23-92"><a href="#cb23-92" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-93"><a href="#cb23-93" aria-hidden="true" tabindex="-1"></a>    d_Q_all[, ((h <span class="sc">-</span> <span class="dv">1</span>) <span class="sc">*</span> head_size <span class="sc">+</span> <span class="dv">1</span>)<span class="sc">:</span>(h <span class="sc">*</span> head_size)] <span class="ot">=</span> d_Q_all[, ((h <span class="sc">-</span> <span class="dv">1</span>) <span class="sc">*</span> head_size <span class="sc">+</span> <span class="dv">1</span>)<span class="sc">:</span>(h <span class="sc">*</span> head_size)] <span class="sc">+</span> d_Q_h</span>
<span id="cb23-94"><a href="#cb23-94" aria-hidden="true" tabindex="-1"></a>    d_K_all[, ((h <span class="sc">-</span> <span class="dv">1</span>) <span class="sc">*</span> head_size <span class="sc">+</span> <span class="dv">1</span>)<span class="sc">:</span>(h <span class="sc">*</span> head_size)] <span class="ot">=</span> d_K_all[, ((h <span class="sc">-</span> <span class="dv">1</span>) <span class="sc">*</span> head_size <span class="sc">+</span> <span class="dv">1</span>)<span class="sc">:</span>(h <span class="sc">*</span> head_size)] <span class="sc">+</span> d_K_h</span>
<span id="cb23-95"><a href="#cb23-95" aria-hidden="true" tabindex="-1"></a>    d_V_all[, ((h <span class="sc">-</span> <span class="dv">1</span>) <span class="sc">*</span> head_size <span class="sc">+</span> <span class="dv">1</span>)<span class="sc">:</span>(h <span class="sc">*</span> head_size)] <span class="ot">=</span> d_V_all[, ((h <span class="sc">-</span> <span class="dv">1</span>) <span class="sc">*</span> head_size <span class="sc">+</span> <span class="dv">1</span>)<span class="sc">:</span>(h <span class="sc">*</span> head_size)] <span class="sc">+</span> d_V_h</span>
<span id="cb23-96"><a href="#cb23-96" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb23-97"><a href="#cb23-97" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-98"><a href="#cb23-98" aria-hidden="true" tabindex="-1"></a>  d_Wq <span class="ot">=</span> <span class="fu">t</span>(x_for_qkvo) <span class="sc">%*%</span> d_Q_all</span>
<span id="cb23-99"><a href="#cb23-99" aria-hidden="true" tabindex="-1"></a>  d_Wk <span class="ot">=</span> <span class="fu">t</span>(x_for_qkvo) <span class="sc">%*%</span> d_K_all</span>
<span id="cb23-100"><a href="#cb23-100" aria-hidden="true" tabindex="-1"></a>  d_Wv <span class="ot">=</span> <span class="fu">t</span>(x_for_qkvo) <span class="sc">%*%</span> d_V_all</span>
<span id="cb23-101"><a href="#cb23-101" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-102"><a href="#cb23-102" aria-hidden="true" tabindex="-1"></a>  d_x_from_qkvo <span class="ot">=</span> (d_Q_all <span class="sc">%*%</span> <span class="fu">t</span>(Wq)) <span class="sc">+</span> (d_K_all <span class="sc">%*%</span> <span class="fu">t</span>(Wk)) <span class="sc">+</span> (d_V_all <span class="sc">%*%</span> <span class="fu">t</span>(Wv))</span>
<span id="cb23-103"><a href="#cb23-103" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-104"><a href="#cb23-104" aria-hidden="true" tabindex="-1"></a>  ln_grads_x <span class="ot">=</span> <span class="fu">layer_norm_backward</span>(d_x_from_qkvo, ln_cache_x<span class="sc">$</span>X, ln_cache_x<span class="sc">$</span>X_norm,</span>
<span id="cb23-105"><a href="#cb23-105" aria-hidden="true" tabindex="-1"></a>                                   ln_cache_x<span class="sc">$</span>mean_X, ln_cache_x<span class="sc">$</span>var_X, ln_gamma, <span class="at">epsilon =</span> <span class="fl">1e-5</span>)</span>
<span id="cb23-106"><a href="#cb23-106" aria-hidden="true" tabindex="-1"></a>  d_ln1_gamma <span class="ot">=</span> ln_grads_x<span class="sc">$</span>dgamma</span>
<span id="cb23-107"><a href="#cb23-107" aria-hidden="true" tabindex="-1"></a>  d_ln1_beta <span class="ot">=</span> ln_grads_x<span class="sc">$</span>dbeta</span>
<span id="cb23-108"><a href="#cb23-108" aria-hidden="true" tabindex="-1"></a>  d_x <span class="ot">=</span> dout <span class="sc">+</span> ln_grads_x<span class="sc">$</span>d_X</span>
<span id="cb23-109"><a href="#cb23-109" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-110"><a href="#cb23-110" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(<span class="fu">list</span>(</span>
<span id="cb23-111"><a href="#cb23-111" aria-hidden="true" tabindex="-1"></a>    <span class="at">d_x =</span> d_x, <span class="at">d_Wq =</span> d_Wq, <span class="at">d_Wk =</span> d_Wk, <span class="at">d_Wv =</span> d_Wv, <span class="at">d_Wo =</span> d_Wo,</span>
<span id="cb23-112"><a href="#cb23-112" aria-hidden="true" tabindex="-1"></a>    <span class="at">d_ln1_gamma =</span> d_ln1_gamma, <span class="at">d_ln1_beta =</span> d_ln1_beta</span>
<span id="cb23-113"><a href="#cb23-113" aria-hidden="true" tabindex="-1"></a>  ))</span>
<span id="cb23-114"><a href="#cb23-114" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="position-wise-feed-forward-network" class="level3">
<h3 class="anchored" data-anchor-id="position-wise-feed-forward-network">Position-wise Feed-Forward Network</h3>
<p>This is a fully connected feed forward neural network with a single hidden layer. An activation function (ReLU in this case) is applied to the hidden layer. The hidden layer is 4 times the size of the input and output layers in this implementation, but there is no strong reason that needs to be the case. This image shows a common visualization for how this type of network looks.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="fig3.png" class="img-fluid figure-img"></p>
</figure>
</div>
<p><span class="math display">\[
\begin{aligned}
&amp;a = f(W_1x + b_1) \\
&amp;y = W_2a + b_2
\end{aligned}
\]</span></p>
<p>where:</p>
<ul>
<li>x is the input vector to the layer.</li>
<li>W1 and W2 are the weight matrices into and out of the hidden layer.</li>
<li>b1 and b2 are the bias vectors.</li>
<li>f is the activation function (e.g., ReLU).</li>
<li>a is the activated output of the hidden layer.</li>
<li>y is the linear output after activation.</li>
</ul>
<p>In the visualization above, the rows of x are the nodes on the left, they become the nodes a in the center after the first transformation, and then become the output y after the second transformation.</p>
<p>Given that the transformations are all linear and the activation function ReLU(x) = max(x, 0) is simple to work with, the implementation is fairly straightforward.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a>feed_forward_forward <span class="ot">=</span> <span class="cf">function</span>(x, W_ff1, b_ff1, W_ff2, b_ff2, ln_gamma, ln_beta, dropout_rate, is_training) {</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>  x_in <span class="ot">=</span> x</span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Pre-LN:</span></span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a>  ln_out_x <span class="ot">=</span> <span class="fu">layer_norm_forward</span>(x, ln_gamma, ln_beta)</span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a>  x <span class="ot">=</span> ln_out_x<span class="sc">$</span>out</span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-8"><a href="#cb24-8" aria-hidden="true" tabindex="-1"></a>  <span class="co"># First linear layer + ReLU</span></span>
<span id="cb24-9"><a href="#cb24-9" aria-hidden="true" tabindex="-1"></a>  hidden <span class="ot">=</span> x <span class="sc">%*%</span> W_ff1 <span class="sc">+</span> <span class="fu">matrix</span>(b_ff1, <span class="at">nrow =</span> <span class="fu">nrow</span>(x), <span class="at">ncol =</span> <span class="fu">ncol</span>(W_ff1), <span class="at">byrow =</span> <span class="cn">TRUE</span>)</span>
<span id="cb24-10"><a href="#cb24-10" aria-hidden="true" tabindex="-1"></a>  hidden_activated <span class="ot">=</span> <span class="fu">relu</span>(hidden)</span>
<span id="cb24-11"><a href="#cb24-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-12"><a href="#cb24-12" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Dropout after hidden activation</span></span>
<span id="cb24-13"><a href="#cb24-13" aria-hidden="true" tabindex="-1"></a>  hidden_dropout <span class="ot">=</span> <span class="fu">dropout_forward</span>(hidden_activated, dropout_rate, is_training)</span>
<span id="cb24-14"><a href="#cb24-14" aria-hidden="true" tabindex="-1"></a>  hidden_dropped <span class="ot">=</span> hidden_dropout<span class="sc">$</span>out</span>
<span id="cb24-15"><a href="#cb24-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-16"><a href="#cb24-16" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Second linear layer</span></span>
<span id="cb24-17"><a href="#cb24-17" aria-hidden="true" tabindex="-1"></a>  ff_out <span class="ot">=</span> hidden_dropped <span class="sc">%*%</span> W_ff2 <span class="sc">+</span> <span class="fu">matrix</span>(b_ff2, <span class="at">nrow =</span> <span class="fu">nrow</span>(hidden_dropped), <span class="at">ncol =</span> <span class="fu">ncol</span>(W_ff2), <span class="at">byrow =</span> <span class="cn">TRUE</span>)</span>
<span id="cb24-18"><a href="#cb24-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-19"><a href="#cb24-19" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Dropout after FF block output (before residual)</span></span>
<span id="cb24-20"><a href="#cb24-20" aria-hidden="true" tabindex="-1"></a>  ff_out_dropout <span class="ot">=</span> <span class="fu">dropout_forward</span>(ff_out, dropout_rate, is_training)</span>
<span id="cb24-21"><a href="#cb24-21" aria-hidden="true" tabindex="-1"></a>  ff_out_dropped <span class="ot">=</span> ff_out_dropout<span class="sc">$</span>out</span>
<span id="cb24-22"><a href="#cb24-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-23"><a href="#cb24-23" aria-hidden="true" tabindex="-1"></a>  out <span class="ot">=</span> x_in <span class="sc">+</span> ff_out_dropped</span>
<span id="cb24-24"><a href="#cb24-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-25"><a href="#cb24-25" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(<span class="fu">list</span>(</span>
<span id="cb24-26"><a href="#cb24-26" aria-hidden="true" tabindex="-1"></a>    <span class="at">out =</span> out,</span>
<span id="cb24-27"><a href="#cb24-27" aria-hidden="true" tabindex="-1"></a>    <span class="at">ff_out_dropped =</span> ff_out_dropped,</span>
<span id="cb24-28"><a href="#cb24-28" aria-hidden="true" tabindex="-1"></a>    <span class="at">ff_out_dropout_mask =</span> ff_out_dropout<span class="sc">$</span>mask,</span>
<span id="cb24-29"><a href="#cb24-29" aria-hidden="true" tabindex="-1"></a>    <span class="at">hidden =</span> hidden,</span>
<span id="cb24-30"><a href="#cb24-30" aria-hidden="true" tabindex="-1"></a>    <span class="at">hidden_activated =</span> hidden_activated,</span>
<span id="cb24-31"><a href="#cb24-31" aria-hidden="true" tabindex="-1"></a>    <span class="at">hidden_dropout_mask =</span> hidden_dropout<span class="sc">$</span>mask,</span>
<span id="cb24-32"><a href="#cb24-32" aria-hidden="true" tabindex="-1"></a>    <span class="at">ln_cache_x =</span> ln_out_x</span>
<span id="cb24-33"><a href="#cb24-33" aria-hidden="true" tabindex="-1"></a>  ))</span>
<span id="cb24-34"><a href="#cb24-34" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb24-35"><a href="#cb24-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-36"><a href="#cb24-36" aria-hidden="true" tabindex="-1"></a>feed_forward_backward <span class="ot">=</span> <span class="cf">function</span>(dout, x_in, ff_out_dropped, ff_out_dropout_mask, hidden, hidden_activated, hidden_dropout_mask,</span>
<span id="cb24-37"><a href="#cb24-37" aria-hidden="true" tabindex="-1"></a>                                 ln_cache_x, W_ff1, b_ff1, W_ff2, b_ff2, ln_gamma, ln_beta, dropout_rate) {</span>
<span id="cb24-38"><a href="#cb24-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-39"><a href="#cb24-39" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Backward through dropout on ff_out</span></span>
<span id="cb24-40"><a href="#cb24-40" aria-hidden="true" tabindex="-1"></a>  d_ff_out <span class="ot">=</span> <span class="fu">dropout_backward</span>(dout, ff_out_dropout_mask, dropout_rate)</span>
<span id="cb24-41"><a href="#cb24-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-42"><a href="#cb24-42" aria-hidden="true" tabindex="-1"></a>  d_W_ff2 <span class="ot">=</span> <span class="fu">t</span>(hidden_activated) <span class="sc">%*%</span> d_ff_out <span class="co"># Use original hidden_activated here for grad of W_ff2</span></span>
<span id="cb24-43"><a href="#cb24-43" aria-hidden="true" tabindex="-1"></a>  d_b_ff2 <span class="ot">=</span> <span class="fu">colSums</span>(d_ff_out)</span>
<span id="cb24-44"><a href="#cb24-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-45"><a href="#cb24-45" aria-hidden="true" tabindex="-1"></a>  d_hidden_dropped <span class="ot">=</span> d_ff_out <span class="sc">%*%</span> <span class="fu">t</span>(W_ff2)</span>
<span id="cb24-46"><a href="#cb24-46" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Backward through dropout on hidden</span></span>
<span id="cb24-47"><a href="#cb24-47" aria-hidden="true" tabindex="-1"></a>  d_hidden_activated <span class="ot">=</span> <span class="fu">dropout_backward</span>(d_hidden_dropped, hidden_dropout_mask, dropout_rate)</span>
<span id="cb24-48"><a href="#cb24-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-49"><a href="#cb24-49" aria-hidden="true" tabindex="-1"></a>  d_hidden <span class="ot">=</span> d_hidden_activated <span class="sc">*</span> <span class="fu">relu_grad</span>(hidden)</span>
<span id="cb24-50"><a href="#cb24-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-51"><a href="#cb24-51" aria-hidden="true" tabindex="-1"></a>  d_W_ff1 <span class="ot">=</span> <span class="fu">t</span>(ln_cache_x<span class="sc">$</span>out) <span class="sc">%*%</span> d_hidden</span>
<span id="cb24-52"><a href="#cb24-52" aria-hidden="true" tabindex="-1"></a>  d_b_ff1 <span class="ot">=</span> <span class="fu">colSums</span>(d_hidden)</span>
<span id="cb24-53"><a href="#cb24-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-54"><a href="#cb24-54" aria-hidden="true" tabindex="-1"></a>  d_x_from_ff <span class="ot">=</span> d_hidden <span class="sc">%*%</span> <span class="fu">t</span>(W_ff1)</span>
<span id="cb24-55"><a href="#cb24-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-56"><a href="#cb24-56" aria-hidden="true" tabindex="-1"></a>  ln_grads_x <span class="ot">=</span> <span class="fu">layer_norm_backward</span>(d_x_from_ff, ln_cache_x<span class="sc">$</span>X, ln_cache_x<span class="sc">$</span>X_norm,</span>
<span id="cb24-57"><a href="#cb24-57" aria-hidden="true" tabindex="-1"></a>                                   ln_cache_x<span class="sc">$</span>mean_X, ln_cache_x<span class="sc">$</span>var_X, ln_gamma, <span class="at">epsilon =</span> <span class="fl">1e-5</span>)</span>
<span id="cb24-58"><a href="#cb24-58" aria-hidden="true" tabindex="-1"></a>  d_ln2_gamma <span class="ot">=</span> ln_grads_x<span class="sc">$</span>dgamma</span>
<span id="cb24-59"><a href="#cb24-59" aria-hidden="true" tabindex="-1"></a>  d_ln2_beta <span class="ot">=</span> ln_grads_x<span class="sc">$</span>dbeta</span>
<span id="cb24-60"><a href="#cb24-60" aria-hidden="true" tabindex="-1"></a>  d_x <span class="ot">=</span> dout <span class="sc">+</span> ln_grads_x<span class="sc">$</span>d_X</span>
<span id="cb24-61"><a href="#cb24-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-62"><a href="#cb24-62" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(<span class="fu">list</span>(</span>
<span id="cb24-63"><a href="#cb24-63" aria-hidden="true" tabindex="-1"></a>    <span class="at">d_x =</span> d_x, <span class="at">d_W_ff1 =</span> d_W_ff1, <span class="at">d_b_ff1 =</span> d_b_ff1,</span>
<span id="cb24-64"><a href="#cb24-64" aria-hidden="true" tabindex="-1"></a>    <span class="at">d_W_ff2 =</span> d_W_ff2, <span class="at">d_b_ff2 =</span> d_b_ff2,</span>
<span id="cb24-65"><a href="#cb24-65" aria-hidden="true" tabindex="-1"></a>    <span class="at">d_ln2_gamma =</span> d_ln2_gamma, <span class="at">d_ln2_beta =</span> d_ln2_beta</span>
<span id="cb24-66"><a href="#cb24-66" aria-hidden="true" tabindex="-1"></a>  ))</span>
<span id="cb24-67"><a href="#cb24-67" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="transformer-model-forward-pass-and-loss" class="level3">
<h3 class="anchored" data-anchor-id="transformer-model-forward-pass-and-loss">Transformer Model Forward Pass and Loss</h3>
<p>This function puts together all the pieces we’ve built previously to get a single forward pass for the full model. It starts by getting the token embeddings for the input data (from the C matrix parameter) and adding the position embedding parameter data. This feeds into the single transformer block for this model (one layer of multi-headed self-attention, followed by a feed forward neural network with a single hidden layer). Then finally one last linear layer which converts the inputs into probabilities over the vocabulary space.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a>transformer_forward <span class="ot">=</span> <span class="cf">function</span>(X_batch, y_batch, params, dropout_rate, is_training) {</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a>  <span class="co"># 1. Input and Positional Embeddings</span></span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a>  x_emb <span class="ot">=</span> <span class="fu">t</span>(<span class="fu">sapply</span>(<span class="dv">1</span><span class="sc">:</span>block_size, <span class="cf">function</span>(j) params<span class="sc">$</span>C[X_batch[<span class="dv">1</span>, j], ]))</span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a>  x <span class="ot">=</span> x_emb <span class="sc">+</span> params<span class="sc">$</span>P_emb</span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-6"><a href="#cb25-6" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Dropout after combined embeddings</span></span>
<span id="cb25-7"><a href="#cb25-7" aria-hidden="true" tabindex="-1"></a>  embed_dropout_out <span class="ot">=</span> <span class="fu">dropout_forward</span>(x, dropout_rate, is_training)</span>
<span id="cb25-8"><a href="#cb25-8" aria-hidden="true" tabindex="-1"></a>  x <span class="ot">=</span> embed_dropout_out<span class="sc">$</span>out</span>
<span id="cb25-9"><a href="#cb25-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-10"><a href="#cb25-10" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Transformer Block</span></span>
<span id="cb25-11"><a href="#cb25-11" aria-hidden="true" tabindex="-1"></a>  attn_out_cache <span class="ot">=</span> <span class="fu">multi_head_attention_forward</span>(x, params<span class="sc">$</span>Wq, params<span class="sc">$</span>Wk, params<span class="sc">$</span>Wv, params<span class="sc">$</span>Wo,</span>
<span id="cb25-12"><a href="#cb25-12" aria-hidden="true" tabindex="-1"></a>                                                n_head, head_size, params<span class="sc">$</span>ln1_gamma, params<span class="sc">$</span>ln1_beta,</span>
<span id="cb25-13"><a href="#cb25-13" aria-hidden="true" tabindex="-1"></a>                                                dropout_rate, is_training)</span>
<span id="cb25-14"><a href="#cb25-14" aria-hidden="true" tabindex="-1"></a>  x <span class="ot">=</span> attn_out_cache<span class="sc">$</span>out</span>
<span id="cb25-15"><a href="#cb25-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-16"><a href="#cb25-16" aria-hidden="true" tabindex="-1"></a>  ff_out_cache <span class="ot">=</span> <span class="fu">feed_forward_forward</span>(x, params<span class="sc">$</span>W_ff1, params<span class="sc">$</span>b_ff1, params<span class="sc">$</span>W_ff2, params<span class="sc">$</span>b_ff2,</span>
<span id="cb25-17"><a href="#cb25-17" aria-hidden="true" tabindex="-1"></a>                                     params<span class="sc">$</span>ln2_gamma, params<span class="sc">$</span>ln2_beta, dropout_rate, is_training)</span>
<span id="cb25-18"><a href="#cb25-18" aria-hidden="true" tabindex="-1"></a>  x <span class="ot">=</span> ff_out_cache<span class="sc">$</span>out</span>
<span id="cb25-19"><a href="#cb25-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-20"><a href="#cb25-20" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Final linear layer to get logits for all tokens in the sequence</span></span>
<span id="cb25-21"><a href="#cb25-21" aria-hidden="true" tabindex="-1"></a>  logits <span class="ot">=</span> x <span class="sc">%*%</span> params<span class="sc">$</span>W_final <span class="sc">+</span> <span class="fu">matrix</span>(params<span class="sc">$</span>b_final, <span class="at">nrow =</span> block_size, <span class="at">ncol =</span> vocab_size, <span class="at">byrow =</span> <span class="cn">TRUE</span>)</span>
<span id="cb25-22"><a href="#cb25-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-23"><a href="#cb25-23" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Softmax to get probabilities</span></span>
<span id="cb25-24"><a href="#cb25-24" aria-hidden="true" tabindex="-1"></a>  probs <span class="ot">=</span> <span class="fu">softmax_matrix_rows</span>(logits)</span>
<span id="cb25-25"><a href="#cb25-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-26"><a href="#cb25-26" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Select the probability of the true token for each entry in the flattened batch</span></span>
<span id="cb25-27"><a href="#cb25-27" aria-hidden="true" tabindex="-1"></a>  y_batch_flat <span class="ot">=</span> <span class="fu">as.vector</span>(<span class="fu">t</span>(y_batch))</span>
<span id="cb25-28"><a href="#cb25-28" aria-hidden="true" tabindex="-1"></a>  indices <span class="ot">=</span> <span class="fu">cbind</span>(<span class="dv">1</span><span class="sc">:</span><span class="fu">length</span>(y_batch_flat), y_batch_flat)</span>
<span id="cb25-29"><a href="#cb25-29" aria-hidden="true" tabindex="-1"></a>  correct_probs <span class="ot">=</span> probs[indices]</span>
<span id="cb25-30"><a href="#cb25-30" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb25-31"><a href="#cb25-31" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Compute Negative Log Likelihood / Cross-Entropy</span></span>
<span id="cb25-32"><a href="#cb25-32" aria-hidden="true" tabindex="-1"></a>  token_losses <span class="ot">=</span> <span class="sc">-</span><span class="fu">log</span>(correct_probs)</span>
<span id="cb25-33"><a href="#cb25-33" aria-hidden="true" tabindex="-1"></a>  loss <span class="ot">=</span> <span class="fu">mean</span>(token_losses)</span>
<span id="cb25-34"><a href="#cb25-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-35"><a href="#cb25-35" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(<span class="fu">list</span>(</span>
<span id="cb25-36"><a href="#cb25-36" aria-hidden="true" tabindex="-1"></a>    <span class="at">logits =</span> logits, <span class="at">probs =</span> probs, <span class="at">loss =</span> loss,</span>
<span id="cb25-37"><a href="#cb25-37" aria-hidden="true" tabindex="-1"></a>    <span class="at">x_emb =</span> x_emb,</span>
<span id="cb25-38"><a href="#cb25-38" aria-hidden="true" tabindex="-1"></a>    <span class="at">embed_dropout_mask =</span> embed_dropout_out<span class="sc">$</span>mask,</span>
<span id="cb25-39"><a href="#cb25-39" aria-hidden="true" tabindex="-1"></a>    <span class="at">attn_cache =</span> attn_out_cache,</span>
<span id="cb25-40"><a href="#cb25-40" aria-hidden="true" tabindex="-1"></a>    <span class="at">ff_cache =</span> ff_out_cache,</span>
<span id="cb25-41"><a href="#cb25-41" aria-hidden="true" tabindex="-1"></a>    <span class="at">last_x_output =</span> x</span>
<span id="cb25-42"><a href="#cb25-42" aria-hidden="true" tabindex="-1"></a>  ))</span>
<span id="cb25-43"><a href="#cb25-43" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="transformer-model-backward-pass" class="level3">
<h3 class="anchored" data-anchor-id="transformer-model-backward-pass">Transformer Model Backward Pass</h3>
<p>This function puts together all the pieces we’ve built previously to get a single backward pass for the full model, producing gradients for all the parameters. It works backwards from the final output of probabilities, computing gradients for each of the model parameters and the associated data, and passing the gradient for the input data back to each prior step along the series of model transformations.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>transformer_backward <span class="ot">=</span> <span class="cf">function</span>(cache, X_batch, y_batch, params, dropout_rate) {</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>  grads <span class="ot">=</span> <span class="fu">list</span>(</span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a>    <span class="at">d_C =</span> <span class="fu">matrix</span>(<span class="dv">0</span>, <span class="at">nrow =</span> <span class="fu">nrow</span>(params<span class="sc">$</span>C), <span class="at">ncol =</span> <span class="fu">ncol</span>(params<span class="sc">$</span>C)),</span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a>    <span class="at">d_P_emb =</span> <span class="fu">matrix</span>(<span class="dv">0</span>, <span class="at">nrow =</span> <span class="fu">nrow</span>(params<span class="sc">$</span>P_emb), <span class="at">ncol =</span> <span class="fu">ncol</span>(params<span class="sc">$</span>P_emb)),</span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a>    <span class="at">d_Wq =</span> <span class="fu">matrix</span>(<span class="dv">0</span>, <span class="at">nrow =</span> <span class="fu">nrow</span>(params<span class="sc">$</span>Wq), <span class="at">ncol =</span> <span class="fu">ncol</span>(params<span class="sc">$</span>Wq)),</span>
<span id="cb26-6"><a href="#cb26-6" aria-hidden="true" tabindex="-1"></a>    <span class="at">d_Wk =</span> <span class="fu">matrix</span>(<span class="dv">0</span>, <span class="at">nrow =</span> <span class="fu">nrow</span>(params<span class="sc">$</span>Wk), <span class="at">ncol =</span> <span class="fu">ncol</span>(params<span class="sc">$</span>Wk)),</span>
<span id="cb26-7"><a href="#cb26-7" aria-hidden="true" tabindex="-1"></a>    <span class="at">d_Wv =</span> <span class="fu">matrix</span>(<span class="dv">0</span>, <span class="at">nrow =</span> <span class="fu">nrow</span>(params<span class="sc">$</span>Wv), <span class="at">ncol =</span> <span class="fu">ncol</span>(params<span class="sc">$</span>Wv)),</span>
<span id="cb26-8"><a href="#cb26-8" aria-hidden="true" tabindex="-1"></a>    <span class="at">d_Wo =</span> <span class="fu">matrix</span>(<span class="dv">0</span>, <span class="at">nrow =</span> <span class="fu">nrow</span>(params<span class="sc">$</span>Wo), <span class="at">ncol =</span> <span class="fu">ncol</span>(params<span class="sc">$</span>Wo)),</span>
<span id="cb26-9"><a href="#cb26-9" aria-hidden="true" tabindex="-1"></a>    <span class="at">d_ln1_gamma =</span> <span class="fu">rep</span>(<span class="dv">0</span>, <span class="fu">length</span>(params<span class="sc">$</span>ln1_gamma)),</span>
<span id="cb26-10"><a href="#cb26-10" aria-hidden="true" tabindex="-1"></a>    <span class="at">d_ln1_beta =</span> <span class="fu">rep</span>(<span class="dv">0</span>, <span class="fu">length</span>(params<span class="sc">$</span>ln1_beta)),</span>
<span id="cb26-11"><a href="#cb26-11" aria-hidden="true" tabindex="-1"></a>    <span class="at">d_W_ff1 =</span> <span class="fu">matrix</span>(<span class="dv">0</span>, <span class="at">nrow =</span> <span class="fu">nrow</span>(params<span class="sc">$</span>W_ff1), <span class="at">ncol =</span> <span class="fu">ncol</span>(params<span class="sc">$</span>W_ff1)),</span>
<span id="cb26-12"><a href="#cb26-12" aria-hidden="true" tabindex="-1"></a>    <span class="at">d_b_ff1 =</span> <span class="fu">rep</span>(<span class="dv">0</span>, <span class="fu">length</span>(params<span class="sc">$</span>b_ff1)),</span>
<span id="cb26-13"><a href="#cb26-13" aria-hidden="true" tabindex="-1"></a>    <span class="at">d_W_ff2 =</span> <span class="fu">matrix</span>(<span class="dv">0</span>, <span class="at">nrow =</span> <span class="fu">nrow</span>(params<span class="sc">$</span>W_ff2), <span class="at">ncol =</span> <span class="fu">ncol</span>(params<span class="sc">$</span>W_ff2)),</span>
<span id="cb26-14"><a href="#cb26-14" aria-hidden="true" tabindex="-1"></a>    <span class="at">d_b_ff2 =</span> <span class="fu">rep</span>(<span class="dv">0</span>, <span class="fu">length</span>(params<span class="sc">$</span>b_ff2)),</span>
<span id="cb26-15"><a href="#cb26-15" aria-hidden="true" tabindex="-1"></a>    <span class="at">d_ln2_gamma =</span> <span class="fu">rep</span>(<span class="dv">0</span>, <span class="fu">length</span>(params<span class="sc">$</span>ln2_gamma)),</span>
<span id="cb26-16"><a href="#cb26-16" aria-hidden="true" tabindex="-1"></a>    <span class="at">d_ln2_beta =</span> <span class="fu">rep</span>(<span class="dv">0</span>, <span class="fu">length</span>(params<span class="sc">$</span>ln2_beta)),</span>
<span id="cb26-17"><a href="#cb26-17" aria-hidden="true" tabindex="-1"></a>    <span class="at">d_W_final =</span> <span class="fu">matrix</span>(<span class="dv">0</span>, <span class="at">nrow =</span> <span class="fu">nrow</span>(params<span class="sc">$</span>W_final), <span class="at">ncol =</span> <span class="fu">ncol</span>(params<span class="sc">$</span>W_final)),</span>
<span id="cb26-18"><a href="#cb26-18" aria-hidden="true" tabindex="-1"></a>    <span class="at">d_b_final =</span> <span class="fu">rep</span>(<span class="dv">0</span>, <span class="fu">length</span>(params<span class="sc">$</span>b_final))</span>
<span id="cb26-19"><a href="#cb26-19" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb26-20"><a href="#cb26-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-21"><a href="#cb26-21" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Transpose y_batch to get elements row by row, then flatten</span></span>
<span id="cb26-22"><a href="#cb26-22" aria-hidden="true" tabindex="-1"></a>  y_batch_flat <span class="ot">=</span> <span class="fu">as.vector</span>(<span class="fu">t</span>(y_batch))</span>
<span id="cb26-23"><a href="#cb26-23" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb26-24"><a href="#cb26-24" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Create one-hot targets for the flattened y_batch</span></span>
<span id="cb26-25"><a href="#cb26-25" aria-hidden="true" tabindex="-1"></a>  one_hot_targets_flat <span class="ot">=</span> <span class="fu">matrix</span>(<span class="dv">0</span>, <span class="at">nrow =</span> <span class="fu">length</span>(y_batch_flat), <span class="at">ncol =</span> vocab_size)</span>
<span id="cb26-26"><a href="#cb26-26" aria-hidden="true" tabindex="-1"></a>  one_hot_targets_flat[<span class="fu">cbind</span>(<span class="dv">1</span><span class="sc">:</span><span class="fu">length</span>(y_batch_flat), y_batch_flat)] <span class="ot">=</span> <span class="dv">1</span></span>
<span id="cb26-27"><a href="#cb26-27" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb26-28"><a href="#cb26-28" aria-hidden="true" tabindex="-1"></a>  d_logits <span class="ot">=</span> cache<span class="sc">$</span>probs <span class="sc">-</span> one_hot_targets_flat</span>
<span id="cb26-29"><a href="#cb26-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-30"><a href="#cb26-30" aria-hidden="true" tabindex="-1"></a>  d_W_final <span class="ot">=</span> <span class="fu">t</span>(cache<span class="sc">$</span>last_x_output) <span class="sc">%*%</span> d_logits</span>
<span id="cb26-31"><a href="#cb26-31" aria-hidden="true" tabindex="-1"></a>  d_b_final <span class="ot">=</span> <span class="fu">colSums</span>(d_logits)</span>
<span id="cb26-32"><a href="#cb26-32" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb26-33"><a href="#cb26-33" aria-hidden="true" tabindex="-1"></a>  grads<span class="sc">$</span>d_W_final <span class="ot">=</span> grads<span class="sc">$</span>d_W_final <span class="sc">+</span> d_W_final</span>
<span id="cb26-34"><a href="#cb26-34" aria-hidden="true" tabindex="-1"></a>  grads<span class="sc">$</span>d_b_final <span class="ot">=</span> grads<span class="sc">$</span>d_b_final <span class="sc">+</span> d_b_final</span>
<span id="cb26-35"><a href="#cb26-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-36"><a href="#cb26-36" aria-hidden="true" tabindex="-1"></a>  <span class="co"># d_last_token_output = d_logits %*% t(params$W_final)</span></span>
<span id="cb26-37"><a href="#cb26-37" aria-hidden="true" tabindex="-1"></a>  <span class="co"># </span></span>
<span id="cb26-38"><a href="#cb26-38" aria-hidden="true" tabindex="-1"></a>  <span class="co"># d_x_from_logits = matrix(0, nrow = block_size, ncol = n_embd)</span></span>
<span id="cb26-39"><a href="#cb26-39" aria-hidden="true" tabindex="-1"></a>  <span class="co"># d_x_from_logits[block_size, ] = d_last_token_output[1, ]</span></span>
<span id="cb26-40"><a href="#cb26-40" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb26-41"><a href="#cb26-41" aria-hidden="true" tabindex="-1"></a>  d_x_from_logits <span class="ot">=</span> d_logits <span class="sc">%*%</span> <span class="fu">t</span>(params<span class="sc">$</span>W_final)</span>
<span id="cb26-42"><a href="#cb26-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-43"><a href="#cb26-43" aria-hidden="true" tabindex="-1"></a>  ff_grads <span class="ot">=</span> <span class="fu">feed_forward_backward</span>(d_x_from_logits, cache<span class="sc">$</span>attn_cache<span class="sc">$</span>out, cache<span class="sc">$</span>ff_cache<span class="sc">$</span>ff_out_dropped,</span>
<span id="cb26-44"><a href="#cb26-44" aria-hidden="true" tabindex="-1"></a>                                   cache<span class="sc">$</span>ff_cache<span class="sc">$</span>ff_out_dropout_mask, cache<span class="sc">$</span>ff_cache<span class="sc">$</span>hidden,</span>
<span id="cb26-45"><a href="#cb26-45" aria-hidden="true" tabindex="-1"></a>                                   cache<span class="sc">$</span>ff_cache<span class="sc">$</span>hidden_activated, cache<span class="sc">$</span>ff_cache<span class="sc">$</span>hidden_dropout_mask,</span>
<span id="cb26-46"><a href="#cb26-46" aria-hidden="true" tabindex="-1"></a>                                   cache<span class="sc">$</span>ff_cache<span class="sc">$</span>ln_cache_x, params<span class="sc">$</span>W_ff1, params<span class="sc">$</span>b_ff1,</span>
<span id="cb26-47"><a href="#cb26-47" aria-hidden="true" tabindex="-1"></a>                                   params<span class="sc">$</span>W_ff2, params<span class="sc">$</span>b_ff2, params<span class="sc">$</span>ln2_gamma, params<span class="sc">$</span>ln2_beta, dropout_rate)</span>
<span id="cb26-48"><a href="#cb26-48" aria-hidden="true" tabindex="-1"></a>  d_x_from_ff <span class="ot">=</span> ff_grads<span class="sc">$</span>d_x</span>
<span id="cb26-49"><a href="#cb26-49" aria-hidden="true" tabindex="-1"></a>  grads<span class="sc">$</span>d_W_ff1 <span class="ot">=</span> grads<span class="sc">$</span>d_W_ff1 <span class="sc">+</span> ff_grads<span class="sc">$</span>d_W_ff1</span>
<span id="cb26-50"><a href="#cb26-50" aria-hidden="true" tabindex="-1"></a>  grads<span class="sc">$</span>d_b_ff1 <span class="ot">=</span> grads<span class="sc">$</span>d_b_ff1 <span class="sc">+</span> ff_grads<span class="sc">$</span>d_b_ff1</span>
<span id="cb26-51"><a href="#cb26-51" aria-hidden="true" tabindex="-1"></a>  grads<span class="sc">$</span>d_W_ff2 <span class="ot">=</span> grads<span class="sc">$</span>d_W_ff2 <span class="sc">+</span> ff_grads<span class="sc">$</span>d_W_ff2</span>
<span id="cb26-52"><a href="#cb26-52" aria-hidden="true" tabindex="-1"></a>  grads<span class="sc">$</span>d_b_ff2 <span class="ot">=</span> grads<span class="sc">$</span>d_b_ff2 <span class="sc">+</span> ff_grads<span class="sc">$</span>d_b_ff2</span>
<span id="cb26-53"><a href="#cb26-53" aria-hidden="true" tabindex="-1"></a>  grads<span class="sc">$</span>d_ln2_gamma <span class="ot">=</span> grads<span class="sc">$</span>d_ln2_gamma <span class="sc">+</span> ff_grads<span class="sc">$</span>d_ln2_gamma</span>
<span id="cb26-54"><a href="#cb26-54" aria-hidden="true" tabindex="-1"></a>  grads<span class="sc">$</span>d_ln2_beta <span class="ot">=</span> grads<span class="sc">$</span>d_ln2_beta <span class="sc">+</span> ff_grads<span class="sc">$</span>d_ln2_beta</span>
<span id="cb26-55"><a href="#cb26-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-56"><a href="#cb26-56" aria-hidden="true" tabindex="-1"></a>  attn_grads <span class="ot">=</span> <span class="fu">multi_head_attention_backward</span>(d_x_from_ff, cache<span class="sc">$</span>x_emb <span class="sc">+</span> params<span class="sc">$</span>P_emb,</span>
<span id="cb26-57"><a href="#cb26-57" aria-hidden="true" tabindex="-1"></a>                                             cache<span class="sc">$</span>attn_cache<span class="sc">$</span>attn_out_dropped, cache<span class="sc">$</span>attn_cache<span class="sc">$</span>attn_out_dropout_mask,</span>
<span id="cb26-58"><a href="#cb26-58" aria-hidden="true" tabindex="-1"></a>                                             cache<span class="sc">$</span>attn_cache<span class="sc">$</span>concat_heads, cache<span class="sc">$</span>attn_cache<span class="sc">$</span>outputs_per_head,</span>
<span id="cb26-59"><a href="#cb26-59" aria-hidden="true" tabindex="-1"></a>                                             cache<span class="sc">$</span>attn_cache<span class="sc">$</span>x_for_qkvo, cache<span class="sc">$</span>attn_cache<span class="sc">$</span>ln_cache_x,</span>
<span id="cb26-60"><a href="#cb26-60" aria-hidden="true" tabindex="-1"></a>                                             params<span class="sc">$</span>Wq, params<span class="sc">$</span>Wk, params<span class="sc">$</span>Wv, params<span class="sc">$</span>Wo, n_head, head_size,</span>
<span id="cb26-61"><a href="#cb26-61" aria-hidden="true" tabindex="-1"></a>                                             params<span class="sc">$</span>ln1_gamma, params<span class="sc">$</span>ln1_beta, dropout_rate)</span>
<span id="cb26-62"><a href="#cb26-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-63"><a href="#cb26-63" aria-hidden="true" tabindex="-1"></a>  d_x_from_attn <span class="ot">=</span> attn_grads<span class="sc">$</span>d_x</span>
<span id="cb26-64"><a href="#cb26-64" aria-hidden="true" tabindex="-1"></a>  grads<span class="sc">$</span>d_Wq <span class="ot">=</span> grads<span class="sc">$</span>d_Wq <span class="sc">+</span> attn_grads<span class="sc">$</span>d_Wq</span>
<span id="cb26-65"><a href="#cb26-65" aria-hidden="true" tabindex="-1"></a>  grads<span class="sc">$</span>d_Wk <span class="ot">=</span> grads<span class="sc">$</span>d_Wk <span class="sc">+</span> attn_grads<span class="sc">$</span>d_Wk</span>
<span id="cb26-66"><a href="#cb26-66" aria-hidden="true" tabindex="-1"></a>  grads<span class="sc">$</span>d_Wv <span class="ot">=</span> grads<span class="sc">$</span>d_Wv <span class="sc">+</span> attn_grads<span class="sc">$</span>d_Wv</span>
<span id="cb26-67"><a href="#cb26-67" aria-hidden="true" tabindex="-1"></a>  grads<span class="sc">$</span>d_Wo <span class="ot">=</span> grads<span class="sc">$</span>d_Wo <span class="sc">+</span> attn_grads<span class="sc">$</span>d_Wo</span>
<span id="cb26-68"><a href="#cb26-68" aria-hidden="true" tabindex="-1"></a>  grads<span class="sc">$</span>d_ln1_gamma <span class="ot">=</span> grads<span class="sc">$</span>d_ln1_gamma <span class="sc">+</span> attn_grads<span class="sc">$</span>d_ln1_gamma</span>
<span id="cb26-69"><a href="#cb26-69" aria-hidden="true" tabindex="-1"></a>  grads<span class="sc">$</span>d_ln1_beta <span class="ot">=</span> grads<span class="sc">$</span>d_ln1_beta <span class="sc">+</span> attn_grads<span class="sc">$</span>d_ln1_beta</span>
<span id="cb26-70"><a href="#cb26-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-71"><a href="#cb26-71" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Backward through embedding dropout</span></span>
<span id="cb26-72"><a href="#cb26-72" aria-hidden="true" tabindex="-1"></a>  d_x_from_attn_and_pos <span class="ot">=</span> <span class="fu">dropout_backward</span>(d_x_from_attn, cache<span class="sc">$</span>embed_dropout_mask, dropout_rate)</span>
<span id="cb26-73"><a href="#cb26-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-74"><a href="#cb26-74" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Gradient for Positional Embeddings</span></span>
<span id="cb26-75"><a href="#cb26-75" aria-hidden="true" tabindex="-1"></a>  grads<span class="sc">$</span>d_P_emb <span class="ot">=</span> grads<span class="sc">$</span>d_P_emb <span class="sc">+</span> d_x_from_attn_and_pos</span>
<span id="cb26-76"><a href="#cb26-76" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-77"><a href="#cb26-77" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Gradient for Character Embeddings (C)</span></span>
<span id="cb26-78"><a href="#cb26-78" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>block_size) {</span>
<span id="cb26-79"><a href="#cb26-79" aria-hidden="true" tabindex="-1"></a>    char_idx <span class="ot">=</span> X_batch[<span class="dv">1</span>, i]</span>
<span id="cb26-80"><a href="#cb26-80" aria-hidden="true" tabindex="-1"></a>    grads<span class="sc">$</span>d_C[char_idx, ] <span class="ot">=</span> grads<span class="sc">$</span>d_C[char_idx, ] <span class="sc">+</span> d_x_from_attn_and_pos[i, ]</span>
<span id="cb26-81"><a href="#cb26-81" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb26-82"><a href="#cb26-82" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-83"><a href="#cb26-83" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(grads)</span>
<span id="cb26-84"><a href="#cb26-84" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
</section>
<section id="training-loop" class="level2">
<h2 class="anchored" data-anchor-id="training-loop">Training Loop</h2>
<p>This is where we train the model by picking one example sequence of tokens at a time (batch size is 1 in this implementation), calculating the loss, and updating the model parameters with their calculated gradients. Gradient clipping is applied here as a preventative measure to address the potential for the exploding gradient problem. Training and validation loss are printed out after every 10000 training examples. Training loss is very noisy since it is calculated on only one training example at a time, as compared to validation loss which is calculated over the full validation dataset.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Training loop</span></span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (iter <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>n_iter) {</span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a>  <span class="co"># --- Training Step ---</span></span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Sample a random training example</span></span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a>  idx_train <span class="ot">=</span> <span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span><span class="fu">nrow</span>(X_train), <span class="dv">1</span>)</span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a>  X_batch_train <span class="ot">=</span> <span class="fu">matrix</span>(X_train[idx_train, ], <span class="at">nrow =</span> <span class="dv">1</span>)</span>
<span id="cb27-7"><a href="#cb27-7" aria-hidden="true" tabindex="-1"></a>  y_batch_train <span class="ot">=</span> <span class="fu">matrix</span>(y_train[idx_train, ], <span class="at">nrow =</span> <span class="dv">1</span>)</span>
<span id="cb27-8"><a href="#cb27-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-9"><a href="#cb27-9" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Forward pass (training mode)</span></span>
<span id="cb27-10"><a href="#cb27-10" aria-hidden="true" tabindex="-1"></a>  cache_train <span class="ot">=</span> <span class="fu">transformer_forward</span>(X_batch_train, y_batch_train, params, dropout_rate, <span class="at">is_training =</span> <span class="cn">TRUE</span>)</span>
<span id="cb27-11"><a href="#cb27-11" aria-hidden="true" tabindex="-1"></a>  loss_train <span class="ot">=</span> cache_train<span class="sc">$</span>loss</span>
<span id="cb27-12"><a href="#cb27-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-13"><a href="#cb27-13" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Backward pass</span></span>
<span id="cb27-14"><a href="#cb27-14" aria-hidden="true" tabindex="-1"></a>  grads <span class="ot">=</span> <span class="fu">transformer_backward</span>(cache_train, X_batch_train, y_batch_train, params, dropout_rate)</span>
<span id="cb27-15"><a href="#cb27-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-16"><a href="#cb27-16" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Apply Gradient Clipping</span></span>
<span id="cb27-17"><a href="#cb27-17" aria-hidden="true" tabindex="-1"></a>  clipped_grads <span class="ot">=</span> <span class="fu">clip_gradients_by_norm</span>(grads, gradient_clip_threshold)</span>
<span id="cb27-18"><a href="#cb27-18" aria-hidden="true" tabindex="-1"></a>  <span class="co"># clipped_grads = grads # or not</span></span>
<span id="cb27-19"><a href="#cb27-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-20"><a href="#cb27-20" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Update parameters using clipped gradients</span></span>
<span id="cb27-21"><a href="#cb27-21" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> (p_name <span class="cf">in</span> <span class="fu">names</span>(params)) {</span>
<span id="cb27-22"><a href="#cb27-22" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Check if a gradient for this parameter exists in clipped_grads</span></span>
<span id="cb27-23"><a href="#cb27-23" aria-hidden="true" tabindex="-1"></a>    grad_name <span class="ot">=</span> <span class="fu">paste0</span>(<span class="st">"d_"</span>, p_name)</span>
<span id="cb27-24"><a href="#cb27-24" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> (<span class="sc">!</span><span class="fu">is.null</span>(clipped_grads[[grad_name]])) {</span>
<span id="cb27-25"><a href="#cb27-25" aria-hidden="true" tabindex="-1"></a>      params[[p_name]] <span class="ot">=</span> params[[p_name]] <span class="sc">-</span> learning_rate <span class="sc">*</span> clipped_grads[[grad_name]]</span>
<span id="cb27-26"><a href="#cb27-26" aria-hidden="true" tabindex="-1"></a>    } <span class="cf">else</span> {</span>
<span id="cb27-27"><a href="#cb27-27" aria-hidden="true" tabindex="-1"></a>        <span class="co"># This case should ideally not be hit if `grads` contains all corresponding `d_` parameters</span></span>
<span id="cb27-28"><a href="#cb27-28" aria-hidden="true" tabindex="-1"></a>        <span class="co"># but included for robustness if `params` has entries not tracked by `grads`</span></span>
<span id="cb27-29"><a href="#cb27-29" aria-hidden="true" tabindex="-1"></a>        <span class="fu">cat</span>(<span class="st">"Warning: No gradient found for parameter:"</span>, p_name, <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span>
<span id="cb27-30"><a href="#cb27-30" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb27-31"><a href="#cb27-31" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb27-32"><a href="#cb27-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-33"><a href="#cb27-33" aria-hidden="true" tabindex="-1"></a>  <span class="co"># --- Validation Step ---</span></span>
<span id="cb27-34"><a href="#cb27-34" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> (iter <span class="sc">%%</span> <span class="dv">10000</span> <span class="sc">==</span> <span class="dv">0</span>) {</span>
<span id="cb27-35"><a href="#cb27-35" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Calculate validation loss (inference mode, no dropout)</span></span>
<span id="cb27-36"><a href="#cb27-36" aria-hidden="true" tabindex="-1"></a>    val_losses <span class="ot">=</span> <span class="fu">c</span>()</span>
<span id="cb27-37"><a href="#cb27-37" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> (<span class="fu">nrow</span>(X_val) <span class="sc">&gt;</span> <span class="dv">0</span>) { <span class="co"># Check if validation set is not empty</span></span>
<span id="cb27-38"><a href="#cb27-38" aria-hidden="true" tabindex="-1"></a>      <span class="cf">for</span> (val_idx <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">nrow</span>(X_val)) {</span>
<span id="cb27-39"><a href="#cb27-39" aria-hidden="true" tabindex="-1"></a>        X_batch_val <span class="ot">=</span> <span class="fu">matrix</span>(X_val[val_idx, ], <span class="at">nrow =</span> <span class="dv">1</span>)</span>
<span id="cb27-40"><a href="#cb27-40" aria-hidden="true" tabindex="-1"></a>        y_batch_val <span class="ot">=</span> <span class="fu">matrix</span>(y_val[val_idx, ], <span class="at">nrow =</span> <span class="dv">1</span>)</span>
<span id="cb27-41"><a href="#cb27-41" aria-hidden="true" tabindex="-1"></a>        cache_val <span class="ot">=</span> <span class="fu">transformer_forward</span>(X_batch_val, y_batch_val, params, dropout_rate, <span class="at">is_training =</span> <span class="cn">FALSE</span>)</span>
<span id="cb27-42"><a href="#cb27-42" aria-hidden="true" tabindex="-1"></a>        val_losses <span class="ot">=</span> <span class="fu">c</span>(val_losses, cache_val<span class="sc">$</span>loss)</span>
<span id="cb27-43"><a href="#cb27-43" aria-hidden="true" tabindex="-1"></a>      }</span>
<span id="cb27-44"><a href="#cb27-44" aria-hidden="true" tabindex="-1"></a>      avg_val_loss <span class="ot">=</span> <span class="fu">mean</span>(val_losses)</span>
<span id="cb27-45"><a href="#cb27-45" aria-hidden="true" tabindex="-1"></a>      <span class="fu">cat</span>(<span class="st">"Iteration:"</span>, iter, <span class="st">" Training Loss:"</span>, <span class="fu">round</span>(loss_train, <span class="dv">4</span>), <span class="st">" Validation Loss:"</span>, <span class="fu">round</span>(avg_val_loss, <span class="dv">4</span>), <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span>
<span id="cb27-46"><a href="#cb27-46" aria-hidden="true" tabindex="-1"></a>    } <span class="cf">else</span> {</span>
<span id="cb27-47"><a href="#cb27-47" aria-hidden="true" tabindex="-1"></a>      <span class="fu">cat</span>(<span class="st">"Iteration:"</span>, iter, <span class="st">" Training Loss:"</span>, <span class="fu">round</span>(loss_train, <span class="dv">4</span>), <span class="st">" (No validation data)</span><span class="sc">\n</span><span class="st">"</span>)</span>
<span id="cb27-48"><a href="#cb27-48" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb27-49"><a href="#cb27-49" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb27-50"><a href="#cb27-50" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Iteration: 10000  Training Loss: 3.6363  Validation Loss: 3.1276 
Iteration: 20000  Training Loss: 3.2004  Validation Loss: 3.0208 
Iteration: 30000  Training Loss: 2.7591  Validation Loss: 2.9624 
Iteration: 40000  Training Loss: 2.0192  Validation Loss: 2.906 
Iteration: 50000  Training Loss: 3.4338  Validation Loss: 2.8703 
Iteration: 60000  Training Loss: 3.1453  Validation Loss: 2.8311 
Iteration: 70000  Training Loss: 3.7388  Validation Loss: 2.8091 
Iteration: 80000  Training Loss: 2.8175  Validation Loss: 2.7793 
Iteration: 90000  Training Loss: 3.1989  Validation Loss: 2.7818 
Iteration: 100000  Training Loss: 2.6022  Validation Loss: 2.7582 </code></pre>
</div>
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"</span><span class="sc">\n</span><span class="st">Training complete. Final Training Loss:"</span>, <span class="fu">round</span>(loss_train, <span class="dv">4</span>), <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Training complete. Final Training Loss: 2.6022 </code></pre>
</div>
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> (<span class="fu">nrow</span>(X_val) <span class="sc">&gt;</span> <span class="dv">0</span>) {</span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">cat</span>(<span class="st">"Final Average Validation Loss:"</span>, <span class="fu">round</span>(avg_val_loss, <span class="dv">4</span>), <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Final Average Validation Loss: 2.7582 </code></pre>
</div>
</div>
</section>
<section id="generating-new-text" class="level2">
<h2 class="anchored" data-anchor-id="generating-new-text">Generating New Text</h2>
<p>The text generation function will run in inference mode (is_training = FALSE) to ensure no dropout is applied. It can be fed a starting sequence, or will pad with blank spaces if the sequence is shorter than expected or empty. It generates output similar to what it is trained on, so even if the starting sequence is very different from the training content, the output will still look like the training content. In this case, the model was trained on Shakespeare, so the output will look like Shakespeare even if the generation is seeded with text that looks very different from Shakespeare.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a>generate_text <span class="ot">=</span> <span class="cf">function</span>(current_params, start_string, num_characters_to_generate) {</span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a>  generated_sequence_indices <span class="ot">=</span> <span class="fu">encode</span>(start_string)</span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-4"><a href="#cb33-4" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> (<span class="fu">length</span>(generated_sequence_indices) <span class="sc">&lt;</span> block_size) {</span>
<span id="cb33-5"><a href="#cb33-5" aria-hidden="true" tabindex="-1"></a>    padded_start <span class="ot">=</span> <span class="fu">c</span>(<span class="fu">rep</span>(stoi[[<span class="st">" "</span>]], block_size <span class="sc">-</span> <span class="fu">length</span>(generated_sequence_indices)), generated_sequence_indices)</span>
<span id="cb33-6"><a href="#cb33-6" aria-hidden="true" tabindex="-1"></a>    generated_sequence_indices <span class="ot">=</span> padded_start</span>
<span id="cb33-7"><a href="#cb33-7" aria-hidden="true" tabindex="-1"></a>    <span class="fu">cat</span>(<span class="st">"Padded start string to block_size with spaces: '"</span>, <span class="fu">decode</span>(padded_start), <span class="st">"'</span><span class="sc">\n</span><span class="st">"</span>, <span class="at">sep =</span> <span class="st">""</span>)</span>
<span id="cb33-8"><a href="#cb33-8" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb33-9"><a href="#cb33-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-10"><a href="#cb33-10" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>num_characters_to_generate) {</span>
<span id="cb33-11"><a href="#cb33-11" aria-hidden="true" tabindex="-1"></a>    context_indices <span class="ot">=</span> <span class="fu">tail</span>(generated_sequence_indices, block_size)</span>
<span id="cb33-12"><a href="#cb33-12" aria-hidden="true" tabindex="-1"></a>    X_predict <span class="ot">=</span> <span class="fu">matrix</span>(context_indices, <span class="at">nrow =</span> <span class="dv">1</span>)</span>
<span id="cb33-13"><a href="#cb33-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-14"><a href="#cb33-14" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Simplified forward pass for inference (is_training = FALSE)</span></span>
<span id="cb33-15"><a href="#cb33-15" aria-hidden="true" tabindex="-1"></a>    x_emb_infer <span class="ot">=</span> <span class="fu">t</span>(<span class="fu">sapply</span>(<span class="dv">1</span><span class="sc">:</span>block_size, <span class="cf">function</span>(j) current_params<span class="sc">$</span>C[X_predict[<span class="dv">1</span>, j], ]))</span>
<span id="cb33-16"><a href="#cb33-16" aria-hidden="true" tabindex="-1"></a>    x_infer <span class="ot">=</span> x_emb_infer <span class="sc">+</span> current_params<span class="sc">$</span>P_emb</span>
<span id="cb33-17"><a href="#cb33-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-18"><a href="#cb33-18" aria-hidden="true" tabindex="-1"></a>    <span class="co"># No dropout applied during inference (dropout_forward handles this)</span></span>
<span id="cb33-19"><a href="#cb33-19" aria-hidden="true" tabindex="-1"></a>    x_infer <span class="ot">=</span> <span class="fu">dropout_forward</span>(x_infer, dropout_rate, <span class="at">is_training =</span> <span class="cn">FALSE</span>)<span class="sc">$</span>out</span>
<span id="cb33-20"><a href="#cb33-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-21"><a href="#cb33-21" aria-hidden="true" tabindex="-1"></a>    attn_out_infer <span class="ot">=</span> <span class="fu">multi_head_attention_forward</span>(x_infer, current_params<span class="sc">$</span>Wq, current_params<span class="sc">$</span>Wk, current_params<span class="sc">$</span>Wv, current_params<span class="sc">$</span>Wo,</span>
<span id="cb33-22"><a href="#cb33-22" aria-hidden="true" tabindex="-1"></a>                                                 n_head, head_size, current_params<span class="sc">$</span>ln1_gamma, current_params<span class="sc">$</span>ln1_beta,</span>
<span id="cb33-23"><a href="#cb33-23" aria-hidden="true" tabindex="-1"></a>                                                 dropout_rate, <span class="at">is_training =</span> <span class="cn">FALSE</span>)<span class="sc">$</span>out</span>
<span id="cb33-24"><a href="#cb33-24" aria-hidden="true" tabindex="-1"></a>    x_infer <span class="ot">=</span> attn_out_infer</span>
<span id="cb33-25"><a href="#cb33-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-26"><a href="#cb33-26" aria-hidden="true" tabindex="-1"></a>    ff_out_infer <span class="ot">=</span> <span class="fu">feed_forward_forward</span>(x_infer, current_params<span class="sc">$</span>W_ff1, current_params<span class="sc">$</span>b_ff1, current_params<span class="sc">$</span>W_ff2, current_params<span class="sc">$</span>b_ff2,</span>
<span id="cb33-27"><a href="#cb33-27" aria-hidden="true" tabindex="-1"></a>                                       current_params<span class="sc">$</span>ln2_gamma, current_params<span class="sc">$</span>ln2_beta,</span>
<span id="cb33-28"><a href="#cb33-28" aria-hidden="true" tabindex="-1"></a>                                       dropout_rate, <span class="at">is_training =</span> <span class="cn">FALSE</span>)<span class="sc">$</span>out</span>
<span id="cb33-29"><a href="#cb33-29" aria-hidden="true" tabindex="-1"></a>    x_infer <span class="ot">=</span> ff_out_infer</span>
<span id="cb33-30"><a href="#cb33-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-31"><a href="#cb33-31" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Final linear layer to get logits for the *last* token in the sequence</span></span>
<span id="cb33-32"><a href="#cb33-32" aria-hidden="true" tabindex="-1"></a>    last_token_output_infer <span class="ot">=</span> <span class="fu">matrix</span>(x_infer[block_size, ], <span class="at">nrow =</span> <span class="dv">1</span>)</span>
<span id="cb33-33"><a href="#cb33-33" aria-hidden="true" tabindex="-1"></a>    logits_infer <span class="ot">=</span> last_token_output_infer <span class="sc">%*%</span> current_params<span class="sc">$</span>W_final <span class="sc">+</span> <span class="fu">matrix</span>(current_params<span class="sc">$</span>b_final, <span class="at">nrow =</span> <span class="dv">1</span>, <span class="at">ncol =</span> vocab_size)</span>
<span id="cb33-34"><a href="#cb33-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-35"><a href="#cb33-35" aria-hidden="true" tabindex="-1"></a>    probs_infer <span class="ot">=</span> <span class="fu">softmax_matrix_rows</span>(logits_infer)</span>
<span id="cb33-36"><a href="#cb33-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-37"><a href="#cb33-37" aria-hidden="true" tabindex="-1"></a>    next_char_idx <span class="ot">=</span> <span class="fu">sample.int</span>(vocab_size, <span class="dv">1</span>, <span class="at">prob =</span> probs_infer)</span>
<span id="cb33-38"><a href="#cb33-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-39"><a href="#cb33-39" aria-hidden="true" tabindex="-1"></a>    generated_sequence_indices <span class="ot">=</span> <span class="fu">c</span>(generated_sequence_indices, next_char_idx)</span>
<span id="cb33-40"><a href="#cb33-40" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb33-41"><a href="#cb33-41" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(<span class="fu">decode</span>(generated_sequence_indices))</span>
<span id="cb33-42"><a href="#cb33-42" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb33-43"><a href="#cb33-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-44"><a href="#cb33-44" aria-hidden="true" tabindex="-1"></a><span class="co"># Example usage: Generate new characters starting with a given string</span></span>
<span id="cb33-45"><a href="#cb33-45" aria-hidden="true" tabindex="-1"></a>start_seq <span class="ot">=</span> <span class="st">"Charizard is my favorite Pokemon."</span></span>
<span id="cb33-46"><a href="#cb33-46" aria-hidden="true" tabindex="-1"></a>num_to_generate <span class="ot">=</span> <span class="dv">1000</span></span>
<span id="cb33-47"><a href="#cb33-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-48"><a href="#cb33-48" aria-hidden="true" tabindex="-1"></a>generated_text <span class="ot">=</span> <span class="fu">generate_text</span>(params, start_seq, num_to_generate)</span>
<span id="cb33-49"><a href="#cb33-49" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"</span><span class="sc">\n</span><span class="st">Generated text starting with '"</span>, start_seq, <span class="st">"':</span><span class="sc">\n</span><span class="st">"</span>, generated_text, <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>, <span class="at">sep =</span> <span class="st">""</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Generated text starting with 'Charizard is my favorite Pokemon.':
Charizard is my favorite Pokemon.B
S
Ahed,
Oshe un  bekoy tirde n min, w m s ta ktwicorrfandon, tal uorlee
N

T
Wls tiitind vsiw R
Be  f girun y ifrovh woln
Le w
F tiuns n cthen ins, geou tonrda nd congr hen mpe-rle aromisono, telsd
As adinen sseq wGicwp AhRO
RAe'.
Eite,
Thaloncis i'f- tind rgre tht theedis vrrtas, g ld ether,
Finl'd
UAh:Sli?AhnE
SArah
HHck T a,,
Wirw ale.od alr id r wRI-eatien'tathethyinrir,anvuthy,

Wil bdo an fe m.

Aig,
N:
Aa!Nloue bu ounacsin frlaag.dg, apeitconeoomeyertaond,
Heobellup sow? thasivr memongoosy kld onDimcy wur tthot n ead lvecon d-,
YUd
WOJn dgouin moe.UdAhoITiit ulvin wesir cper nod.
SGiv
Ty s y I eEQI geth kranye irorldetVOoathye'ot th ns aonofras tes cathpen,:
Aif.
CBs iulokega tou vShoylryheyerrertino, che capyy t. douus,

Yss ,
G p nndendev
Qouerego kirourehet fy fe do srhe ri sas, wnduldeaali y bloiy g souronths, le.
Gos IAuthe, g net:

IUSG!asot'ril bl,, IB wiworaf ple pesove ewe uran intes winof; b'roningl kesrk,.


Oingerodom? bisdthilan t:unl vAee

Fe kd:
IAare IOeae whou</code></pre>
</div>
</div>
<p>We can see that while the language is not correct, the model is producing text structured very similarly to the input Shakespeare text. The validation loss achieved here is worse than what Karpathy achieves in his implementation, but this is due to the choice to simplify several aspects of the model (batch size of 1, only a single transformer block instead of 3 or 4 in sequence) to make it easier to implement everything from scratch and keep the matrices 2-dimensional for easier manual inspection in RStudio’s global environment.</p>
<p>There are a number of improvements than could be made to this implementation. The most obvious ones are having a batch size greater than 1, and including multiple transformer blocks in sequence instead of just 1. Optimizing the weighting scheme in the gradient update to decay the learning rate or use some other algorithm like the Adam optimizer would also be a good choice. The reason this output doesn’t look as impressive as ChatGPT comes down to the complexity of the model. In this implementation, the count of final model parameters is 5489, which is very small in comparison to the count of 1.5 billion parameters in GPT-2 or 175 billion parameters in GPT-3.</p>
</section>
<section id="attention-plot" class="level2">
<h2 class="anchored" data-anchor-id="attention-plot">Attention Plot</h2>
<p>After training the model, there are a number of interesting visualizations we can create. One is an attention plot, where we pick an input sequence and show how the attention layer creates weights to pay attention to prior tokens over that sequence. We can examine the input sequence “methinks” which is a word that exists commonly in the input Shakespeare text. At this point, we will call a couple of libraries just for the plotting functionality.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidyr)</span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(gridExtra)</span>
<span id="cb35-4"><a href="#cb35-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-5"><a href="#cb35-5" aria-hidden="true" tabindex="-1"></a>attn_sequence <span class="ot">=</span> <span class="fu">encode</span>(<span class="st">"methinks"</span>)</span>
<span id="cb35-6"><a href="#cb35-6" aria-hidden="true" tabindex="-1"></a>X_predict <span class="ot">=</span> <span class="fu">matrix</span>(attn_sequence, <span class="at">nrow =</span> <span class="dv">1</span>)</span>
<span id="cb35-7"><a href="#cb35-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-8"><a href="#cb35-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Token and positional embedding</span></span>
<span id="cb35-9"><a href="#cb35-9" aria-hidden="true" tabindex="-1"></a>x_emb_infer <span class="ot">=</span> <span class="fu">t</span>(<span class="fu">sapply</span>(<span class="dv">1</span><span class="sc">:</span>block_size, <span class="cf">function</span>(j) params<span class="sc">$</span>C[X_predict[<span class="dv">1</span>, j], ]))</span>
<span id="cb35-10"><a href="#cb35-10" aria-hidden="true" tabindex="-1"></a>x_infer <span class="ot">=</span> x_emb_infer <span class="sc">+</span> params<span class="sc">$</span>P_emb</span>
<span id="cb35-11"><a href="#cb35-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-12"><a href="#cb35-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Layer normalization</span></span>
<span id="cb35-13"><a href="#cb35-13" aria-hidden="true" tabindex="-1"></a>ln_out_x <span class="ot">=</span> <span class="fu">layer_norm_forward</span>(x_infer, params<span class="sc">$</span>ln1_gamma, params<span class="sc">$</span>ln1_beta)</span>
<span id="cb35-14"><a href="#cb35-14" aria-hidden="true" tabindex="-1"></a>x <span class="ot">=</span> ln_out_x<span class="sc">$</span>out</span>
<span id="cb35-15"><a href="#cb35-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-16"><a href="#cb35-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Attention block</span></span>
<span id="cb35-17"><a href="#cb35-17" aria-hidden="true" tabindex="-1"></a>Q_all <span class="ot">=</span> x <span class="sc">%*%</span> Wq</span>
<span id="cb35-18"><a href="#cb35-18" aria-hidden="true" tabindex="-1"></a>K_all <span class="ot">=</span> x <span class="sc">%*%</span> Wk</span>
<span id="cb35-19"><a href="#cb35-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-20"><a href="#cb35-20" aria-hidden="true" tabindex="-1"></a>plots_per_head <span class="ot">=</span> <span class="fu">lapply</span>(<span class="dv">1</span><span class="sc">:</span>n_head, <span class="cf">function</span>(h) {</span>
<span id="cb35-21"><a href="#cb35-21" aria-hidden="true" tabindex="-1"></a>  start_col <span class="ot">=</span> (h <span class="sc">-</span> <span class="dv">1</span>) <span class="sc">*</span> head_size <span class="sc">+</span> <span class="dv">1</span></span>
<span id="cb35-22"><a href="#cb35-22" aria-hidden="true" tabindex="-1"></a>  end_col <span class="ot">=</span> h <span class="sc">*</span> head_size</span>
<span id="cb35-23"><a href="#cb35-23" aria-hidden="true" tabindex="-1"></a>  Q_h <span class="ot">=</span> Q_all[, start_col<span class="sc">:</span>end_col]</span>
<span id="cb35-24"><a href="#cb35-24" aria-hidden="true" tabindex="-1"></a>  K_h <span class="ot">=</span> K_all[, start_col<span class="sc">:</span>end_col]</span>
<span id="cb35-25"><a href="#cb35-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-26"><a href="#cb35-26" aria-hidden="true" tabindex="-1"></a>  scores <span class="ot">=</span> (Q_h <span class="sc">%*%</span> <span class="fu">t</span>(K_h)) <span class="sc">/</span> <span class="fu">sqrt</span>(head_size)</span>
<span id="cb35-27"><a href="#cb35-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-28"><a href="#cb35-28" aria-hidden="true" tabindex="-1"></a>  mask_causal <span class="ot">=</span> <span class="fu">upper.tri</span>(<span class="fu">matrix</span>(<span class="dv">0</span>, <span class="fu">nrow</span>(scores), <span class="fu">ncol</span>(scores)), <span class="at">diag =</span> <span class="cn">FALSE</span>)</span>
<span id="cb35-29"><a href="#cb35-29" aria-hidden="true" tabindex="-1"></a>  scores[mask_causal] <span class="ot">=</span> <span class="sc">-</span><span class="cn">Inf</span></span>
<span id="cb35-30"><a href="#cb35-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-31"><a href="#cb35-31" aria-hidden="true" tabindex="-1"></a>  attention_weights <span class="ot">=</span> <span class="fu">softmax_matrix_rows</span>(scores)</span>
<span id="cb35-32"><a href="#cb35-32" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb35-33"><a href="#cb35-33" aria-hidden="true" tabindex="-1"></a>  <span class="co"># make plot</span></span>
<span id="cb35-34"><a href="#cb35-34" aria-hidden="true" tabindex="-1"></a>  df <span class="ot">=</span> <span class="fu">as.data.frame</span>(attention_weights)</span>
<span id="cb35-35"><a href="#cb35-35" aria-hidden="true" tabindex="-1"></a>  <span class="fu">colnames</span>(df) <span class="ot">=</span> <span class="fu">c</span>(<span class="st">"m"</span>,<span class="st">"e"</span>,<span class="st">"t"</span>,<span class="st">"h"</span>,<span class="st">"i"</span>,<span class="st">"n"</span>,<span class="st">"k"</span>,<span class="st">"s"</span>)</span>
<span id="cb35-36"><a href="#cb35-36" aria-hidden="true" tabindex="-1"></a>  df<span class="sc">$</span>row <span class="ot">=</span> <span class="fu">c</span>(<span class="st">"m"</span>,<span class="st">"e"</span>,<span class="st">"t"</span>,<span class="st">"h"</span>,<span class="st">"i"</span>,<span class="st">"n"</span>,<span class="st">"k"</span>,<span class="st">"s"</span>)</span>
<span id="cb35-37"><a href="#cb35-37" aria-hidden="true" tabindex="-1"></a>  df <span class="ot">=</span> tidyr<span class="sc">::</span><span class="fu">pivot_longer</span>(df, <span class="sc">-</span>row, <span class="at">names_to =</span> <span class="st">"col"</span>, <span class="at">values_to =</span> <span class="st">"val"</span>)</span>
<span id="cb35-38"><a href="#cb35-38" aria-hidden="true" tabindex="-1"></a>  df<span class="sc">$</span>row <span class="ot">=</span> <span class="fu">factor</span>(df<span class="sc">$</span>row, <span class="at">levels =</span> <span class="fu">rev</span>(<span class="fu">c</span>(<span class="st">"m"</span>,<span class="st">"e"</span>,<span class="st">"t"</span>,<span class="st">"h"</span>,<span class="st">"i"</span>,<span class="st">"n"</span>,<span class="st">"k"</span>,<span class="st">"s"</span>)))</span>
<span id="cb35-39"><a href="#cb35-39" aria-hidden="true" tabindex="-1"></a>  df<span class="sc">$</span>col <span class="ot">=</span> <span class="fu">factor</span>(df<span class="sc">$</span>col, <span class="at">levels =</span> <span class="fu">c</span>(<span class="st">"m"</span>,<span class="st">"e"</span>,<span class="st">"t"</span>,<span class="st">"h"</span>,<span class="st">"i"</span>,<span class="st">"n"</span>,<span class="st">"k"</span>,<span class="st">"s"</span>))</span>
<span id="cb35-40"><a href="#cb35-40" aria-hidden="true" tabindex="-1"></a>  p <span class="ot">=</span> <span class="fu">ggplot</span>(df, <span class="fu">aes</span>(<span class="at">x =</span> col, <span class="at">y =</span> row, <span class="at">fill =</span> val)) <span class="sc">+</span></span>
<span id="cb35-41"><a href="#cb35-41" aria-hidden="true" tabindex="-1"></a>      <span class="fu">geom_tile</span>() <span class="sc">+</span></span>
<span id="cb35-42"><a href="#cb35-42" aria-hidden="true" tabindex="-1"></a>      <span class="fu">scale_fill_viridis_c</span>() <span class="sc">+</span></span>
<span id="cb35-43"><a href="#cb35-43" aria-hidden="true" tabindex="-1"></a>      <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">"prior tokens in sequence"</span>, <span class="at">y =</span> <span class="st">"input token"</span>,</span>
<span id="cb35-44"><a href="#cb35-44" aria-hidden="true" tabindex="-1"></a>           <span class="at">fill =</span> <span class="st">"weights"</span>, <span class="at">title =</span> <span class="fu">paste0</span>(<span class="st">"Attention Head #"</span>, h)) <span class="sc">+</span></span>
<span id="cb35-45"><a href="#cb35-45" aria-hidden="true" tabindex="-1"></a>      <span class="fu">theme</span>(<span class="at">plot.title =</span> <span class="fu">element_text</span>(<span class="at">size =</span> <span class="dv">11</span>))</span>
<span id="cb35-46"><a href="#cb35-46" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb35-47"><a href="#cb35-47" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Return plot object to list output</span></span>
<span id="cb35-48"><a href="#cb35-48" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(p)</span>
<span id="cb35-49"><a href="#cb35-49" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb35-50"><a href="#cb35-50" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb35-51"><a href="#cb35-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-52"><a href="#cb35-52" aria-hidden="true" tabindex="-1"></a><span class="co"># Display plots in 2x2 grid</span></span>
<span id="cb35-53"><a href="#cb35-53" aria-hidden="true" tabindex="-1"></a><span class="fu">grid.arrange</span>(<span class="at">grobs =</span> plots_per_head, <span class="at">ncol =</span> <span class="dv">2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="llm_transformer_from_scratch_files/figure-html/attention-plot-1.png" class="img-fluid" width="672"></p>
</div>
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Save plot for thumbnail</span></span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a>g <span class="ot">=</span> <span class="fu">arrangeGrob</span>(<span class="at">grobs =</span> plots_per_head, <span class="at">ncol =</span> <span class="dv">2</span>)</span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a><span class="fu">ggsave</span>(<span class="at">file =</span> <span class="st">"thumbnail.jpg"</span>, g, <span class="at">width =</span> <span class="dv">6</span>, <span class="at">height =</span> <span class="dv">6</span><span class="sc">/</span><span class="fl">1.618</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Each of the four attention heads generates different weights during the self-attention process. The first token always gets a 100% weight since there are no other tokens at that point, but for later tokens we can see that the model has learned to pay attention (give higher weights to) tokens that are further back in the sequence, and which prior tokens are highly weighted differs from one attention head to the next, so they are providing meaningfully distinct information.</p>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>While implementing everything from scratch without relying on any packages adds significant complexity to this model, it offers a detailed understanding of the core transformer architecture. The process of implementing some of this core functionality led to a deeper understanding of how and why certain calculations are what they are (See derivations in earlier sections regarding He initialization and Layer Normalization). This example serves as an educational tool for delving into the fundamental mechanisms of attention, normalization, and regularization within deep learning models, all without external dependencies.</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>