[
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "California Fire Evacuations\n\n\n\n\n\nA proof of concept for the American Red Cross about making data driven predictions of shelter demand related to California wildfire evacuation orders\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nGenerative Art\n\n\n\n\n\nAn exploration of using R to produce artwork, from static images to animations\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nImpact Evaluation\n\n\n\n\n\nA summary of some topics in the field of impact evaluation from my volunteer work with the American Red Cross\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nLLM Transformer Implemented in Base R\n\n\n\n\n\nAn exercise in understanding the mechanics of the transformer architechture used in Large Language Models (LLMs)\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nRisk Parity Portfolio\n\n\n\n\n\nAn exercise in understanding the mechanics of the risk parity portfolio by implementing the algorithm myself\n\n\n\n\n \n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/llm_transformer_from_scratch/llm_transformer_from_scratch.html",
    "href": "projects/llm_transformer_from_scratch/llm_transformer_from_scratch.html",
    "title": "LLM Transformer Implemented in Base R",
    "section": "",
    "text": "One of the core components in AI models like ChatGPT is a language model. These language models are usually focused on predicting the next token in a sequence, and are used to generate language. For example, if the tokens are characters then given the sequence “orang” the model should predict the next character is “e”. One of the major innovations for these models is the attention mechanism introduced by Vaswani, Ashish, et al.(2017) in their paper “Attention is all you need”. This model architecture is called a transformer, and is what is used in the GPT (Generative Pre-trained Transformer) models in ChatGPT.\n\n\n\n\n\nIn the following work, I will be implementing the transformer architecture from scratch in base R. The figure above has both encoder and decoder blocks as it is the architecture used for translation tasks, but I will only be implementing the decoder block on the right part of the figure without the encoder block embedding section, since that is not necessary for the next token prediction task.\nMy work heavily references this YouTube video from Andrej Karpathy, as well as his related minGPT github project. Karpathy is well known for developing and teaching the first deep learning course at Stanford, CS 231n: Convolutional Neural Networks for Visual Recognition. His materials break down the core mechanics of the transformer model architecture using the pytorch library in Python. My implementation will differ by being built in R (which I am more familiar with than Python) as well as not relying on any libraries and implementing the backpropogation from scratch. This will make it slower and less efficient than Karpathy’s implementation which relies on the pytorch library to handle the backpropogation, but will help improve my understanding of the full process.\nFull disclosure, around 80% of the content in the code blocks was constructed with assistance from an AI model: Google’s Gemini 2.5 flash. I have implemented feed forward neural network models before, but this example is more complex than the proof of concepts I usually work on and Gemini was very helpful in figuring out how to structure the various functions and pass parameter information back and forth between them."
  },
  {
    "objectID": "projects/llm_transformer_from_scratch/llm_transformer_from_scratch.html#introoverview",
    "href": "projects/llm_transformer_from_scratch/llm_transformer_from_scratch.html#introoverview",
    "title": "LLM Transformer Implemented in Base R",
    "section": "",
    "text": "One of the core components in AI models like ChatGPT is a language model. These language models are usually focused on predicting the next token in a sequence, and are used to generate language. For example, if the tokens are characters then given the sequence “orang” the model should predict the next character is “e”. One of the major innovations for these models is the attention mechanism introduced by Vaswani, Ashish, et al.(2017) in their paper “Attention is all you need”. This model architecture is called a transformer, and is what is used in the GPT (Generative Pre-trained Transformer) models in ChatGPT.\n\n\n\n\n\nIn the following work, I will be implementing the transformer architecture from scratch in base R. The figure above has both encoder and decoder blocks as it is the architecture used for translation tasks, but I will only be implementing the decoder block on the right part of the figure without the encoder block embedding section, since that is not necessary for the next token prediction task.\nMy work heavily references this YouTube video from Andrej Karpathy, as well as his related minGPT github project. Karpathy is well known for developing and teaching the first deep learning course at Stanford, CS 231n: Convolutional Neural Networks for Visual Recognition. His materials break down the core mechanics of the transformer model architecture using the pytorch library in Python. My implementation will differ by being built in R (which I am more familiar with than Python) as well as not relying on any libraries and implementing the backpropogation from scratch. This will make it slower and less efficient than Karpathy’s implementation which relies on the pytorch library to handle the backpropogation, but will help improve my understanding of the full process.\nFull disclosure, around 80% of the content in the code blocks was constructed with assistance from an AI model: Google’s Gemini 2.5 flash. I have implemented feed forward neural network models before, but this example is more complex than the proof of concepts I usually work on and Gemini was very helpful in figuring out how to structure the various functions and pass parameter information back and forth between them."
  },
  {
    "objectID": "projects/llm_transformer_from_scratch/llm_transformer_from_scratch.html#setting-up",
    "href": "projects/llm_transformer_from_scratch/llm_transformer_from_scratch.html#setting-up",
    "title": "LLM Transformer Implemented in Base R",
    "section": "Setting Up",
    "text": "Setting Up\n\nData Preparation\nSince the intention is to follow Karpathy’s example it makes the most sense to use the same dataset, which is a text file containing the complete works of William Shakespeare. The original file can be found on the Project Gutenberg website, but the file Karpathy uses is much cleaner so it is easier to use his file directly.\n\n# text corpus\ntext_corpus = readLines(\"input.txt\")\n\nWarning in readLines(\"input.txt\"): incomplete final line found on 'input.txt'\n\ntext_corpus = paste(text_corpus, collapse = \"\\n\")\n\n# Create a vocabulary of unique characters\nchars = sort(unique(strsplit(text_corpus, \"\")[[1]]))\nvocab_size = length(chars)\n\n# Create a mapping from character to integer (stoi: string_to_int)\nstoi = setNames(1:vocab_size, chars)\n\n# Create a mapping from integer to character (itos: int_to_string)\nitos = setNames(chars, 1:vocab_size)\n\n# Helper functions for encoding and decoding\nencode = function(s) {\n  sapply(strsplit(s, \"\")[[1]], function(char) stoi[[char]])\n}\n\ndecode = function(l) {\n  paste(sapply(l, function(idx) itos[[as.character(idx)]]), collapse = \"\")\n}\n\ncat(\"Vocabulary size:\", vocab_size, \"\\n\")\n\nVocabulary size: 65 \n\ncat(\"Character to index mapping (first 5):\", paste(names(head(stoi, 5)), head(stoi, 5), sep = \":\", collapse = \", \"), \"\\n\")\n\nCharacter to index mapping (first 5): ':1, -:2,  :3, \n:4, !:5 \n\ncat(\"Example encoding of 'hello':\", encode(\"hello\"), \"\\n\")\n\nExample encoding of 'hello': 28 22 36 36 42 \n\ncat(\"Example decoding of [1 2 3 4 5]:\", decode(c(1, 2, 3, 4, 5)), \"\\n\")\n\nExample decoding of [1 2 3 4 5]: '- \n!"
  },
  {
    "objectID": "projects/llm_transformer_from_scratch/llm_transformer_from_scratch.html#hyperparameters-and-model-architecture",
    "href": "projects/llm_transformer_from_scratch/llm_transformer_from_scratch.html#hyperparameters-and-model-architecture",
    "title": "LLM Transformer Implemented in Base R",
    "section": "Hyperparameters and Model Architecture",
    "text": "Hyperparameters and Model Architecture\nUsually in a transformer model, one layer of multi-headed attention will be followed by one feed forward layer, and that will constitute one transformer block. Then there will be multiple transformer blocks in sequence that make up the model. In this example, only a single transformer block will be used for simplicity, since keeping track of all the parameters for the gradient update is already complex, even for only one block.\n\nblock_size = 8    # how many characters we consider for prediction\nn_embd = 16       # embedding dimension, also model dimension (d_model)\nn_head = 4        # number of attention heads\nhead_size = n_embd / n_head # dimension of each attention head\nlearning_rate = 0.01\nn_iter = 100000     # number of training iterations (more needed for real performance)\ndropout_rate = 0.10 # Dropout probability (e.g., 0.10 means 10% of neurons dropped)\ngradient_clip_threshold = 1.0 # Gradient clipping threshold\n# Batch size is 1 for simplicity in this from-scratch example.\n\n\nTraining and Validation Split\nThe data is split into 90% training data and 10% validation data. One important note is that the first 90% of the file is used as training while the last 10% is used for validation, there is no random sampling in creating this partition.\nThis is a deliberate choice due to the autoregressive nature of this model, if we used simple random sampling to make this split, we could lose the independence between training and validation by training on very similar data to what would exist in validation. As a simple example, let’s pretend we have a context window of 5 characters and the word “incredible” is in the dataset. We could end up with a case like this:\n\ninput: “incr”, target: “e”, split: training\ninput: “ncre”, target: “d”, split: validation\ninput: “cred”, target: “i”, split: training\n\nThe problem in this case is that the model has already been trained on most of that string already, so when it sees it in validation the performance will look similar to performance on the training set because the data is similar, and not because the model is actually generalizing well.\n\n# Create training examples (X, y)\ndata_encoded = encode(text_corpus)\n\nX_data = matrix(NA, nrow = length(data_encoded) - block_size, ncol = block_size)\ny_data = matrix(NA, nrow = length(data_encoded) - block_size, ncol = block_size)\n\nfor (i in 1:(length(data_encoded) - block_size)) {\n  X_data[i, ] = data_encoded[i:(i + block_size - 1)]\n  y_data[i, ] = data_encoded[(i + 1):(i + block_size)]\n}\n\n# create 90%/10% train/validation split\nnum_samples = nrow(X_data)\ntrain_indices = 1:floor(0.9 * num_samples)\nval_indices = setdiff(1:num_samples, train_indices)\n\n# debugging, try to intentionally overfit\n# train_indices = (1:20)*20\n# val_indices = (1:20)*20\n\nX_train = X_data[train_indices, ]\ny_train = y_data[train_indices, ]\nX_val = X_data[val_indices, ]\ny_val = y_data[val_indices, ]\n\ncat(\"Shape of X_train:\", dim(X_train), \"\\n\")\n\nShape of X_train: 1003846 8 \n\ncat(\"Shape of y_train:\", dim(y_train), \"\\n\")\n\nShape of y_train: 1003846 8 \n\ncat(\"Shape of X_val:\", dim(X_val), \"\\n\")\n\nShape of X_val: 111539 8 \n\ncat(\"Shape of y_val:\", dim(y_val), \"\\n\")\n\nShape of y_val: 111539 8 \n\n\n\n\nCore Utility Functions\nThese are helper functions that handle things like the softmax operation, neuron activation, dropout, etc. Most operations are being applied to matrix objects and we want the output to also be a matrix. There are a lot of scenarios where base R functions want to simplify the data type of the output and may try to return a vector or something else when the input was a matrix, so the matrix() function is used heavily here to ensure the output is still a matrix object.\n\nLayer Normalization\nI struggled a lot with the implementation of layer normalization, so it is worth adding some additional explanation here around some aspects I found confusing.\nLet’s assume the input data to LayerNorm is \\(X \\in R^{N \\times D}\\), where each row \\(X_i \\in R^D\\) is a sample. LayerNorm operates independently on each sample, so for an example of the forward pass we drop the batch index \\(i\\) while focusing on a single vector \\(x \\in R^D\\).\nForward Pass:\n\\[\n\\begin{aligned}\n\\mu &= \\frac{1}{D} \\sum_{j=1}^D x_j \\\\\n\\sigma^2 &= \\frac{1}{D} \\sum_{j=1}^D (x_j - \\mu)^2 \\\\\n\\hat{x_j} &= \\frac{x_j - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} \\\\\ny_j &= \\gamma_j \\hat{x_j} + \\beta_j\n\\end{aligned}\n\\]\nThe \\(y_j\\) elements are the output of LayerNorm. An important observation is that the third line above shows the normalization, which is happening over the rows of \\(X\\). Note how the first two lines calculating \\(\\mu\\) and \\(\\sigma^2\\) are over the dimension of the columns \\(D\\), so each row has its own mean and variance and is being normalized. However, the affine transformation in the fourth line shows parameters \\(\\gamma_j\\) and \\(\\beta_j\\) with the index \\(j\\) over the column space, so this last transformation is a scale and shift over the columns.\nI also had trouble understanding the backward pass for LayerNorm. The gradients for \\(\\gamma\\) and \\(\\beta\\) are fairly straightforward since they are just an affine transformation at the end, but passing the gradient back through to the input data was tricky, and this difference between transformations over rows vs columns complicates things somewhat.\nBackward Pass:\nWe want \\(\\frac{dL}{dx_j}\\), the gradient for the input row data.\nUsing the chain rule:\n\\[\n\\frac{dL}{dx_j} = \\sum_k \\frac{dL}{d\\hat{x_k}} \\frac{d\\hat{x_k}}{dx_j}\n\\]\nwhere \\(k\\) is a summation index (over columns) we are introducing to iterate over the columns, while \\(j\\) is the specific column index of the individual element within the row vector \\(x\\) that we are differentiating with respect to.\nComputing \\(\\frac{d \\hat{x_k}}{dx_j}\\):\nrecall that:\n\\[\n\\begin{aligned}\n\\mu &= \\frac{1}{D} \\sum_{k=1}^D x_k \\\\\n\\sigma^2 &= \\frac{1}{D} \\sum_{k=1}^D (x_k - \\mu)^2 \\\\\n\\hat{x_k} &= \\frac{x_k - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}}\n\\end{aligned}\n\\]\nThis has two dependencies, one directly through \\(x_k\\) as each element of \\(x_k\\) is \\(x_j\\) when \\(k=j\\), and also indirectly through \\(\\mu\\) and \\(\\sigma^2\\) which are functions of \\(x_j\\).\nFor the mean \\(\\mu\\):\n\\[\n\\frac{d\\mu}{dx_j} = \\frac{1}{D}\n\\]\nFor the variance \\(\\sigma^2\\):\n\\[\n\\begin{aligned}\n\\frac{d\\sigma^2}{dx_j} &= \\frac{1}{D} \\sum_{k=1}^D \\frac{d}{dx_j} (x_k - \\mu)^2 \\\\\n&= \\frac{1}{D} \\sum_{k=1}^D 2(x_k - \\mu) \\frac{d}{dx_j} (x_k - \\mu) \\\\\n&= \\frac{2}{D} \\sum_{k=1}^D (x_k - \\mu) (\\delta_{kj} - \\frac{1}{D})\n\\end{aligned}\n\\]\nNote that \\(\\frac{dx_k}{dx_j} = \\delta_{kj}\\) (1 if k=j, 0 otherwise)\nNow we have to think of that summation in 2 parts. First, when k=j, then \\(\\delta_{kj} = 1\\) and we have:\n\\[\n(x_k - \\mu) (\\delta_{kj} - \\frac{1}{D}) = (x_j - \\mu) (1 - \\frac{1}{D})\n\\]\nThen for the other terms where \\(k \\ne j\\):\n\\[\n\\sum_{k \\ne j} (x_k - \\mu) (\\delta_{kj} - \\frac{1}{D}) = - \\frac{1}{D} \\sum_{k \\ne j} (x_k - \\mu)\n\\]\nHowever, this second term can be simplified because summing over a vector minus its mean is zero.\n\\[\n\\sum_{k=1}^D (x_k - \\mu) = 0 \\\\\n\\sum_{k \\ne j} (x_k - \\mu) = -(x_j - \\mu)\n\\]\nSo the second term becomes\n\\[\n- \\frac{1}{D} * -(x_j - \\mu) = \\frac{1}{D} (x_j - \\mu)\n\\] then putting both parts back together gives us\n\\[\n\\begin{aligned}\n\\frac{d\\sigma^2}{dx_j} &= \\frac{2}{D}((x_j - \\mu) (1 - \\frac{1}{D}) + \\frac{1}{D} (x_j - \\mu)) \\\\\n&= \\frac{2}{D}(x_j - \\mu)\n\\end{aligned}\n\\]\nNow we can use the results for \\(\\frac{d\\mu}{dx_j}\\) and \\(\\frac{d\\sigma^2}{dx_j}\\) to calculate \\(\\frac{d\\hat{x_k}}{dx_j}\\). Using the quotient rule:\n\\[\n\\begin{aligned}\n\\frac{d\\hat{x_k}}{dx_j} &= (\\sqrt{\\sigma^2 + \\epsilon} \\frac{d}{dx_j} [(x_k - \\mu)] - (x_k - \\mu) \\frac{d}{dx_j} [\\sqrt{\\sigma^2 + \\epsilon}]) (\\sigma^2 + \\epsilon)^{-1} \\\\\n&= (\\sigma^2 + \\epsilon)^{-1/2}(\\delta_{kj} - \\frac{1}{D}) - (x_k - \\mu) \\frac{1}{2}(\\sigma^2 + \\epsilon)^{-3/2} \\frac{d\\sigma^2}{dx_j} \\\\\n&= (\\sigma^2 + \\epsilon)^{-1/2}(\\delta_{kj} - \\frac{1}{D}) - (x_k - \\mu) \\frac{1}{2}(\\sigma^2 + \\epsilon)^{-3/2} \\frac{2}{D}(x_j - \\mu) \\\\\n&= \\frac{1}{\\sqrt{\\sigma^2 + \\epsilon}} [\\delta_{kj} - \\frac{1}{D} - \\frac{(x_k - \\mu)(x_j - \\mu)}{(\\sigma^2 + \\epsilon)D}]\n\\end{aligned}\n\\]\nFinally, we can plug this back into the first equation for the gradient we are after. To simplify the notation slightly, we will say \\(\\frac{dL}{d\\hat{x_k}} = \\hat{g_k}\\)\n\\[\n\\begin{aligned}\n\\frac{dL}{dx_j} &= \\sum_k \\frac{dL}{d\\hat{x_k}} \\frac{d\\hat{x_k}}{dx_j} \\\\\n&= \\sum_k \\hat{g_k} \\frac{d\\hat{x_k}}{dx_j} \\\\\n&= \\sum_k \\hat{g_k} \\frac{1}{\\sqrt{\\sigma^2 + \\epsilon}} [\\delta_{kj} - \\frac{1}{D} - \\frac{(x_k - \\mu)(x_j - \\mu)}{(\\sigma^2 + \\epsilon)D}] \\\\\n&= \\frac{1}{\\sqrt{\\sigma^2 + \\epsilon}} (\\hat{g_j} - \\frac{1}{D} \\sum_k \\hat{g_k} - \\frac{(x_j - \\mu)}{(\\sigma^2 + \\epsilon)D} \\sum_k \\hat{g_k} (x_k - \\mu)\n\\end{aligned}\n\\]\nIf you look at the layer norm backward function implemented below, you can see that the above formula is what is used to calculate the gradient d_X.\n\n# Softmax activation function (row-wise for matrices)\nsoftmax_matrix_rows = function(X) {\n  t(apply(X, 1, function(row) {\n    exp_row = exp(row - max(row)) # Numerical stability\n    return(exp_row / sum(exp_row))\n  })) # wrapped with t() to transpose back to original shape\n}\n\n# Layer Normalization\nlayer_norm_forward = function(X, gamma, beta, epsilon = 1e-5) {\n  mean_X = apply(X, 1, mean)\n  var_X = apply(X, 1, var)\n\n  X_norm = (X - matrix(mean_X, nrow = nrow(X), ncol = ncol(X), byrow = FALSE)) /\n           sqrt(matrix(var_X + epsilon, nrow = nrow(X), ncol = ncol(X), byrow = FALSE))\n\n  out = X_norm * matrix(gamma, nrow = nrow(X), ncol = ncol(X), byrow = TRUE) +\n        matrix(beta, nrow = nrow(X), ncol = ncol(X), byrow = TRUE)\n\n  return(list(out = out, X_norm = X_norm, mean_X = mean_X, var_X = var_X, X = X)) # Store X for backward\n}\n\nlayer_norm_backward = function(dout, X, X_norm, mean_X, var_X, gamma, epsilon = 1e-5) {\n  N = nrow(X) # Number of samples (batch size)\n  D = ncol(X) # Number of features (embedding dimension)\n\n  # Gradients for gamma and beta\n  dgamma = colSums(dout * X_norm)\n  dbeta = colSums(dout)\n\n  # Gradient of X_norm\n  d_X_norm = dout * matrix(gamma, nrow = N, ncol = D, byrow = TRUE)\n\n  std_inv = 1 / sqrt(var_X + epsilon)  # (N,)\n  std_inv_mat = matrix(std_inv, nrow = N, ncol = D)\n\n  # Sum over features (columns) for each row\n  d_X_norm_sum = rowSums(d_X_norm)                   # shape (N,)\n  d_X_norm_dot_Xnorm = rowSums(d_X_norm * X_norm)    # shape (N,)\n\n  term1 = d_X_norm\n  term2 = matrix(d_X_norm_sum / D, nrow = N, ncol = D)\n  term3 = X_norm * matrix(d_X_norm_dot_Xnorm / D, nrow = N, ncol = D)\n\n  d_X = (term1 - term2 - term3) * std_inv_mat\n\n  return(list(d_X = d_X, dgamma = dgamma, dbeta = dbeta))\n}\n\n# ReLU activation function\nrelu = function(x) {\n  matrix(pmax(0, x), nrow = nrow(x), ncol = ncol(x))\n}\n\nrelu_grad = function(x) {\n  (x &gt; 0) * 1\n}\n\n# Dropout layer\ndropout_forward = function(x, dropout_rate, is_training) {\n  if (is_training) {\n    mask = matrix(runif(nrow(x)*ncol(x)), nrow = nrow(x), ncol = ncol(x)) &gt; dropout_rate\n    out = x * mask / (1 - dropout_rate) # Scale up during training\n    return(list(out = out, mask = mask))\n  } else {\n    return(list(out = x, mask = NULL)) # No dropout during inference\n  }\n}\n\ndropout_backward = function(dout, mask, dropout_rate) {\n  # Apply the same mask and scaling to the gradient\n  return(dout * mask / (1 - dropout_rate))\n}\n\n# He Initialization function for weights\nhe_init_weights = function(fan_in, fan_out, ReLU_activation = FALSE) {\n  # Only account for halving the variance if ReLU actiavtion is being applied\n  if(ReLU_activation){\n    std_dev = sqrt(2 / fan_in)\n  }else{\n    std_dev = sqrt(1 / fan_in)\n  }\n    \n  weights = matrix(rnorm(fan_in * fan_out, mean = 0, sd = std_dev),\n                   nrow = fan_in, ncol = fan_out)\n  return(weights)\n}\n\n# Gradient Clipping function (by global norm)\nclip_gradients_by_norm = function(gradients_list, clip_threshold) {\n  # Flatten all gradients into a single vector to calculate the norm\n  all_grad_elements = c()\n  for (grad_name in names(gradients_list)) {\n    all_grad_elements = c(all_grad_elements, as.vector(gradients_list[[grad_name]]))\n  }\n\n  global_norm = sqrt(sum(all_grad_elements^2))\n\n  # If the global norm exceeds the threshold, scale all gradients\n  if (global_norm &gt; clip_threshold) {\n    scale_factor = clip_threshold / global_norm\n    clipped_gradients = lapply(gradients_list, function(grad_matrix) {\n      grad_matrix * scale_factor\n    })\n    return(clipped_gradients)\n  } else {\n    # No clipping needed\n    return(gradients_list)\n  }\n}\n\n\n\n\nModel Parameters Initialization\nMany of the parameters will be initialized using He initialization. He initialization (also known as Kaiming initialization) is a weight initialization technique widely used in deep neural networks, especially when using Rectified Linear Unit (ReLU) as the activation function which we are doing here. The weights are determined like this:\n\\[\nW = N(0, \\sigma^2) \\text{ where } \\sigma = \\sqrt{\\frac{2}{n_{in}}}\n\\]\nThe basic idea is to choose starting weights that won’t immediately cause problems with vanishing or exploding gradients. We want to keep the variance of activations and gradients consistent as we move forward and backward through the layers in the network. Consider the simple feed forward layer as follows:\n\\[\n\\begin{aligned}\n&y = Wx + b \\\\\n&a = f(y)\n\\end{aligned}\n\\]\nwhere:\n\nx is the input vector to the layer.\nW is the weight matrix of the layer.\nb is the bias vector.\ny is the linear output before activation.\nf is the activation function (e.g., ReLU).\na is the activated output of the layer.\n\nFor simplicity, let’s assume the biases b are initialized to zero (which is common and is what we are doing here) and that the elements of x and W are independent and have zero mean.\nReLU is defined as f(y)=max(0,y). This means that for y&lt;0, the output is 0, and for y&gt;=0, the output is y. When initialized, roughly half of the inputs to a ReLU neuron will be negative (resulting in zero output) and half will be positive (resulting in the input value). This effectively halves the variance compared to a linear activation function.\nWe want the variance of the output a to be roughly equal to the variance of the input x.\nConsider the variance of y=Wx:\n\\[\nvar(y) = var(\\sum_{i=1}^{n_{in}} W_i x_i)\n\\]\nAssuming \\(W_i\\) and \\(x_i\\) are independent and have zero mean:\n\\[\n\\begin{aligned}\nvar(y) &= \\sum_{i=1}^{n_{in}} var(W_i x_i) \\\\\n&= \\sum_{i=1}^{n_{in}} E[(W_i x_i)^2] - E[(W_i x_i)]^2 \\\\\n&= \\sum_{i=1}^{n_{in}} E[W_i^2] E[x_i^2] - (E[W_i] E[x_i])^2 \\\\\n&= \\sum_{i=1}^{n_{in}} (var(W_i) + E[W_i]^2) (var(x_i) + E[x_i]^2) - (E[W_i] E[x_i])^2 \\\\\n&= \\sum_{i=1}^{n_{in}} (E[W_i]^2 var(x_i) + E[x_i]^2 var(W_i) + var(W_i)var(x_i))\n\\end{aligned}\n\\]\nSince we assume \\(E[W_i]=0\\) and \\(E[x_i]=0\\):\n\\[\n    var(y) = \\sum_{i=1}^{n_{in}} var(W_i)var(x_i)\n\\] Assuming all \\(W_i\\) and \\(x_i\\) are identically distributed:\n\\[\n    var(y) = n_{in} var(W)var(x)\n\\]\nNow, for ReLU, a=max(0,y). If y has zero mean and is symmetric around zero, then roughly half of the values of y will be positive and half will be negative. The negative values become zero, effectively reducing the variance by half. So, for the output after ReLU:\n\\[\n\\begin{aligned}\nvar(a) &\\approx \\frac{1}{2} var(y) \\\\\n&\\approx \\frac{1}{2} n_{in} var(W)var(x)\n\\end{aligned}\n\\]\nRemember the objective: We want the variance of the output a to be roughly equal to the variance of the input x. So we substitute \\(var(x)\\) for \\(var(a)\\) in this last equation, solving for the variance in weights \\(W\\) that achieve the desired outcome.\n\\[\n    var(x) \\approx \\frac{1}{2} n_{in} var(W)var(x)\n\\]\nand therefore:\n\\[\n\\begin{aligned}\n\\frac{1}{2} n_{in} var(W) &= 1 \\\\\nvar(W_i) &= \\frac{2}{n_{in}}\n\\end{aligned}\n\\]\nMost layers don’t have activation functions applied, so we only double the variance in the He initialization for the layer where the ReLU activation is used. In other cases, the same logic applies, just without the need to double the variance.\n\nset.seed(42) # For reproducibility\n\n# Character Embeddings - typically not He-initialized, using uniform\nC = matrix(runif(vocab_size * n_embd, -1, 1), nrow = vocab_size, ncol = n_embd)\n\n# Positional Embeddings - typically not He-initialized (often fixed or small random)\nP_emb = matrix(runif(block_size * n_embd, -1, 1), nrow = block_size, ncol = n_embd)\n\n# Multi-Head Attention Parameters (using He Initialization)\n# For Wq, Wk, Wv, fan_in is n_embd\nWq = he_init_weights(n_embd, n_head * head_size)\nWk = he_init_weights(n_embd, n_head * head_size)\nWv = he_init_weights(n_embd, n_head * head_size)\n# For Wo, fan_in is n_head * head_size\nWo = he_init_weights(n_head * head_size, n_embd)\n\n# LayerNorm for Attention - gamma initialized to 1s, beta to 0s\nln1_gamma = rep(1, n_embd)\nln1_beta = rep(0, n_embd)\n\n# Feed-Forward Network Parameters (using He Initialization)\n# For W_ff1, fan_in is n_embd\nW_ff1 = he_init_weights(n_embd, 4 * n_embd, ReLU_activation = TRUE)\nb_ff1 = rep(0, 4 * n_embd) # Biases are kept at 0\n# For W_ff2, fan_in is 4 * n_embd\nW_ff2 = he_init_weights(4 * n_embd, n_embd)\nb_ff2 = rep(0, n_embd) # Biases are kept at 0\n\n# LayerNorm for FF - gamma initialized to 1s, beta to 0s\nln2_gamma = rep(1, n_embd)\nln2_beta = rep(0, n_embd)\n\n# Final linear layer to logits (using He Initialization)\n# For W_final, fan_in is n_embd\nW_final = he_init_weights(n_embd, vocab_size)\nb_final = rep(0, vocab_size) # Biases are kept at 0\n\n# Store all parameters in a list for easier management\nparams = list(\n  C = C, P_emb = P_emb,\n  Wq = Wq, Wk = Wk, Wv = Wv, Wo = Wo,\n  ln1_gamma = ln1_gamma, ln1_beta = ln1_beta,\n  W_ff1 = W_ff1, b_ff1 = b_ff1, W_ff2 = W_ff2, b_ff2 = b_ff2,\n  ln2_gamma = ln2_gamma, ln2_beta = ln2_beta,\n  W_final = W_final, b_final = b_final\n)\n\ncat(\"Parameters initialized.\\n\")\n\nParameters initialized."
  },
  {
    "objectID": "projects/llm_transformer_from_scratch/llm_transformer_from_scratch.html#transformer-block-implementation-functions",
    "href": "projects/llm_transformer_from_scratch/llm_transformer_from_scratch.html#transformer-block-implementation-functions",
    "title": "LLM Transformer Implemented in Base R",
    "section": "Transformer Block Implementation Functions",
    "text": "Transformer Block Implementation Functions\n\nMulti-Head Self-Attention\nThe self-attention mechanism helps the model learn weights associated with prior tokens in the sequence. It could actually look forward as well if allowed to do so, like when translation is the desired function, but usually for next token prediction the attention mechanism is restricted to only look backwards at previous tokens. This is handled through the “is_causal” parameter in the function below, which will set the weights for any future token positions to zero during the softmax step.\nThe multi-headed piece of the mechanism is basically because multiple “heads” of the attention mechanism are run in parallel, and then the output is joined together in a final linear layer at the end. This lets the model learn more than one way to pay attention to previous tokens, if that is beneficial for the prediction.\nThis mechanism is summarized in figure 2 of Vaswani, Ashish, et al.(2017), reproduced here:\n\n\n\n\n\nThe key equation from that paper which we are implementing here is:\n\\[\n\\text{Attention}(Q, K, V) = \\text{softmax}(\\frac{QK^T}{\\sqrt{d_k}}) V\n\\]\nThe softmax function produces weights which are what determine how much attention is paid to previous tokens, while the V matrix contains the values emitted by those previous tokens.\n\nmulti_head_attention_forward = function(x, Wq, Wk, Wv, Wo, n_head, head_size, ln_gamma, ln_beta, dropout_rate, is_training, is_causal = TRUE) {\n  x_in = x\n\n  # Pre-LN:\n  ln_out_x = layer_norm_forward(x, ln_gamma, ln_beta)\n  x = ln_out_x$out\n\n  Q_all = x %*% Wq\n  K_all = x %*% Wk\n  V_all = x %*% Wv\n\n  outputs_per_head = lapply(1:n_head, function(h) {\n    start_col = (h - 1) * head_size + 1\n    end_col = h * head_size\n    Q_h = Q_all[, start_col:end_col]\n    K_h = K_all[, start_col:end_col]\n    V_h = V_all[, start_col:end_col]\n\n    scores = (Q_h %*% t(K_h)) / sqrt(head_size)\n\n    if (is_causal) {\n      mask_causal = upper.tri(matrix(0, nrow(scores), ncol(scores)), diag = FALSE)\n      scores[mask_causal] = -Inf\n    }\n\n    attention_weights = softmax_matrix_rows(scores)\n    # Dropout after attention weights (before multiplying by V)\n    attn_weights_dropout = dropout_forward(attention_weights, dropout_rate, is_training)\n    attention_weights_dropped = attn_weights_dropout$out\n\n    attended_values = attention_weights_dropped %*% V_h\n    return(list(attended_values = attended_values,\n                attention_weights = attention_weights, # Original for backward\n                attention_weights_dropped_mask = attn_weights_dropout$mask,\n                scores = scores, Q_h = Q_h, K_h = K_h, V_h = V_h))\n  })\n\n  concat_heads = do.call(cbind, lapply(outputs_per_head, function(o) o$attended_values))\n  attn_out = concat_heads %*% Wo\n\n  # Dropout after attention block output (before residual)\n  attn_out_dropout = dropout_forward(attn_out, dropout_rate, is_training)\n  attn_out_dropped = attn_out_dropout$out\n\n  out = x_in + attn_out_dropped\n\n  return(list(\n    out = out,\n    attn_out_dropped = attn_out_dropped,\n    attn_out_dropout_mask = attn_out_dropout$mask,\n    concat_heads = concat_heads,\n    outputs_per_head = outputs_per_head,\n    x_for_qkvo = x, # Input to Q, K, V linear layers (post-LN)\n    ln_cache_x = ln_out_x\n  ))\n}\n\nmulti_head_attention_backward = function(dout, x_in, attn_out_dropped, attn_out_dropout_mask, concat_heads,\n                                       outputs_per_head, x_for_qkvo, ln_cache_x, Wq, Wk, Wv, Wo, n_head, head_size, ln_gamma, ln_beta, dropout_rate, is_causal = TRUE) {\n\n  # Backward through dropout on attn_out\n  d_attn_out = dropout_backward(dout, attn_out_dropout_mask, dropout_rate)\n\n  d_Wo = t(concat_heads) %*% d_attn_out\n  d_concat_heads = d_attn_out %*% t(Wo)\n\n  d_Q_all = matrix(0, nrow = nrow(x_for_qkvo), ncol = ncol(Wq))\n  d_K_all = matrix(0, nrow = nrow(x_for_qkvo), ncol = ncol(Wk))\n  d_V_all = matrix(0, nrow = nrow(x_for_qkvo), ncol = ncol(Wv))\n\n  for (h in 1:n_head) {\n    head_cache = outputs_per_head[[h]]\n    d_attended_values = d_concat_heads[, ((h - 1) * head_size + 1):(h * head_size)]\n\n    # Backward through dropout on attention_weights\n    d_attention_weights_dropped = d_attended_values %*% t(head_cache$V_h)\n    d_attention_weights = dropout_backward(d_attention_weights_dropped, head_cache$attention_weights_dropped_mask, dropout_rate)\n\n    d_V_h = t(head_cache$attention_weights_dropped) %*% d_attended_values # Use dropped weights for V_h grad\n\n    # Backward through softmax (simplified Jacobian-vector product)\n    d_scores = d_attention_weights * head_cache$attention_weights # Element-wise multiply for part of softmax grad\n    d_scores = d_scores - rowSums(d_scores * head_cache$attention_weights) * head_cache$attention_weights # Sum over columns and apply second part\n\n    if (is_causal) {\n      mask_causal = upper.tri(matrix(0, nrow(d_scores), ncol(d_scores)), diag = FALSE)\n      d_scores[mask_causal] = 0\n    }\n\n    d_Q_h = (d_scores %*% head_cache$K_h) / sqrt(head_size)\n    d_K_h = (t(d_scores) %*% head_cache$Q_h) / sqrt(head_size)\n\n    d_Q_all[, ((h - 1) * head_size + 1):(h * head_size)] = d_Q_all[, ((h - 1) * head_size + 1):(h * head_size)] + d_Q_h\n    d_K_all[, ((h - 1) * head_size + 1):(h * head_size)] = d_K_all[, ((h - 1) * head_size + 1):(h * head_size)] + d_K_h\n    d_V_all[, ((h - 1) * head_size + 1):(h * head_size)] = d_V_all[, ((h - 1) * head_size + 1):(h * head_size)] + d_V_h\n  }\n\n  d_Wq = t(x_for_qkvo) %*% d_Q_all\n  d_Wk = t(x_for_qkvo) %*% d_K_all\n  d_Wv = t(x_for_qkvo) %*% d_V_all\n\n  d_x_from_qkvo = (d_Q_all %*% t(Wq)) + (d_K_all %*% t(Wk)) + (d_V_all %*% t(Wv))\n\n  ln_grads_x = layer_norm_backward(d_x_from_qkvo, ln_cache_x$X, ln_cache_x$X_norm,\n                                   ln_cache_x$mean_X, ln_cache_x$var_X, ln_gamma, epsilon = 1e-5)\n  d_ln1_gamma = ln_grads_x$dgamma\n  d_ln1_beta = ln_grads_x$dbeta\n  d_x = dout + ln_grads_x$d_X\n\n  return(list(\n    d_x = d_x, d_Wq = d_Wq, d_Wk = d_Wk, d_Wv = d_Wv, d_Wo = d_Wo,\n    d_ln1_gamma = d_ln1_gamma, d_ln1_beta = d_ln1_beta\n  ))\n}\n\n\n\nPosition-wise Feed-Forward Network\nThis is a fully connected feed forward neural network with a single hidden layer. An activation function (ReLU in this case) is applied to the hidden layer. The hidden layer is 4 times the size of the input and output layers in this implementation, but there is no strong reason that needs to be the case. This image shows a common visualization for how this type of network looks.\n\n\n\n\n\n\\[\n\\begin{aligned}\n&a = f(W_1x + b_1) \\\\\n&y = W_2a + b_2\n\\end{aligned}\n\\]\nwhere:\n\nx is the input vector to the layer.\nW1 and W2 are the weight matrices into and out of the hidden layer.\nb1 and b2 are the bias vectors.\nf is the activation function (e.g., ReLU).\na is the activated output of the hidden layer.\ny is the linear output after activation.\n\nIn the visualization above, the rows of x are the nodes on the left, they become the nodes a in the center after the first transformation, and then become the output y after the second transformation.\nGiven that the transformations are all linear and the activation function ReLU(x) = max(x, 0) is simple to work with, the implementation is fairly straightforward.\n\nfeed_forward_forward = function(x, W_ff1, b_ff1, W_ff2, b_ff2, ln_gamma, ln_beta, dropout_rate, is_training) {\n  x_in = x\n\n  # Pre-LN:\n  ln_out_x = layer_norm_forward(x, ln_gamma, ln_beta)\n  x = ln_out_x$out\n\n  # First linear layer + ReLU\n  hidden = x %*% W_ff1 + matrix(b_ff1, nrow = nrow(x), ncol = ncol(W_ff1), byrow = TRUE)\n  hidden_activated = relu(hidden)\n\n  # Dropout after hidden activation\n  hidden_dropout = dropout_forward(hidden_activated, dropout_rate, is_training)\n  hidden_dropped = hidden_dropout$out\n\n  # Second linear layer\n  ff_out = hidden_dropped %*% W_ff2 + matrix(b_ff2, nrow = nrow(hidden_dropped), ncol = ncol(W_ff2), byrow = TRUE)\n\n  # Dropout after FF block output (before residual)\n  ff_out_dropout = dropout_forward(ff_out, dropout_rate, is_training)\n  ff_out_dropped = ff_out_dropout$out\n\n  out = x_in + ff_out_dropped\n\n  return(list(\n    out = out,\n    ff_out_dropped = ff_out_dropped,\n    ff_out_dropout_mask = ff_out_dropout$mask,\n    hidden = hidden,\n    hidden_activated = hidden_activated,\n    hidden_dropout_mask = hidden_dropout$mask,\n    ln_cache_x = ln_out_x\n  ))\n}\n\nfeed_forward_backward = function(dout, x_in, ff_out_dropped, ff_out_dropout_mask, hidden, hidden_activated, hidden_dropout_mask,\n                                 ln_cache_x, W_ff1, b_ff1, W_ff2, b_ff2, ln_gamma, ln_beta, dropout_rate) {\n\n  # Backward through dropout on ff_out\n  d_ff_out = dropout_backward(dout, ff_out_dropout_mask, dropout_rate)\n\n  d_W_ff2 = t(hidden_activated) %*% d_ff_out # Use original hidden_activated here for grad of W_ff2\n  d_b_ff2 = colSums(d_ff_out)\n\n  d_hidden_dropped = d_ff_out %*% t(W_ff2)\n  # Backward through dropout on hidden\n  d_hidden_activated = dropout_backward(d_hidden_dropped, hidden_dropout_mask, dropout_rate)\n\n  d_hidden = d_hidden_activated * relu_grad(hidden)\n\n  d_W_ff1 = t(ln_cache_x$out) %*% d_hidden\n  d_b_ff1 = colSums(d_hidden)\n\n  d_x_from_ff = d_hidden %*% t(W_ff1)\n\n  ln_grads_x = layer_norm_backward(d_x_from_ff, ln_cache_x$X, ln_cache_x$X_norm,\n                                   ln_cache_x$mean_X, ln_cache_x$var_X, ln_gamma, epsilon = 1e-5)\n  d_ln2_gamma = ln_grads_x$dgamma\n  d_ln2_beta = ln_grads_x$dbeta\n  d_x = dout + ln_grads_x$d_X\n\n  return(list(\n    d_x = d_x, d_W_ff1 = d_W_ff1, d_b_ff1 = d_b_ff1,\n    d_W_ff2 = d_W_ff2, d_b_ff2 = d_b_ff2,\n    d_ln2_gamma = d_ln2_gamma, d_ln2_beta = d_ln2_beta\n  ))\n}\n\n\n\nTransformer Model Forward Pass and Loss\nThis function puts together all the pieces we’ve built previously to get a single forward pass for the full model. It starts by getting the token embeddings for the input data (from the C matrix parameter) and adding the position embedding parameter data. This feeds into the single transformer block for this model (one layer of multi-headed self-attention, followed by a feed forward neural network with a single hidden layer). Then finally one last linear layer which converts the inputs into probabilities over the vocabulary space.\n\ntransformer_forward = function(X_batch, y_batch, params, dropout_rate, is_training) {\n  # 1. Input and Positional Embeddings\n  x_emb = t(sapply(1:block_size, function(j) params$C[X_batch[1, j], ]))\n  x = x_emb + params$P_emb\n\n  # Dropout after combined embeddings\n  embed_dropout_out = dropout_forward(x, dropout_rate, is_training)\n  x = embed_dropout_out$out\n\n  # Transformer Block\n  attn_out_cache = multi_head_attention_forward(x, params$Wq, params$Wk, params$Wv, params$Wo,\n                                                n_head, head_size, params$ln1_gamma, params$ln1_beta,\n                                                dropout_rate, is_training)\n  x = attn_out_cache$out\n\n  ff_out_cache = feed_forward_forward(x, params$W_ff1, params$b_ff1, params$W_ff2, params$b_ff2,\n                                     params$ln2_gamma, params$ln2_beta, dropout_rate, is_training)\n  x = ff_out_cache$out\n\n  # Final linear layer to get logits for all tokens in the sequence\n  logits = x %*% params$W_final + matrix(params$b_final, nrow = block_size, ncol = vocab_size, byrow = TRUE)\n\n  # Softmax to get probabilities\n  probs = softmax_matrix_rows(logits)\n\n  # Select the probability of the true token for each entry in the flattened batch\n  y_batch_flat = as.vector(t(y_batch))\n  indices = cbind(1:length(y_batch_flat), y_batch_flat)\n  correct_probs = probs[indices]\n  \n  # Compute Negative Log Likelihood / Cross-Entropy\n  token_losses = -log(correct_probs)\n  loss = mean(token_losses)\n\n  return(list(\n    logits = logits, probs = probs, loss = loss,\n    x_emb = x_emb,\n    embed_dropout_mask = embed_dropout_out$mask,\n    attn_cache = attn_out_cache,\n    ff_cache = ff_out_cache,\n    last_x_output = x\n  ))\n}\n\n\n\nTransformer Model Backward Pass\nThis function puts together all the pieces we’ve built previously to get a single backward pass for the full model, producing gradients for all the parameters. It works backwards from the final output of probabilities, computing gradients for each of the model parameters and the associated data, and passing the gradient for the input data back to each prior step along the series of model transformations.\n\ntransformer_backward = function(cache, X_batch, y_batch, params, dropout_rate) {\n  grads = list(\n    d_C = matrix(0, nrow = nrow(params$C), ncol = ncol(params$C)),\n    d_P_emb = matrix(0, nrow = nrow(params$P_emb), ncol = ncol(params$P_emb)),\n    d_Wq = matrix(0, nrow = nrow(params$Wq), ncol = ncol(params$Wq)),\n    d_Wk = matrix(0, nrow = nrow(params$Wk), ncol = ncol(params$Wk)),\n    d_Wv = matrix(0, nrow = nrow(params$Wv), ncol = ncol(params$Wv)),\n    d_Wo = matrix(0, nrow = nrow(params$Wo), ncol = ncol(params$Wo)),\n    d_ln1_gamma = rep(0, length(params$ln1_gamma)),\n    d_ln1_beta = rep(0, length(params$ln1_beta)),\n    d_W_ff1 = matrix(0, nrow = nrow(params$W_ff1), ncol = ncol(params$W_ff1)),\n    d_b_ff1 = rep(0, length(params$b_ff1)),\n    d_W_ff2 = matrix(0, nrow = nrow(params$W_ff2), ncol = ncol(params$W_ff2)),\n    d_b_ff2 = rep(0, length(params$b_ff2)),\n    d_ln2_gamma = rep(0, length(params$ln2_gamma)),\n    d_ln2_beta = rep(0, length(params$ln2_beta)),\n    d_W_final = matrix(0, nrow = nrow(params$W_final), ncol = ncol(params$W_final)),\n    d_b_final = rep(0, length(params$b_final))\n  )\n\n  # Transpose y_batch to get elements row by row, then flatten\n  y_batch_flat = as.vector(t(y_batch))\n  \n  # Create one-hot targets for the flattened y_batch\n  one_hot_targets_flat = matrix(0, nrow = length(y_batch_flat), ncol = vocab_size)\n  one_hot_targets_flat[cbind(1:length(y_batch_flat), y_batch_flat)] = 1\n  \n  d_logits = cache$probs - one_hot_targets_flat\n\n  d_W_final = t(cache$last_x_output) %*% d_logits\n  d_b_final = colSums(d_logits)\n  \n  grads$d_W_final = grads$d_W_final + d_W_final\n  grads$d_b_final = grads$d_b_final + d_b_final\n\n  # d_last_token_output = d_logits %*% t(params$W_final)\n  # \n  # d_x_from_logits = matrix(0, nrow = block_size, ncol = n_embd)\n  # d_x_from_logits[block_size, ] = d_last_token_output[1, ]\n  \n  d_x_from_logits = d_logits %*% t(params$W_final)\n\n  ff_grads = feed_forward_backward(d_x_from_logits, cache$attn_cache$out, cache$ff_cache$ff_out_dropped,\n                                   cache$ff_cache$ff_out_dropout_mask, cache$ff_cache$hidden,\n                                   cache$ff_cache$hidden_activated, cache$ff_cache$hidden_dropout_mask,\n                                   cache$ff_cache$ln_cache_x, params$W_ff1, params$b_ff1,\n                                   params$W_ff2, params$b_ff2, params$ln2_gamma, params$ln2_beta, dropout_rate)\n  d_x_from_ff = ff_grads$d_x\n  grads$d_W_ff1 = grads$d_W_ff1 + ff_grads$d_W_ff1\n  grads$d_b_ff1 = grads$d_b_ff1 + ff_grads$d_b_ff1\n  grads$d_W_ff2 = grads$d_W_ff2 + ff_grads$d_W_ff2\n  grads$d_b_ff2 = grads$d_b_ff2 + ff_grads$d_b_ff2\n  grads$d_ln2_gamma = grads$d_ln2_gamma + ff_grads$d_ln2_gamma\n  grads$d_ln2_beta = grads$d_ln2_beta + ff_grads$d_ln2_beta\n\n  attn_grads = multi_head_attention_backward(d_x_from_ff, cache$x_emb + params$P_emb,\n                                             cache$attn_cache$attn_out_dropped, cache$attn_cache$attn_out_dropout_mask,\n                                             cache$attn_cache$concat_heads, cache$attn_cache$outputs_per_head,\n                                             cache$attn_cache$x_for_qkvo, cache$attn_cache$ln_cache_x,\n                                             params$Wq, params$Wk, params$Wv, params$Wo, n_head, head_size,\n                                             params$ln1_gamma, params$ln1_beta, dropout_rate)\n\n  d_x_from_attn = attn_grads$d_x\n  grads$d_Wq = grads$d_Wq + attn_grads$d_Wq\n  grads$d_Wk = grads$d_Wk + attn_grads$d_Wk\n  grads$d_Wv = grads$d_Wv + attn_grads$d_Wv\n  grads$d_Wo = grads$d_Wo + attn_grads$d_Wo\n  grads$d_ln1_gamma = grads$d_ln1_gamma + attn_grads$d_ln1_gamma\n  grads$d_ln1_beta = grads$d_ln1_beta + attn_grads$d_ln1_beta\n\n  # Backward through embedding dropout\n  d_x_from_attn_and_pos = dropout_backward(d_x_from_attn, cache$embed_dropout_mask, dropout_rate)\n\n  # Gradient for Positional Embeddings\n  grads$d_P_emb = grads$d_P_emb + d_x_from_attn_and_pos\n\n  # Gradient for Character Embeddings (C)\n  for (i in 1:block_size) {\n    char_idx = X_batch[1, i]\n    grads$d_C[char_idx, ] = grads$d_C[char_idx, ] + d_x_from_attn_and_pos[i, ]\n  }\n\n  return(grads)\n}"
  },
  {
    "objectID": "projects/llm_transformer_from_scratch/llm_transformer_from_scratch.html#training-loop",
    "href": "projects/llm_transformer_from_scratch/llm_transformer_from_scratch.html#training-loop",
    "title": "LLM Transformer Implemented in Base R",
    "section": "Training Loop",
    "text": "Training Loop\nThis is where we train the model by picking one example sequence of tokens at a time (batch size is 1 in this implementation), calculating the loss, and updating the model parameters with their calculated gradients. Gradient clipping is applied here as a preventative measure to address the potential for the exploding gradient problem. Training and validation loss are printed out after every 10000 training examples. Training loss is very noisy since it is calculated on only one training example at a time, as compared to validation loss which is calculated over the full validation dataset.\n\n# Training loop\nfor (iter in 1:n_iter) {\n  # --- Training Step ---\n  # Sample a random training example\n  idx_train = sample(1:nrow(X_train), 1)\n  X_batch_train = matrix(X_train[idx_train, ], nrow = 1)\n  y_batch_train = matrix(y_train[idx_train, ], nrow = 1)\n\n  # Forward pass (training mode)\n  cache_train = transformer_forward(X_batch_train, y_batch_train, params, dropout_rate, is_training = TRUE)\n  loss_train = cache_train$loss\n\n  # Backward pass\n  grads = transformer_backward(cache_train, X_batch_train, y_batch_train, params, dropout_rate)\n\n  # Apply Gradient Clipping\n  clipped_grads = clip_gradients_by_norm(grads, gradient_clip_threshold)\n  # clipped_grads = grads # or not\n\n  # Update parameters using clipped gradients\n  for (p_name in names(params)) {\n    # Check if a gradient for this parameter exists in clipped_grads\n    grad_name = paste0(\"d_\", p_name)\n    if (!is.null(clipped_grads[[grad_name]])) {\n      params[[p_name]] = params[[p_name]] - learning_rate * clipped_grads[[grad_name]]\n    } else {\n        # This case should ideally not be hit if `grads` contains all corresponding `d_` parameters\n        # but included for robustness if `params` has entries not tracked by `grads`\n        cat(\"Warning: No gradient found for parameter:\", p_name, \"\\n\")\n    }\n  }\n\n  # --- Validation Step ---\n  if (iter %% 10000 == 0) {\n    # Calculate validation loss (inference mode, no dropout)\n    val_losses = c()\n    if (nrow(X_val) &gt; 0) { # Check if validation set is not empty\n      for (val_idx in 1:nrow(X_val)) {\n        X_batch_val = matrix(X_val[val_idx, ], nrow = 1)\n        y_batch_val = matrix(y_val[val_idx, ], nrow = 1)\n        cache_val = transformer_forward(X_batch_val, y_batch_val, params, dropout_rate, is_training = FALSE)\n        val_losses = c(val_losses, cache_val$loss)\n      }\n      avg_val_loss = mean(val_losses)\n      cat(\"Iteration:\", iter, \" Training Loss:\", round(loss_train, 4), \" Validation Loss:\", round(avg_val_loss, 4), \"\\n\")\n    } else {\n      cat(\"Iteration:\", iter, \" Training Loss:\", round(loss_train, 4), \" (No validation data)\\n\")\n    }\n  }\n}\n\nIteration: 10000  Training Loss: 3.6363  Validation Loss: 3.1276 \nIteration: 20000  Training Loss: 3.2004  Validation Loss: 3.0208 \nIteration: 30000  Training Loss: 2.7591  Validation Loss: 2.9624 \nIteration: 40000  Training Loss: 2.0192  Validation Loss: 2.906 \nIteration: 50000  Training Loss: 3.4338  Validation Loss: 2.8703 \nIteration: 60000  Training Loss: 3.1453  Validation Loss: 2.8311 \nIteration: 70000  Training Loss: 3.7388  Validation Loss: 2.8091 \nIteration: 80000  Training Loss: 2.8175  Validation Loss: 2.7793 \nIteration: 90000  Training Loss: 3.1989  Validation Loss: 2.7818 \nIteration: 100000  Training Loss: 2.6022  Validation Loss: 2.7582 \n\ncat(\"\\nTraining complete. Final Training Loss:\", round(loss_train, 4), \"\\n\")\n\n\nTraining complete. Final Training Loss: 2.6022 \n\nif (nrow(X_val) &gt; 0) {\n  cat(\"Final Average Validation Loss:\", round(avg_val_loss, 4), \"\\n\")\n}\n\nFinal Average Validation Loss: 2.7582"
  },
  {
    "objectID": "projects/llm_transformer_from_scratch/llm_transformer_from_scratch.html#generating-new-text",
    "href": "projects/llm_transformer_from_scratch/llm_transformer_from_scratch.html#generating-new-text",
    "title": "LLM Transformer Implemented in Base R",
    "section": "Generating New Text",
    "text": "Generating New Text\nThe text generation function will run in inference mode (is_training = FALSE) to ensure no dropout is applied. It can be fed a starting sequence, or will pad with blank spaces if the sequence is shorter than expected or empty. It generates output similar to what it is trained on, so even if the starting sequence is very different from the training content, the output will still look like the training content. In this case, the model was trained on Shakespeare, so the output will look like Shakespeare even if the generation is seeded with text that looks very different from Shakespeare.\n\ngenerate_text = function(current_params, start_string, num_characters_to_generate) {\n  generated_sequence_indices = encode(start_string)\n\n  if (length(generated_sequence_indices) &lt; block_size) {\n    padded_start = c(rep(stoi[[\" \"]], block_size - length(generated_sequence_indices)), generated_sequence_indices)\n    generated_sequence_indices = padded_start\n    cat(\"Padded start string to block_size with spaces: '\", decode(padded_start), \"'\\n\", sep = \"\")\n  }\n\n  for (i in 1:num_characters_to_generate) {\n    context_indices = tail(generated_sequence_indices, block_size)\n    X_predict = matrix(context_indices, nrow = 1)\n\n    # Simplified forward pass for inference (is_training = FALSE)\n    x_emb_infer = t(sapply(1:block_size, function(j) current_params$C[X_predict[1, j], ]))\n    x_infer = x_emb_infer + current_params$P_emb\n\n    # No dropout applied during inference (dropout_forward handles this)\n    x_infer = dropout_forward(x_infer, dropout_rate, is_training = FALSE)$out\n\n    attn_out_infer = multi_head_attention_forward(x_infer, current_params$Wq, current_params$Wk, current_params$Wv, current_params$Wo,\n                                                 n_head, head_size, current_params$ln1_gamma, current_params$ln1_beta,\n                                                 dropout_rate, is_training = FALSE)$out\n    x_infer = attn_out_infer\n\n    ff_out_infer = feed_forward_forward(x_infer, current_params$W_ff1, current_params$b_ff1, current_params$W_ff2, current_params$b_ff2,\n                                       current_params$ln2_gamma, current_params$ln2_beta,\n                                       dropout_rate, is_training = FALSE)$out\n    x_infer = ff_out_infer\n\n    # Final linear layer to get logits for the *last* token in the sequence\n    last_token_output_infer = matrix(x_infer[block_size, ], nrow = 1)\n    logits_infer = last_token_output_infer %*% current_params$W_final + matrix(current_params$b_final, nrow = 1, ncol = vocab_size)\n\n    probs_infer = softmax_matrix_rows(logits_infer)\n\n    next_char_idx = sample.int(vocab_size, 1, prob = probs_infer)\n\n    generated_sequence_indices = c(generated_sequence_indices, next_char_idx)\n  }\n  return(decode(generated_sequence_indices))\n}\n\n# Example usage: Generate new characters starting with a given string\nstart_seq = \"Charizard is my favorite Pokemon.\"\nnum_to_generate = 1000\n\ngenerated_text = generate_text(params, start_seq, num_to_generate)\ncat(\"\\nGenerated text starting with '\", start_seq, \"':\\n\", generated_text, \"\\n\", sep = \"\")\n\n\nGenerated text starting with 'Charizard is my favorite Pokemon.':\nCharizard is my favorite Pokemon.B\nS\nAhed,\nOshe un  bekoy tirde n min, w m s ta ktwicorrfandon, tal uorlee\nN\n\nT\nWls tiitind vsiw R\nBe  f girun y ifrovh woln\nLe w\nF tiuns n cthen ins, geou tonrda nd congr hen mpe-rle aromisono, telsd\nAs adinen sseq wGicwp AhRO\nRAe'.\nEite,\nThaloncis i'f- tind rgre tht theedis vrrtas, g ld ether,\nFinl'd\nUAh:Sli?AhnE\nSArah\nHHck T a,,\nWirw ale.od alr id r wRI-eatien'tathethyinrir,anvuthy,\n\nWil bdo an fe m.\n\nAig,\nN:\nAa!Nloue bu ounacsin frlaag.dg, apeitconeoomeyertaond,\nHeobellup sow? thasivr memongoosy kld onDimcy wur tthot n ead lvecon d-,\nYUd\nWOJn dgouin moe.UdAhoITiit ulvin wesir cper nod.\nSGiv\nTy s y I eEQI geth kranye irorldetVOoathye'ot th ns aonofras tes cathpen,:\nAif.\nCBs iulokega tou vShoylryheyerrertino, che capyy t. douus,\n\nYss ,\nG p nndendev\nQouerego kirourehet fy fe do srhe ri sas, wnduldeaali y bloiy g souronths, le.\nGos IAuthe, g net:\n\nIUSG!asot'ril bl,, IB wiworaf ple pesove ewe uran intes winof; b'roningl kesrk,.\n\n\nOingerodom? bisdthilan t:unl vAee\n\nFe kd:\nIAare IOeae whou\n\n\nWe can see that while the language is not correct, the model is producing text structured very similarly to the input Shakespeare text. The validation loss achieved here is worse than what Karpathy achieves in his implementation, but this is due to the choice to simplify several aspects of the model (batch size of 1, only a single transformer block instead of 3 or 4 in sequence) to make it easier to implement everything from scratch and keep the matrices 2-dimensional for easier manual inspection in RStudio’s global environment.\nThere are a number of improvements than could be made to this implementation. The most obvious ones are having a batch size greater than 1, and including multiple transformer blocks in sequence instead of just 1. Optimizing the weighting scheme in the gradient update to decay the learning rate or use some other algorithm like the Adam optimizer would also be a good choice. The reason this output doesn’t look as impressive as ChatGPT comes down to the complexity of the model. In this implementation, the count of final model parameters is 5489, which is very small in comparison to the count of 1.5 billion parameters in GPT-2 or 175 billion parameters in GPT-3."
  },
  {
    "objectID": "projects/llm_transformer_from_scratch/llm_transformer_from_scratch.html#attention-plot",
    "href": "projects/llm_transformer_from_scratch/llm_transformer_from_scratch.html#attention-plot",
    "title": "LLM Transformer Implemented in Base R",
    "section": "Attention Plot",
    "text": "Attention Plot\nAfter training the model, there are a number of interesting visualizations we can create. One is an attention plot, where we pick an input sequence and show how the attention layer creates weights to pay attention to prior tokens over that sequence. We can examine the input sequence “methinks” which is a word that exists commonly in the input Shakespeare text. At this point, we will call a couple of libraries just for the plotting functionality.\n\nlibrary(tidyr)\nlibrary(ggplot2)\nlibrary(gridExtra)\n\nattn_sequence = encode(\"methinks\")\nX_predict = matrix(attn_sequence, nrow = 1)\n\n# Token and positional embedding\nx_emb_infer = t(sapply(1:block_size, function(j) params$C[X_predict[1, j], ]))\nx_infer = x_emb_infer + params$P_emb\n\n# Layer normalization\nln_out_x = layer_norm_forward(x_infer, params$ln1_gamma, params$ln1_beta)\nx = ln_out_x$out\n\n# Attention block\nQ_all = x %*% Wq\nK_all = x %*% Wk\n\nplots_per_head = lapply(1:n_head, function(h) {\n  start_col = (h - 1) * head_size + 1\n  end_col = h * head_size\n  Q_h = Q_all[, start_col:end_col]\n  K_h = K_all[, start_col:end_col]\n\n  scores = (Q_h %*% t(K_h)) / sqrt(head_size)\n\n  mask_causal = upper.tri(matrix(0, nrow(scores), ncol(scores)), diag = FALSE)\n  scores[mask_causal] = -Inf\n\n  attention_weights = softmax_matrix_rows(scores)\n  \n  # make plot\n  df = as.data.frame(attention_weights)\n  colnames(df) = c(\"m\",\"e\",\"t\",\"h\",\"i\",\"n\",\"k\",\"s\")\n  df$row = c(\"m\",\"e\",\"t\",\"h\",\"i\",\"n\",\"k\",\"s\")\n  df = tidyr::pivot_longer(df, -row, names_to = \"col\", values_to = \"val\")\n  df$row = factor(df$row, levels = rev(c(\"m\",\"e\",\"t\",\"h\",\"i\",\"n\",\"k\",\"s\")))\n  df$col = factor(df$col, levels = c(\"m\",\"e\",\"t\",\"h\",\"i\",\"n\",\"k\",\"s\"))\n  p = ggplot(df, aes(x = col, y = row, fill = val)) +\n      geom_tile() +\n      scale_fill_viridis_c() +\n      labs(x = \"prior tokens in sequence\", y = \"input token\",\n           fill = \"weights\", title = paste0(\"Attention Head #\", h)) +\n      theme(plot.title = element_text(size = 11))\n  \n  # Return plot object to list output\n  return(p)\n  \n})\n\n# Display plots in 2x2 grid\ngrid.arrange(grobs = plots_per_head, ncol = 2)\n\n\n\n# Save plot for thumbnail\ng = arrangeGrob(grobs = plots_per_head, ncol = 2)\nggsave(file = \"thumbnail.jpg\", g, width = 6, height = 6/1.618)\n\nEach of the four attention heads generates different weights during the self-attention process. The first token always gets a 100% weight since there are no other tokens at that point, but for later tokens we can see that the model has learned to pay attention (give higher weights to) tokens that are further back in the sequence, and which prior tokens are highly weighted differs from one attention head to the next, so they are providing meaningfully distinct information."
  },
  {
    "objectID": "projects/llm_transformer_from_scratch/llm_transformer_from_scratch.html#conclusion",
    "href": "projects/llm_transformer_from_scratch/llm_transformer_from_scratch.html#conclusion",
    "title": "LLM Transformer Implemented in Base R",
    "section": "Conclusion",
    "text": "Conclusion\nWhile implementing everything from scratch without relying on any packages adds significant complexity to this model, it offers a detailed understanding of the core transformer architecture. The process of implementing some of this core functionality led to a deeper understanding of how and why certain calculations are what they are (See derivations in earlier sections regarding He initialization and Layer Normalization). This example serves as an educational tool for delving into the fundamental mechanisms of attention, normalization, and regularization within deep learning models, all without external dependencies."
  },
  {
    "objectID": "projects/generative_art/generative_art.html",
    "href": "projects/generative_art/generative_art.html",
    "title": "Generative Art",
    "section": "",
    "text": "I came across a few blog posts about using R to generate art which piqued my interest. For readers unfamiliar with the idea, the aRtsy package is a great reference. It implements a large suite of different algorithms which are used to draw interesting patterns. I was inspired by the idea of using R for art instead of analysis and wanted to try it out myself.\nWhile I love the content in the aRtsy package, one limitation (at least at the time I’m writing this) is that the package only produces static images. My idea for a new contribution in this space would be to make a generative animation."
  },
  {
    "objectID": "projects/generative_art/generative_art.html#introoverview",
    "href": "projects/generative_art/generative_art.html#introoverview",
    "title": "Generative Art",
    "section": "",
    "text": "I came across a few blog posts about using R to generate art which piqued my interest. For readers unfamiliar with the idea, the aRtsy package is a great reference. It implements a large suite of different algorithms which are used to draw interesting patterns. I was inspired by the idea of using R for art instead of analysis and wanted to try it out myself.\nWhile I love the content in the aRtsy package, one limitation (at least at the time I’m writing this) is that the package only produces static images. My idea for a new contribution in this space would be to make a generative animation."
  },
  {
    "objectID": "projects/generative_art/generative_art.html#setting-up",
    "href": "projects/generative_art/generative_art.html#setting-up",
    "title": "Generative Art",
    "section": "Setting Up",
    "text": "Setting Up\n\nLoading Libraries\n\nlibrary(aRtsy) # to print out a few examples I like\nlibrary(gridExtra) # to arrange multiple images in one plot\nlibrary(ggplot2) # to draw my own pictures\nlibrary(gganimate) # to turn multiple static images into an animation"
  },
  {
    "objectID": "projects/generative_art/generative_art.html#inspiration-from-the-artsy-package",
    "href": "projects/generative_art/generative_art.html#inspiration-from-the-artsy-package",
    "title": "Generative Art",
    "section": "Inspiration from the aRtsy package",
    "text": "Inspiration from the aRtsy package\nI’ll start by printing out a few examples using the basic functions of the aRtsy package. This is partly to showcase how easy it is for a user to play with, and partly to have an excuse to add nice artwork to my site. While it is easy to colorPalette() function provided by the aRtsy package to pick colors at random, making your own color palette is just a matter of picking a few colors you like, so that’s what I’ll do here. I just googled “hex color picker” and found this site which helped me identify the hex values for a few colors I liked.\n\nmy_color_palette = c(\"#666699\", \"#a894d1\", \"#ab7ab8\", \"#406abf\", \"#1f42ad\", \"#3d0099\", \"#800040\")\n\nset.seed(5813)\np1 &lt;- canvas_collatz(colors = my_color_palette, background = \"#000000\")\nset.seed(1818)\np2 &lt;- canvas_ribbons(colors = my_color_palette, background = \"#000000\", triangle = FALSE)\nset.seed(4497)\np3 &lt;- canvas_flame(colors = my_color_palette, background = \"#000000\")\nset.seed(9819)\np4 &lt;- canvas_flow(colors = my_color_palette, background = \"#000000\")\n\ngrid.arrange(p1, p2, p3, p4, ncol = 2)\n\n\n\n\nOne of the most convenient aspects of the way these functions are designed is that they require very little input from the user to generate new random pictures. All it takes to generate a new image is to set a different seed. I will try to follow the same approach, designing a function to create a specific type of image and using a random seed to drive the contents generated for the images."
  },
  {
    "objectID": "projects/generative_art/generative_art.html#initial-concept",
    "href": "projects/generative_art/generative_art.html#initial-concept",
    "title": "Generative Art",
    "section": "Initial Concept",
    "text": "Initial Concept\nThe functions from the aRtsy package are each based on a specific algorithm, the output from which is transformed in such a way that it generates interesting images. I am not sure how to do that, so I will have to work backwards and start with an idea of what kind of images I want to generate, and then create an algorithm that accomplishes that task.\nSimilar to aRtsy, I will rely on the ggplot2 package to draw images. This means I can then use the gganimate package (which works well with ggplot2) to turn multiple images into an animation. The gganimate package creates transitions for the different shapes that ggplot2 can draw. That means if I can represent my desired animation as multiple frames, or sequential plots, then the gganimate package can blend those frames together into an animation.\n\ndf &lt;- data.frame(x = 1:9, y = 1:9, id = 1:9, t = 1:9)\nplot_list &lt;- lapply(1:9, function(x){\n    df_subset = df[df$t == x,]\n    p = ggplot(df_subset, aes(x = x, y = y)) +\n        geom_point(color = \"#406abf\") +\n        scale_x_continuous(limits = c(0, 10)) +\n        scale_y_continuous(limits = c(0, 10))\n    return(p)\n})\n\ndo.call(\"grid.arrange\", c(plot_list, ncol = 3))\n\n\n\n\n\np = ggplot(df, aes(x = x, y = y, group = id)) +\n    geom_point(color = \"#406abf\") +\n    scale_x_continuous(limits = c(0, 10)) +\n    scale_y_continuous(limits = c(0, 10)) +\n    transition_time(t) +\n    ease_aes(\"linear\")\n\nanimate(p, nframes = 90, fps = 30, width = 400, height = 400, units = \"px\")\n\n\n\n\nI will choose to use points because they are simple to work with and generating a lot of them at random might produce an interesting animation. In order to have something more interesting than just points moving around in the plot, I will introduce a polygon in the center. Randomness can enter this animation in a few ways, such as the number of sides this polygon has and its rotation, the positions of the points that will move through the plot, and the color scheme."
  },
  {
    "objectID": "projects/generative_art/generative_art.html#designing-the-algorithm",
    "href": "projects/generative_art/generative_art.html#designing-the-algorithm",
    "title": "Generative Art",
    "section": "Designing the Algorithm",
    "text": "Designing the Algorithm\nSince the intention is to incorporate randomness, I start by setting the seed so that the steps I am trying out can be reproduced for testing if necessary.\n\nset.seed(1)\n\nThere are a few pieces of the concept to design around which include the polygon, the points, and the transitions.\n\nThe Polygon\nAdding a polygon to the plot is probably most interesting if it takes up a lot of space and is at the center of the plot. If it is small and off to one side it won’t have much impact. My first idea is to start with the unit circle. Pick some number of points on the edge of the unit circle and then if you connect those points you should wind up with a shape inscribed in the unit circle. Randomness can be introduced by randomly picking how many sides this polygon should have. It should be an integer number which is at least 3 so that it has positive area, and no more than 10 because adding more and more sides will just make the polygons closer and closer to an actual circle which will mean less variation in the shapes which is less interesting.\nI start by picking the first point at random, which is just a random angle at which the point will exist on the unit circle. Then I randomly pick the number of sides the polygon will have. With those two pieces of information, the locations of the other points can be generated by taking equally spaced steps along the unit circle where the step size is based on the number of sides desired.\n\ntheta = 2*pi*runif(1)\nn_sides = sample(3:10, 1)\n\n# initialize all points as the same starting point\ndf_shape = data.frame(\n    x = rep(cos(theta), n_sides),\n    y = rep(sin(theta), n_sides)\n)\n\n# apply rotation for each point sequentially\nfor(i in 2:n_sides){\n    df_shape$x[i] = df_shape$x[i-1] * cos(2*pi/n_sides) - df_shape$y[i-1] * sin(2*pi/n_sides)\n    df_shape$y[i] = df_shape$x[i-1] * sin(2*pi/n_sides) + df_shape$y[i-1] * cos(2*pi/n_sides)\n}\n\nggplot() +\ngeom_polygon(aes(x = x, y = y), data = df_shape, fill = \"#406abf\")\n\n\n\n\n\n\nThe Points\nNext comes the points. The points will have their positions and velocities randomly generated. The time at which they appear in the plot will also be random. I will start with 1000 points and see how well that works, it can be be updated easily later if necessary. They will move from the top left of the plot towards the bottom right covering about 5% of the distance per frame, plus or minus some random normal noise.\n\nn_obs = 1000\ndf_pts = data.frame(\n    id = 1:n_obs,\n    t = sample.int(400, size = n_obs, replace = TRUE),\n    x = 0,\n    y = 2.8*runif(n_obs) - 1.4,\n    v_x =  0.1 + 0.02*rnorm(n_obs),\n    v_y = -0.1 + 0.02*rnorm(n_obs)\n)\n\n\n\nCalculating Intersections\nSince there are both points and a big polygon in the image, an obvious thing to do to make it more interesting is to change the behavior based on whether or not they overlap. This means figuring out when they are overlapping and when they are not.\nOne way to think about this is that each point has an x and y velocity, so it is traveling in a straight path, or a line. Similarly, the polygon in the center has a random number of sides between 3 and 10, and if you think of those sides as continuing beyond the edges of the polygon then they are just lines. So we can figure out where the point enters and exits the polygon by calculating where these lines intersect.\nIf we have two lines represented by an equation like \\(y = mx + b\\) then the intersection will be where the x values are equal and the y values are also equal. We set \\(y_1 = y_2\\) and \\(x_1 = x_2\\) and solve for \\(x\\) and \\(y\\).\n\\[\n\\begin{aligned}\n  y_1 - m_1x_1 - b_1 &= y_2 - m_2x_2 - b_2 \\\\\n  y - m_1x - b_1 &= y - m_2x - b_2 \\\\\n  (m_2 - m_1)x &= b_1 - b_2 \\\\\n  x &= (b_1 - b_2)/(m_2 - m_1)\n\\end{aligned}\n\\]\nThis gives the x-coordinate of the intersection point, which can be plugged back into the original equation for the line \\(y_1 = m_1x_1 + b_1\\) to get the y-coordinate.\nIf the polygon has 10 sides, there will now be 10 intersection points, one intersection of each point’s trajectory with each of the 10 lines. But the point should only pass through the polygon in 2 places. This is because the polygon is not actually 10 lines but 10 line segments, so many of the intersections that were just calculated lie outside the bounds of the polygon shape. These incorrect intersection points can be removed by adding a filter to condition on points that lie inside the unit circle.\n\\[\nx^2 + y^2 &lt;= r^2 \\hspace{35pt} (r = 1)\n\\]\nSince the polygon was originally designed to be inscribed within the unit circle, that constraint can be used here to validate which intersection points are the correct ones. Furthermore, I can identify which of the two valid intersection points is the entry vs exit of point’s path through the polygon based on which has the lower x-value, since the velocity in the x direction is positive.\n\n# add the first row of the polygon data frame to the end\n# to make it convenient to calculate parameters for polygon lines\ndf_shape_loop = df_shape\ndf_shape_loop = rbind(df_shape_loop, df_shape[1,])\n\n# initialize data frame to hold candidate intersection point\n# x-coordinates, will add a new column for each intersection\ndf_x_intersect = rep(NA, nrow(df_pts))\n\n# parameters for the lines from the points\n# can be calculated outside the loop\n# m1 and b1 are both vectors\nm1 = df_pts$v_y / df_pts$v_x\nb1 = df_pts$y - m1*df_pts$x\n\n# calculate one intersection for each line (segment) from the polygon\nfor(i in 2:nrow(df_shape_loop)){\n    # m2 and b2 are scalars here\n    m2 = (df_shape_loop$y[i] - df_shape_loop$y[i-1]) / (df_shape_loop$x[i] - df_shape_loop$x[i-1])\n    b2 = df_shape_loop$y[i] - m2*df_shape_loop$x[i]\n    x_intersect = (b1 - b2)/(m2 - m1)\n    df_x_intersect = cbind(df_x_intersect, x_intersect)\n}\n\n# calculate corresponding y-coordinate for each intersection\ndf_y_intersect = sweep(df_x_intersect, 1, m1, FUN=\"*\")\ndf_y_intersect = sweep(df_y_intersect, 1, b1, FUN=\"+\")\n\n# check which 2 out of the 3+ intersections are valid\n# all valid intersection must lie within the unit circle\ncheck_valid = (sqrt(df_x_intersect^2 + df_y_intersect^2) &lt;= 1)\ncheck_valid[check_valid == 0] = NA\n\n# replace invalid intersection points with NA\ndf_x_intersect = df_x_intersect*check_valid\n\n# calculate entry and exit points\nx_entry = apply(df_x_intersect, 1, min, na.rm = TRUE)\nx_entry = ifelse(is.finite(x_entry), x_entry, NA)\nx_exit  = apply(df_x_intersect, 1, max, na.rm = TRUE)\nx_exit  = ifelse(is.finite(x_exit), x_exit, NA)\ny_entry = m1*x_entry + b1\ny_exit  = m1*x_exit + b1\n\n# consolidate entry and exit points into a data frame\ndf_entry_exit = as.data.frame(cbind(x_entry, y_entry, x_exit, y_exit))\n# cut speed of point in half inside the polygon\ndf_entry_exit$n_frames = ceiling((df_entry_exit$x_exit - df_entry_exit$x_entry)/(df_pts$v_x/2))\ndf_entry_exit$v_x = (df_entry_exit$x_exit - df_entry_exit$x_entry)/df_entry_exit$n_frames\ndf_entry_exit$v_y = (df_entry_exit$y_exit - df_entry_exit$y_entry)/df_entry_exit$n_frames\n\nNot all of the randomly generated points will actually come into contact with the polygon. This will require handling separately. If a point will never come into contact with the polygon, it will stay unchanged and be flagged as not in contact. If a point does contact the polygon at some point, I will change it’s initial value to be the point where it first contacts the polygon and flag it as in contact.\n\ndf_pts$x = ifelse(is.na(df_entry_exit$x_entry), 0, df_entry_exit$x_entry)\ndf_pts$y = ifelse(is.na(df_entry_exit$y_entry), df_pts$y, df_entry_exit$y_entry)\ndf_pts$contact = ifelse(is.na(df_entry_exit$x_entry), 0, 1)\n\n\n\nCalculating Movement\nNow that all the points have been initialized I need to calculate how their positions will change over time. As the time step increases by one, the x and y positions should update based on the v_x and v_y velocity parameters. Since points will start either in the center of the plot or where they first make contact with the polygon, I will work out their motion backwards from their initialized location. I can then handle motion while in contact with the polygon as a separate case, and then motion forwards again after exiting the polygon.\nThe first thing to do is to save the initialized state in a new variable so it can be used again later. After that, the data frame that contains the points will be updated to add their positions moving backwards in time.\n\n# save initialized state\ndf_init = df_pts\n\n# work backwards 15 frames\n# to get position of points before contact\n# with polygon or center of canvas\ndf_backwards = df_init\ndf_backwards$contact = 0\n\nfor(i in 1:15){\n    df_backwards$t = df_backwards$t - 1\n    df_backwards$x = df_backwards$x - df_backwards$v_x\n    df_backwards$y = df_backwards$y - df_backwards$v_y\n    # append to points data\n    df_pts = rbind(df_pts, df_backwards)\n}\n\nNext comes movement within the polygon. Not all point come into contact with the polygon, so this should only be done for the points where contact occurs.\n\n# get the subset of initialized points which\n# make contact with the polygon\ndf_within = df_init\ndf_within$v_x = df_entry_exit$v_x\ndf_within$v_y = df_entry_exit$v_y\ndf_within$n_frames = df_entry_exit$n_frames\ndf_within = df_within[df_within$contact == 1,]\n\n# add one frame at contact position\n# basically, the point will appear to pause\n# when it hits the polygon\ndf_within$t = df_within$t + 1\ndf_pts = rbind(df_pts, df_within[,-which(colnames(df_within)==\"n_frames\")])\n\n# different points will spend different amounts of time\n# traversing the polygon, so only keep iterating\n# for the points that have not reached the other side yet\nfor(i in 1:max(df_within$n_frames)){\n    df_within = df_within[df_within$n_frames &gt;= i,]\n    df_within$t = df_within$t + 1\n    df_within$x = df_within$x + df_within$v_x\n    df_within$y = df_within$y + df_within$v_y\n    # append to points data\n    df_pts = rbind(df_pts, df_within[,-which(colnames(df_within)==\"n_frames\")])\n}\n\nFinally there is the forwards movement either from the center of the plot to the right or from exiting the polygon to the right.\n\n# reset locations for points that contact the polygon\n# to be at the location where they exit, and also add 2\n# to their time index to create a delay on exit\ndf_forwards = df_init\ndf_forwards$x = ifelse(is.na(df_entry_exit$x_exit), df_init$x, df_entry_exit$x_exit)\ndf_forwards$y = ifelse(is.na(df_entry_exit$y_exit), df_init$y, df_entry_exit$y_exit)\ndf_forwards$t = ifelse(is.na(df_entry_exit$n_frames), df_forwards$t, df_forwards$t + df_entry_exit$n_frames + 2)\ndf_forwards$contact = 0\n\n# work forwards 15 frames past exit from polygon\n# to get position of points as they exit the canvas\nfor(i in 1:15){\n    df_forwards$t = df_forwards$t + 1\n    df_forwards$x = df_forwards$x + df_forwards$v_x\n    df_forwards$y = df_forwards$y + df_forwards$v_y\n    # append to points data\n    df_pts = rbind(df_pts, df_forwards)\n}\n\n\n\nCleanup\nWhile calculating point positions, I picked a number of time steps to make sure that motion within the bounds of the plot is being captured. If the plot x-axis covers [-1.25, 1.25] and average velocity in the x direction is 0.1 then it should take around (1.2 + 1.2)/0.1 = 24 time steps to get from side of the plot to the other, or 12 steps backwards from the center of the plot, and 12 steps forwards. In previous sections I calculated 15 time steps in each direction to be conservative due to the randomness added to the point velocities. In addition to this, motion within the polygon is handled separately. There will also be cases where points only pass through small sections of the plot such as at the top right or bottom left. This means there are many point locations that have been calculated which are outside the bounds of the plot and should be thrown away.\n\ndf_pts = df_pts[(df_pts$x &gt;= -1.2) & (df_pts$x &lt;= 1.2),]\ndf_pts = df_pts[(df_pts$y &gt;= -1.2) & (df_pts$y &lt;= 1.2),]"
  },
  {
    "objectID": "projects/generative_art/generative_art.html#first-animation",
    "href": "projects/generative_art/generative_art.html#first-animation",
    "title": "Generative Art",
    "section": "First Animation",
    "text": "First Animation\nAll the pieces are in place. Now it’s time to combine all these time and position values into an animation. Here’s a first pass at fitting it all together.\n\nmargin = 0\np = ggplot() +\n    geom_polygon(aes(x = x, y = y), data = df_shape, fill = my_color_palette[4]) +\n    geom_point(aes(x = x, y = y, group = id, color = as.factor(contact)), data = df_pts) +\n    scale_color_manual(values = my_color_palette[1:2]) +\n    scale_x_continuous(limits = c(-1.2, 1.2)) +\n    scale_y_continuous(limits = c(-1.2, 1.2)) +\n    theme(\n        axis.title = ggplot2::element_blank(),\n        axis.text = ggplot2::element_blank(),\n        axis.ticks = ggplot2::element_blank(),\n        axis.line = ggplot2::element_blank(),\n        legend.position = \"none\",\n        plot.background = ggplot2::element_rect(fill = \"#000000\", colour = \"#000000\"),\n        panel.border = ggplot2::element_blank(),\n        panel.grid = ggplot2::element_blank(),\n        plot.margin = ggplot2::unit(rep(margin, 4), \"lines\"),\n        strip.background = ggplot2::element_blank(),\n        strip.text = ggplot2::element_blank(),\n        panel.background = ggplot2::element_blank()\n    ) +\n    transition_time(t) +\n    ease_aes(\"linear\")\n\nanimate(p, nframes = 1500, fps = 30, width = 400, height = 400, units = \"px\")\n\n\n\n\n\nA Smooth Loop\nThis looks pretty good, but there are clear points where the animation is beginning and ending. One more change could be made here to make the animation loop more smoothly or seamlessly.\n\n# take points from just after the start of the animation\n# and use them to replace points towards the end\npts_start  = unique(df_pts$id[df_pts$t == 51])\npts_remove = unique(df_pts$id[df_pts$t &gt;= 350])\ndf_loop    = df_pts[df_pts$id %in% pts_start,]\ndf_loop$t  = df_loop$t + 299\ndf_loop$id = df_loop$id + n_obs\ndf_pts     = df_pts[!(df_pts$id %in% pts_remove),]\ndf_pts     = rbind(df_pts, df_loop)\n\n# limit data shown to the window that starts and ends\n# with the copied points\ndf_plot = df_pts[(df_pts$t &gt;= 51) & (df_pts$t &lt;= 350),]\n\nNow the animation should look like a continuous loop with no obvious starting or stopping point.\n\nmargin = 0\np = ggplot() +\n    geom_polygon(aes(x = x, y = y), data = df_shape, fill = my_color_palette[4]) +\n    geom_point(aes(x = x, y = y, group = id, color = as.factor(contact)), data = df_plot) +\n    scale_color_manual(values = my_color_palette[1:2]) +\n    scale_x_continuous(limits = c(-1.2, 1.2)) +\n    scale_y_continuous(limits = c(-1.2, 1.2)) +\n    theme(\n        axis.title = ggplot2::element_blank(),\n        axis.text = ggplot2::element_blank(),\n        axis.ticks = ggplot2::element_blank(),\n        axis.line = ggplot2::element_blank(),\n        legend.position = \"none\",\n        plot.background = ggplot2::element_rect(fill = \"#000000\", colour = \"#000000\"),\n        panel.border = ggplot2::element_blank(),\n        panel.grid = ggplot2::element_blank(),\n        plot.margin = ggplot2::unit(rep(margin, 4), \"lines\"),\n        strip.background = ggplot2::element_blank(),\n        strip.text = ggplot2::element_blank(),\n        panel.background = ggplot2::element_blank()\n    ) +\n    transition_time(t) +\n    ease_aes(\"linear\")\n\nanimate(p, nframes = 1500, fps = 30, width = 400, height = 400, units = \"px\")"
  },
  {
    "objectID": "projects/generative_art/generative_art.html#final-function",
    "href": "projects/generative_art/generative_art.html#final-function",
    "title": "Generative Art",
    "section": "Final Function",
    "text": "Final Function\nThe last piece of the puzzle is to wrap all of this up into one function so that it can be used the same way as the functions in the aRtsy package. It will take in 3 optional inputs. One is the seed, which can be set outside the function like in aRtsy, or can be passed as an argument. Another is a color palette, if the user doesn’t provide one that is fine, it should just pick colors randomly. Last is a background color, with similar behavior where it is picked randomly if not specified.\n\ndraw_polygon_animation = function(seed = NULL, color_palette = NULL, background = NULL){\n    # set seed here if it was passed as an argument\n    if(!is.null(seed)){ set.seed(seed) }\n    \n    # if color palette is given use that\n    # otherwise pick randomly from default colors\n    color_vec = grDevices::colors()[grep('gr(a|e)y', grDevices::colors(), invert = T)]\n    if(!is.null(color_palette)){ color_vec = color_palette }\n    \n    # if background color is provided use it as the first color\n    # otherwise pick randomly\n    colors4 = sample(color_vec, 4)\n    if(!is.null(background)){ colors4[1] = background }\n    \n    # picking start rotation and sides for polygon randomly\n    theta = 2*pi*runif(1)\n    n_sides = sample(3:10, 1)\n\n    # initialize all polygon vertices as the same starting point\n    df_shape = data.frame(\n        x = rep(cos(theta), n_sides),\n        y = rep(sin(theta), n_sides)\n    )\n\n    # apply rotation for each point sequentially\n    for(i in 2:n_sides){\n        df_shape$x[i] = df_shape$x[i-1] * cos(2*pi/n_sides) - df_shape$y[i-1] * sin(2*pi/n_sides)\n        df_shape$y[i] = df_shape$x[i-1] * sin(2*pi/n_sides) + df_shape$y[i-1] * cos(2*pi/n_sides)\n    }\n    \n    # initialize points\n    n_obs = 1000\n    df_pts = data.frame(\n        id = 1:n_obs,\n        t = sample.int(400, size = n_obs, replace = TRUE),\n        x = 0,\n        y = 2.8*runif(n_obs) - 1.4,\n        v_x =  0.1 + 0.02*rnorm(n_obs),\n        v_y = -0.1 + 0.02*rnorm(n_obs)\n    )\n    \n    # add the first row of the polygon data frame to the end\n    # to make it convenient to calculate parameters for polygon lines\n    df_shape_loop = df_shape\n    df_shape_loop = rbind(df_shape_loop, df_shape[1,])\n    \n    # initialize data frame to hold candidate intersection point\n    # x-coordinates, will add a new column for each intersection\n    df_x_intersect = rep(NA, nrow(df_pts))\n    \n    # parameters for the lines from the points\n    # can be calculated outside the loop\n    # m1 and b1 are both vectors\n    m1 = df_pts$v_y / df_pts$v_x\n    b1 = df_pts$y - m1*df_pts$x\n    \n    # calculate one intersection for each line (segment) from the polygon\n    for(i in 2:nrow(df_shape_loop)){\n        # m2 and b2 are scalars here\n        m2 = (df_shape_loop$y[i] - df_shape_loop$y[i-1]) / (df_shape_loop$x[i] - df_shape_loop$x[i-1])\n        b2 = df_shape_loop$y[i] - m2*df_shape_loop$x[i]\n        x_intersect = (b1 - b2)/(m2 - m1)\n        df_x_intersect = cbind(df_x_intersect, x_intersect)\n    }\n    \n    # calculate corresponding y-coordinate for each intersection\n    df_y_intersect = sweep(df_x_intersect, 1, m1, FUN=\"*\")\n    df_y_intersect = sweep(df_y_intersect, 1, b1, FUN=\"+\")\n    \n    # check which 2 out of the 3+ intersections are valid\n    # all valid intersection must lie within the unit circle\n    check_valid = (sqrt(df_x_intersect^2 + df_y_intersect^2) &lt;= 1)\n    check_valid[check_valid == 0] = NA\n    \n    # replace invalid intersection points with NA\n    df_x_intersect = df_x_intersect*check_valid\n    \n    # calculate entry and exit points\n    x_entry = apply(df_x_intersect, 1, min, na.rm = TRUE)\n    x_entry = ifelse(is.finite(x_entry), x_entry, NA)\n    x_exit  = apply(df_x_intersect, 1, max, na.rm = TRUE)\n    x_exit  = ifelse(is.finite(x_exit), x_exit, NA)\n    y_entry = m1*x_entry + b1\n    y_exit  = m1*x_exit + b1\n    \n    # consolidate entry and exit points into a data frame\n    df_entry_exit = as.data.frame(cbind(x_entry, y_entry, x_exit, y_exit))\n    # cut speed of point in half inside the polygon\n    df_entry_exit$n_frames = ceiling((df_entry_exit$x_exit - df_entry_exit$x_entry)/(df_pts$v_x/2))\n    df_entry_exit$v_x = (df_entry_exit$x_exit - df_entry_exit$x_entry)/df_entry_exit$n_frames\n    df_entry_exit$v_y = (df_entry_exit$y_exit - df_entry_exit$y_entry)/df_entry_exit$n_frames\n    \n    # reset starting locations\n    df_pts$x = ifelse(is.na(df_entry_exit$x_entry), 0, df_entry_exit$x_entry)\n    df_pts$y = ifelse(is.na(df_entry_exit$y_entry), df_pts$y, df_entry_exit$y_entry)\n    df_pts$contact = ifelse(is.na(df_entry_exit$x_entry), 0, 1)\n    \n    # save initialized state\n    df_init = df_pts\n    \n    # work backwards 15 frames\n    # to get position of points before contact\n    # with polygon or center of canvas\n    df_backwards = df_init\n    df_backwards$contact = 0\n    \n    for(i in 1:15){\n        df_backwards$t = df_backwards$t - 1\n        df_backwards$x = df_backwards$x - df_backwards$v_x\n        df_backwards$y = df_backwards$y - df_backwards$v_y\n        # append to points data\n        df_pts = rbind(df_pts, df_backwards)\n    }\n    \n    # get the subset of initialized points which\n    # make contact with the polygon\n    df_within = df_init\n    df_within$v_x = df_entry_exit$v_x\n    df_within$v_y = df_entry_exit$v_y\n    df_within$n_frames = df_entry_exit$n_frames\n    df_within = df_within[df_within$contact == 1,]\n    \n    # add one frame at contact position\n    # basically, the point will appear to pause\n    # when it hits the polygon\n    df_within$t = df_within$t + 1\n    df_pts = rbind(df_pts, df_within[,-which(colnames(df_within)==\"n_frames\")])\n    \n    # different points will spend different amounts of time\n    # traversing the polygon, so only keep iterating\n    # for the points that have not reached the other side yet\n    for(i in 1:max(df_within$n_frames)){\n        df_within = df_within[df_within$n_frames &gt;= i,]\n        df_within$t = df_within$t + 1\n        df_within$x = df_within$x + df_within$v_x\n        df_within$y = df_within$y + df_within$v_y\n        # append to points data\n        df_pts = rbind(df_pts, df_within[,-which(colnames(df_within)==\"n_frames\")])\n    }\n    \n    # reset locations for points that contact the polygon\n    # to be at the location where they exit, and also add 2\n    # to their time index to create a delay on exit\n    df_forwards = df_init\n    df_forwards$x = ifelse(is.na(df_entry_exit$x_exit), df_init$x, df_entry_exit$x_exit)\n    df_forwards$y = ifelse(is.na(df_entry_exit$y_exit), df_init$y, df_entry_exit$y_exit)\n    df_forwards$t = ifelse(is.na(df_entry_exit$n_frames), df_forwards$t, df_forwards$t + df_entry_exit$n_frames + 2)\n    df_forwards$contact = 0\n    \n    # work forwards 15 frames past exit from polygon\n    # to get position of points as they exit the canvas\n    for(i in 1:15){\n        df_forwards$t = df_forwards$t + 1\n        df_forwards$x = df_forwards$x + df_forwards$v_x\n        df_forwards$y = df_forwards$y + df_forwards$v_y\n        # append to points data\n        df_pts = rbind(df_pts, df_forwards)\n    }\n    \n    # remove points located outside of plot window\n    df_pts = df_pts[(df_pts$x &gt;= -1.2) & (df_pts$x &lt;= 1.2),]\n    df_pts = df_pts[(df_pts$y &gt;= -1.2) & (df_pts$y &lt;= 1.2),]\n    \n    # make the loop continuous\n    # take points from just after the start of the animation\n    # and use them to replace points towards the end\n    pts_start  = unique(df_pts$id[df_pts$t == 51])\n    pts_remove = unique(df_pts$id[df_pts$t &gt;= 350])\n    df_loop    = df_pts[df_pts$id %in% pts_start,]\n    df_loop$t  = df_loop$t + 299\n    df_loop$id = df_loop$id + n_obs\n    df_pts     = df_pts[!(df_pts$id %in% pts_remove),]\n    df_pts     = rbind(df_pts, df_loop)\n    \n    # limit data shown to the window that starts and ends\n    # with the copied points\n    df_plot = df_pts[(df_pts$t &gt;= 51) & (df_pts$t &lt;= 350),]\n    \n    # animation\n    margin = 0\n    p = ggplot() +\n        geom_polygon(aes(x = x, y = y), data = df_shape, fill = colors4[2]) +\n        geom_point(aes(x = x, y = y, group = id, color = as.factor(contact)), data = df_plot) +\n        scale_color_manual(values = colors4[3:4]) +\n        scale_x_continuous(limits = c(-1.2, 1.2)) +\n        scale_y_continuous(limits = c(-1.2, 1.2)) +\n        theme(\n            axis.title = ggplot2::element_blank(),\n            axis.text = ggplot2::element_blank(),\n            axis.ticks = ggplot2::element_blank(),\n            axis.line = ggplot2::element_blank(),\n            legend.position = \"none\",\n            plot.background = ggplot2::element_rect(fill = colors4[1], color = colors4[1]),\n            panel.border = ggplot2::element_blank(),\n            panel.grid = ggplot2::element_blank(),\n            plot.margin = ggplot2::unit(rep(margin, 4), \"lines\"),\n            strip.background = ggplot2::element_blank(),\n            strip.text = ggplot2::element_blank(),\n            panel.background = ggplot2::element_blank()\n        ) +\n        transition_time(t) +\n        ease_aes(\"linear\")\n    \n    animate(p, nframes = 1500, fps = 30, width = 400, height = 400, units = \"px\")\n}"
  },
  {
    "objectID": "projects/generative_art/generative_art.html#conclusion",
    "href": "projects/generative_art/generative_art.html#conclusion",
    "title": "Generative Art",
    "section": "Conclusion",
    "text": "Conclusion\nThis was a fun creative exercise exploring a topic very different from the data analysis I normally use R for. Despite being about making art, it was still a very technical exercise and the concept I came up with used a lot of basic algebra and geometry concepts. Hopefully aspects of this will prove useful in future endeavors.\n\ndraw_polygon_animation(seed = 2172, color_palette = my_color_palette, background = \"#000000\")"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Nicholas Burk",
    "section": "",
    "text": "n = 20 // number of layers \nm = 200 // number of samples per layer\nk = 10 // number of bumps per layer\n\nbumps = {\n  // Inspired by Lee Byron’s test data generator.\n  function bump(a, n) {\n    const x = 1 / (0.1 + Math.random());\n    const y = 2 * Math.random() - 0.5;\n    const z = 10 / (0.1 + Math.random());\n    for (let i = 0; i &lt; n; ++i) {\n      const w = (i / n - y) * z;\n      a[i] += x * Math.exp(-w * w);\n    }\n  }\n  return function bumps(n, m) {\n    const a = [];\n    for (let i = 0; i &lt; n; ++i) a[i] = 0;\n    for (let i = 0; i &lt; m; ++i) bump(a, n);\n    return a;\n  };\n}\n\nchart = {\n  //const width = 928;\n  const width = 1200;\n  const height = 500;\n\n  const x = d3.scaleLinear([0, m - 1], [0, width]);\n  const y = d3.scaleLinear([0, 1], [height, 0]);\n  const z = d3.interpolateCool;\n\n  const area = d3.area()\n    .x((d, i) =&gt; x(i))\n    .y0(d =&gt; y(d[0]))\n    .y1(d =&gt; y(d[1]));\n\n  const stack = d3.stack()\n    .keys(d3.range(n))\n    .offset(d3.stackOffsetWiggle)\n    .order(d3.stackOrderNone);\n\n  function randomize() {\n    const layers = stack(d3.transpose(Array.from({length: n}, () =&gt; bumps(m, k))));\n    y.domain([\n      d3.min(layers, l =&gt; d3.min(l, d =&gt; d[0])),\n      d3.max(layers, l =&gt; d3.max(l, d =&gt; d[1]))\n    ]);\n    return layers;\n  }\n  \n  const svg = d3.create(\"svg\")\n      .attr(\"viewBox\", [0, 0, width, height])\n      .attr(\"width\", width)\n      .attr(\"height\", height)\n      .attr(\"style\", \"max-width: 100%; height: auto;\");\n\n  const path = svg.selectAll(\"path\")\n    .data(randomize)\n    .join(\"path\")\n      .attr(\"d\", area)\n      .attr(\"fill\", () =&gt; z(Math.random()));\n\n  while (true) {\n    yield svg.node();\n\n    await path\n      .data(randomize)\n      .transition()\n        .delay(1000)\n        .duration(1500)\n        .attr(\"d\", area)\n      .end();\n  }\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n  \n  \nHi! I’m Nicholas Burk. My background is in math and economics, and I have spent most of my career working in the financial industry in various roles in the Federal Reserve System, Union Bank, and Fannie Mae. My work has ranged from exploratory data analysis to building and testing statistical models. I have also done some data science work for the Amercian Red Cross as a virtual volunteer.\nThe projects section contains posts about some side projects I have worked on unrelated to my job. These are usually short term passion projects related to a topic of interest or volunteer work."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "My background is in math and economics, and I have spent most of my career working in the financial industry doing analytical work ranging from exploratory data analysis to building and testing statistical models."
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About Me",
    "section": "Education",
    "text": "Education\nGeorgia Institute of Technology | M.S. in Analytics\nUniversity of California, San Diego | B.S. in Mathematics, B.S. in Economics"
  },
  {
    "objectID": "about.html#employment",
    "href": "about.html#employment",
    "title": "About Me",
    "section": "Employment",
    "text": "Employment\nFederal Reserve Bank of San Francisco\nReserves Data Specialist | 2019-2023\n\nAssisted in the implementation of monetary policy; operational areas include reserve requirements, interest payments, deficiency charges, TDF operations, etc. Required in-depth knowledge of the functionality of and data flows between a network of internal applications.\nResearched questions from stakeholders to help guide policy decisions. Examples include exploratory analysis of usage of FRB products which guides updates to Regulation D and service offerings, extract data from external sources such as Census and FEMA and combine with internal data for mapping and spatial modeling (tests for spatial dependence, KNN, spatial regression, etc.).\nProduct owner for Reserves business line applications; wrote requirements and user stories and worked with developers to build new functionality, proposed algorithm designs, tested that production implementation matches expectations.\nResearch and development applying statistical and machine learning techniques to ad-hoc problems. Examples include applying spectral analysis to identify cyclical patterns in account activity, LDA text analysis to determine how well business-proposed classifications align with data-driven classifications, simulation to bootstrap confidence intervals and prediction intervals in a variety of cases as well as simulating infrequent business problems to test how well detection algorithms perform, end-to-end model development (logistic regression with and without regularization penalties, random forest, etc.).\n\nMUFG Union Bank\nModel Risk Management Quantitative Analyst | 2017-2019\n\nAs the lead validator on my team, I managed one direct report, participated in the hiring, onboarding, and training for all new employees on the team, and served as project manager for validation projects contracted out to external vendors.\nTested new or updated models to ensure the development process follows appropriate methodology. All testing performed independently; model owner hands over development code and documentation and I use R, Python, to run tests of model assumptions, validate performance, build challenger models, deliver resulting observations.\nResponsible for a wide variety of credit risk models, including rating models, CCAR/DFAST stress testing models, CECL use cases, and more.\nChallenged current models by building alternative models to showcase different methodology and ensure state of the art techniques are considered. For example, building a random forest or neural network model to compare with a logistic regression.\nMonitored performance of existing models on an ongoing basis to identify and mitigate risks around deteriorating model performance. Involved enforcing existing model governance as well as changing governance plans to ensure they are appropriate for the model.\n\nFederal Reserve Bank of San Francisco\nFinancial Institutions Analyst | 2014-2017\n\nCollected and processed data from multiple sources at multiple frequencies, ensuring both timeliness and accuracy of the data. Built models for changepoint detection and structural breaks to make the quality assurance process more dynamic and data-driven.\nDeveloped tools using R, SQL, and MS Office to automate time-intensive processes and aid other analysts in their work. Worked with developers to implement my tools into production for the entire FRS.\nBuilt models for risk identification and exploratory analysis, leveraging external data sources as required to improve performance or yield interesting insights.\n\nBoard of Governors of the Federal Reserve System\nResearch Assistant | 2011-2014\n\nProduced memoranda and briefings to the Board, used in the implementation of monetary policy and in data books and testimony for Board members.\nMaintained and improved systems that collect real-time economic data.\nAssisted Board economists in a variety of projects, most notably the nowcasting project."
  },
  {
    "objectID": "about.html#volunteering",
    "href": "about.html#volunteering",
    "title": "About Me",
    "section": "Volunteering",
    "text": "Volunteering\nAmerican Red Cross\nVirtual Volunteer\n\nConsulted on causal analysis studies for the IFRC anticipatory action programs. Covered topics including study design, exploratory analysis, and communicating quantitative findings with non-technical stakeholders.\nDeveloped proof-of-concept for consistently estimating demand for shelter in relation to fires in California."
  },
  {
    "objectID": "projects/ca_fire_evac/ca_fire_evac.html",
    "href": "projects/ca_fire_evac/ca_fire_evac.html",
    "title": "California Fire Evacuations",
    "section": "",
    "text": "One of the volunteer projects I worked on for the American Red Cross (ARC) was for a chapter in northern California which wanted to improve predictions for shelter demand due to fire-related evacuations. When a wildfire happens, the local authorities may issue an evacuation order to tell people in the affected area to evacuate.\nARC helps by activating and staffing shelters near the affected area so that people who have to flee from the fire have a place to spend the night. These shelters are usually places like stadiums or schools with gymnasiums which have agreements with ARC to use their space as a temporary shelter in emergency situations. A volunteer from ARC will go to the shelter location to make sure it is open and ready to receive people in need.\nARC needs to decide which potential shelter locations to activate for any given evacuation order. There are a lot of factors that go into making this decision, but one of them is an estimate of demand for shelter. How many people will actually want to make use of the shelter that ARC will provide?"
  },
  {
    "objectID": "projects/ca_fire_evac/ca_fire_evac.html#introoverview",
    "href": "projects/ca_fire_evac/ca_fire_evac.html#introoverview",
    "title": "California Fire Evacuations",
    "section": "",
    "text": "One of the volunteer projects I worked on for the American Red Cross (ARC) was for a chapter in northern California which wanted to improve predictions for shelter demand due to fire-related evacuations. When a wildfire happens, the local authorities may issue an evacuation order to tell people in the affected area to evacuate.\nARC helps by activating and staffing shelters near the affected area so that people who have to flee from the fire have a place to spend the night. These shelters are usually places like stadiums or schools with gymnasiums which have agreements with ARC to use their space as a temporary shelter in emergency situations. A volunteer from ARC will go to the shelter location to make sure it is open and ready to receive people in need.\nARC needs to decide which potential shelter locations to activate for any given evacuation order. There are a lot of factors that go into making this decision, but one of them is an estimate of demand for shelter. How many people will actually want to make use of the shelter that ARC will provide?"
  },
  {
    "objectID": "projects/ca_fire_evac/ca_fire_evac.html#setting-up",
    "href": "projects/ca_fire_evac/ca_fire_evac.html#setting-up",
    "title": "California Fire Evacuations",
    "section": "Setting Up",
    "text": "Setting Up\n\nLoading Libraries\n\nlibrary(tigris) # to download CA shapefile from the Census\nlibrary(jsonlite) # to parse responses from the OpenFEMA API\nlibrary(sf) # for manipulating shapefile geometry information\nlibrary(ggplot2) # to make plots\nlibrary(tidycensus) # to get US Census data\nlibrary(dplyr) # for data manipulation\nlibrary(tidyr) # for data manipulation"
  },
  {
    "objectID": "projects/ca_fire_evac/ca_fire_evac.html#getting-the-data",
    "href": "projects/ca_fire_evac/ca_fire_evac.html#getting-the-data",
    "title": "California Fire Evacuations",
    "section": "Getting the Data",
    "text": "Getting the Data\nThe first thing I needed to do is find data related to the problem. For data about evacuation orders related to fires in CA, the OpenFEMA datasets looked like a good option. In particular, the Integrated Public Alert and Warning System (IPAWS) has an API which lets users query historical alert data. This is useful for this problem about CA fires because many counties in CA began adopting IPAWS to distribute their alerts starting around 2013, so it should contain data about the evacuation alerts of interest.\nAnother data source is the US Census. The evacuation order has information about what geographic region is affected, but no information about the people in that region or their demographics. That information will have to come from the census.\n\nGet Data from the OpenFEMA API\nThe IPAWS API lets users query based on certain available fields as listed in their documentation. However, it is a little bit tricky to get what I’m looking for because there is no “state” field to use for filtering so I cannot ask for results where “state=CA”. Instead the user can pass in a geometry in the shape of a polygon as a search area, so it is necessary for me to first represent CA as a polygon in Well Known Text (WKT) format to pass to the API as a filter.\n\n# get CA shapefile from the census\nca = tigris::states(cb = TRUE, resolution = \"20m\")\nca = ca[ca$NAME == \"California\",]\n\n# my approximate guess for longitude and latitude\n# for a polygon covering CA\nca_approx = data.frame(\n    lon = c(-124.2196,-119.9818,-119.9922,-113.9484,-114.3979,-117.2380,-120.1382,-124.5953),\n    lat = c(42.0019,42.0089,39.0026,34.6875,32.6730,32.4818,33.0436,40.3261)\n)\n\n# same as above, just transposed in format\n# to align with what the API expects\n# POLYGON((-124.2196 42.0019,-119.9818 42.0089,-119.9922 39.0026,-113.9498 34.6875,-114.3979 32.6730,-117.2380 32.4818,-120.1382 33.0436,-124.5953 40.3261))\n\n# verify coverage with plot\nggplot() +\n    geom_sf(data = ca) +\n    geom_polygon(aes(x = lon, y = lat), data = ca_approx,\n                 color = \"#F8766D\", fill = \"#F8766D\", alpha = 0.3) +\n    theme(axis.title = element_blank())\n\n\nMy simple polygon covers California as desired. It is okay that it covers some space beyond California because I am only using it to filter results from IPAWS. It may pick up a few cases outside of California, but these will be eliminated later when I merge the IPAWS data with the US Census data since they will not match up with any CA census geometries.\nNow I can start making requests from the IPAWS API. I will skip all the trial and error I had to go through to get it to work and just show the final code that worked for me. Comments below in the relevant sections discuss some of these decisions.\n\n# request to OpenFEMA API\n# give back status='Actual\" (as opposed to test alerts)\n# and state=CA (geo polygon information interects my approximated CA polygon)\napi_query_base = \"https://www.fema.gov/api/open/v1/IpawsArchivedAlerts?$filter=status eq 'Actual' and geo.intersects(searchGeometry, geography 'POLYGON((-124.2196 42.0019,-119.9818 42.0089,-119.9922 39.0026,-113.9498 34.6875,-114.3979 32.6730,-117.2380 32.4818,-120.1382 33.0436,-124.5953 40.3261))') and sent gt 'yyyy-01-01T00:00:01.000z' and sent lt 'yyyy-12-31T23:59:59.000z'&$orderby=sent&$inlinecount=allpages&$skip=\"\n\n# OpenFEMA API returns 1000 records max\n# so need to page through sets of 1000\n# https://www.fema.gov/about/openfema/api\nresults_df = data.frame()\nresults_poly = list()\n\n# IPAWS data is available starting in June 2012\n# I am querying one year of data at a time\n# because I keep running into errors where the API\n# doesn't want to return results when the page count\n# gets too high\nfor(year in 2012:2020){\n    \n    # set up counters to page through data\n    last_page = FALSE\n    skip_records = 0\n    \n    while(!last_page){\n        # query API for up to 1000 records\n        api_query = paste0(api_query_base, skip_records)\n        api_query = gsub(\"yyyy\", year, api_query)\n        json_result = readLines(api_query)\n        parsed_list = fromJSON(json_result)\n        n_records = nrow(parsed_list[[2]])\n        \n        # is this the last page of results?\n        # if so end loop\n        if(n_records &lt; 1000){ last_page = TRUE }\n        \n        # convert results to single data frame\n        # plus list of polygons\n        for(i in 1:n_records){\n            # get metadata for each alert\n            df = data.frame(\n                sent = parsed_list[[2]][i,]$sent,\n                status = parsed_list[[2]][i,]$status,\n                identifier = parsed_list[[2]][i,]$identifier,\n                category = paste0(parsed_list[[2]][i,]$info[[1]]$category[[1]], collapse = \", \"),\n                areaDesc = parsed_list[[2]][i,]$info[[1]]$area[[1]]$areaDesc,\n                responseType = paste0(parsed_list[[2]][i,]$info[[1]]$responseType[[1]], collapse = \", \"),\n                urgency = paste0(unique(parsed_list[[2]][i,]$info[[1]]$urgency), collapse = \", \"),\n                severity = paste0(unique(parsed_list[[2]][i,]$info[[1]]$severity), collapse = \", \"),\n                certainty = paste0(unique(parsed_list[[2]][i,]$info[[1]]$certainty), collapse = \", \"),\n                effective = ifelse(is.null(parsed_list[[2]][i,]$info[[1]]$effective), \"\", paste0(unique(parsed_list[[2]][i,]$info[[1]]$effective), collapse = \", \")),\n                onset = ifelse(is.null(parsed_list[[2]][i,]$info[[1]]$onset), \"\", paste0(unique(parsed_list[[2]][i,]$info[[1]]$onset), collapse = \", \")),\n                expires = ifelse(is.null(parsed_list[[2]][i,]$info[[1]]$expires), \"\", paste0(unique(parsed_list[[2]][i,]$info[[1]]$expires), collapse = \", \")),\n                description = ifelse(is.null(parsed_list[[2]][i,]$info[[1]]$description), \"\", paste0(unique(parsed_list[[2]][i,]$info[[1]]$description), collapse = \", \")),\n                instruction = ifelse(is.null(parsed_list[[2]][i,]$info[[1]]$instruction), \"\", paste0(unique(parsed_list[[2]][i,]$info[[1]]$instruction), collapse = \", \"))\n            )\n            \n            # sometimes evacuation orders for fires might not be coded as category='Fire'\n            # or as responseType='Evacuate' so filter more broadly\n            if((df$category != \"Met\") & (grepl(\"Evacuate\",df$responseType) | (grepl(\"fire\",df$description,ignore.case = TRUE) & grepl(\"evacuat\",df$description,ignore.case = TRUE)) | (grepl(\"fire\",df$instruction,ignore.case = TRUE) & grepl(\"evacuat\",df$instruction,ignore.case = TRUE)))){\n                results_df = rbind(results_df, df)\n                results_poly[[length(results_poly)+1]] = parsed_list[[2]][i,]$info[[1]]$area[[1]]$polygon$coordinates\n            }\n            \n        }\n        # print out progress\n        print(paste0(\"Year: \", year, \", Records Processed: \", skip_records + n_records))\n        # increment page skip count\n        skip_records = skip_records + 1000\n    }\n}\n\n# reformat polygon matrix results to WKT text fields\n# so they can be represented in a column vector\npoly_vec = do.call(\"c\", lapply(results_poly, function(x){\n    paste0(apply(x[[1]], 2, function(y){paste0(y,collapse = \" \")}), collapse = \",\")\n}))\nresults_df$areaPolygon = sub(\"^[^,]*,\", \"\", poly_vec)\nresults_df$areaPolygon = paste0(\"POLYGON((\", poly_vec, \"))\")\n\n\n\nGet Data from the US Census\nThere is a lot of demographic information available via the US Census API, so it helps to have some idea of what to get in advance. In my conversations with ARC, they said age and income are the ones they would consider most important. People who are very old are less likely to leave their home in the event of an evacuation order and consequently less likely to seek shelter from ARC. Similarly, people with higher income may have a second home they can travel to or choose to find paid accommodations with a higher level of comfort than the free but sparse shelter option that ARC is providing.\nI found this information in the American Community Survey (ACS) 5-year data. Here is a link to the census page for this data source. The 5-year data is great because it has information at the block-group level, which is a really low level geography. Smaller geographic units are better because they should allow for closer alignment with the arbitrary areas of the evacuation orders than larger units which may have large areas that are not in the region of interest.\nThe data dictionary is large and hard to wade through. I found what I was looking for under the variable B19037, which provides what is basically the joint distribution of age and income. However, it is not easy to separate age and income from the way the data is given, so I created the mapping manually since the number of cases is small enough and this is the only data I needed.\n\n# read in the mapping I created manually in a csv file\nB19037_mapping = read.csv(\"census_B19037_definitions.csv\")\n\n# display mapping table\nknitr::kable(head(B19037_mapping))\n\n\n\n\nvariable\nage_householder\nincome\n\n\n\n\nB19037_003\n0_24\n0_10K\n\n\nB19037_004\n0_24\n10K_15K\n\n\nB19037_005\n0_24\n15K_20K\n\n\nB19037_006\n0_24\n20K_25K\n\n\nB19037_007\n0_24\n25K_30K\n\n\nB19037_008\n0_24\n30K_35K\n\n\n\n\n\nUsing the census API requires the user to have an API key. This is a really easy thing to apply for and should only take a couple minutes to both apply for it and receive it. Here is the page to apply: https://api.census.gov/data/key_signup.html\n\n# store census API key for use in query\n# using tidycensus package functions\nCENSUS_API_KEY = \"YOUR API KEY HERE\"\ncensus_api_key(CENSUS_API_KEY)\n\n# download the data\nacs5_B19307 = get_acs(geography = \"block group\",\n                      variables = B19307_mapping$variable,\n                      year = 2019,\n                      state = \"CA\",\n                      geometry = TRUE)"
  },
  {
    "objectID": "projects/ca_fire_evac/ca_fire_evac.html#data-transformations",
    "href": "projects/ca_fire_evac/ca_fire_evac.html#data-transformations",
    "title": "California Fire Evacuations",
    "section": "Data Transformations",
    "text": "Data Transformations\nHaving the joint distribution by default is great, but it wouldn’t hurt to decompose age and income into their marginal distributions too. I might as well do that now to prepare for later stages where I might want to use them independently.\n\n# compute marginal distribution for age\ndf_age = as.data.frame(acs5_B19307) %&gt;%\n    merge(B19307_mapping, by = \"variable\", all.x = TRUE) %&gt;%\n    mutate(age_householder = paste0(\"age_\", age_householder)) %&gt;%\n    group_by(GEOID, age_householder) %&gt;%\n    summarise(age_count = sum(estimate)) %&gt;%\n    ungroup() %&gt;%\n    pivot_wider(names_from = age_householder, values_from = age_count)\n\n# compute marginal distribution for income\ndf_income = as.data.frame(acs5_B19307) %&gt;%\n    merge(B19307_mapping, by = \"variable\", all.x = TRUE) %&gt;%\n    group_by(GEOID, income) %&gt;%\n    summarise(income_count = sum(estimate)) %&gt;%\n    ungroup() %&gt;%\n    pivot_wider(names_from = income, values_from = income_count)\n\n# having trouble pivoting original data\n# in the presence of the geometry feature\n# so remove the geometry and do it separately here\ndf_joint = as.data.frame(acs5_B19307) %&gt;%\n    select(GEOID, variable, estimate) %&gt;%\n    pivot_wider(names_from = variable, values_from = estimate)\n\n# join marginal distributions into original data\ndf_acs5 = acs5_B19307 %&gt;%\n    select(GEOID, geometry) %&gt;%\n    filter(!duplicated(GEOID)) %&gt;%\n    inner_join(df_age, by = \"GEOID\") %&gt;%\n    inner_join(df_income, by = \"GEOID\") %&gt;%\n    inner_join(df_joint, by = \"GEOID\") %&gt;%\n    mutate(block_group_area = st_area(.))\n\n\nCombining the Data\nNow that I have data from IPAWS on fire-related evacuation orders in CA, and data on population counts by age and income in CA at the census block group level, I would like to combine that information. This can be done by checking for overlaps between the geometries of the two data sources.\n\n# put IPAWS alert data in a form the sf package likes\n# specifying where to find the geometry information\n# and in what form (WKT)\ndf_alerts = st_as_sf(results_df, wkt = \"areaPolygon\")\nst_crs(df_alerts) = st_crs(df_acs5)\n\n# start by computing just the first case as an example\ndf_intersections = df_alerts[1,] %&gt;%\n    select(identifier, areaPolygon) %&gt;%\n    st_intersection(df_acs5) %&gt;%\n    mutate(intersect_area = st_area(.)) %&gt;%\n    mutate(pct_overlap = as.numeric(intersect_area/block_group_area)) %&gt;%\n    st_drop_geometry()\n\n# plot the census block groups and the\n# evacuation area to show the overlap\nggplot() + \n    geom_sf(data=df_acs5[df_acs5$GEOID %in% df_intersections$GEOID,]) +\n    geom_sf(data=df_alerts[1,],color = \"#F8766D\", fill = \"#F8766D\", alpha = 0.3)\n\n\nEven though the number of evacuation orders is small (just a couple hundred) the time it takes to compute all the intersections and areas is several hours.\n\nfor(i in 2:nrow(df_alerts)){\n    # compute for one evacuation order at a time\n    temp = df_alerts[i,] %&gt;%\n        select(identifier, areaPolygon) %&gt;%\n        st_intersection(df_acs5) %&gt;%\n        mutate(intersect_area = st_area(.)) %&gt;%\n        mutate(pct_overlap = as.numeric(intersect_area/block_group_area))  %&gt;%\n        st_drop_geometry()\n    \n    # merge into larger data frame\n    df_intersections = rbind(df_intersections, temp)\n}\n\n\n\nAggregate by Evacuation Event\nThis combined data shows how the area covered by the evacuation order overlaps with the defined census block groups. This data needs to be aggregated at the level of evacuation orders so that one evacuation order has one set of corresponding variables. Here are a couple of simple options.\n\nWeight the census data by the percentage of the area that overlaps with the evacuation order area. If there is more overlap, more of the data from that census block group will be used. This sounds logical, but it assumes that the population of individuals is uniformly distributed throughout that block group, which does not have to be true.\nGive 100% weight to all census block groups that intersect with the evacuation order area. This ensures no individuals are incorrectly excluded if the population of a block group happens to be concentrated in a small area within that block group. However, it may lead to overestimating the relevant population for the evacuation order.\n\nSince the right choice is not immediately obvious, I will just group the data both ways and figure out which way is preferable at a later stage.\n\n# option 1: census features are weighted average\n# based on the intersection of their block group area\n# with the evacuation order area\ndf_intersections_avg = df_intersections %&gt;%\n    select(-GEOID) %&gt;%\n    group_by(identifier) %&gt;%\n    summarise_all(~ sum(.x * pct_overlap)) %&gt;%\n    select(-pct_overlap) %&gt;%\n    inner_join(results_df[,c(\"identifier\",\"sent\",\"areaDesc\")], by = \"identifier\")\n\n# option 2: census features are summed together\n# (given 100% weight) as long as their block group area\n# has any overlap with the evacuation order area\ndf_intersections_sum = df_intersections %&gt;%\n    select(-GEOID, -pct_overlap) %&gt;%\n    group_by(identifier) %&gt;%\n    summarise_all(~ sum(.x)) %&gt;%\n    inner_join(results_df[,c(\"identifier\",\"sent\",\"areaDesc\")], by = \"identifier\")"
  },
  {
    "objectID": "projects/ca_fire_evac/ca_fire_evac.html#predicting-shelter-demand",
    "href": "projects/ca_fire_evac/ca_fire_evac.html#predicting-shelter-demand",
    "title": "California Fire Evacuations",
    "section": "Predicting Shelter Demand",
    "text": "Predicting Shelter Demand\nThe next step is to use the data gathered around each evacuation event to predict how many people will seek shelter from ARC. This has to be kept very general because I do not have access to the data required to make meaningful progress beyond this point.\n\nGet Data on ARC Shelter Demand\nARC should have some basic data about the number of people who took shelter at ARC shelters during historical wildfire evacuation events. As a volunteer working on this proof of concept, this data was not made available to me at this stage of the analysis. As such, I have to make up some fake data to use here.\nGenerating fake data about shelter demand means none of the following results are meaningful and certain decisions about the analysis cannot be made here. For example, I discussed above two options for how data could be aggregated for unique evacuation events and the way to pick which method to use would be to see which produced the better fitting model. Since I don’t have real data, I cannot evaluate which method will be better, so I will arbitrarily pick the second method just to have some data to use here.\nI will assume the number of people who seek shelter is drawn from a random Poisson distribution (so that it is always an integer number of people), and is about 10% of the impacted population.\n\n# calculate total population from age variables\nn_events = nrow(df_intersections_sum)\nevent_pop = apply(df_intersections_sum[,grepl(\"age\",colnames(df_intersections_sum))], 1, sum)\n\n# add affected population and random poisson shelter demand\ndf_intersections_sum$event_pop = event_pop\ndf_intersections_sum$shelter_demand = rpois(n_events, lambda = 0.1*event_pop)\n\nThis also skips several other complications that are likely to arise with the actual data, such as figuring out how to match the shelter data with the right evacuation event.\n\n\nA Simple Model\nMy first instinct when thinking about this problem is to try a Poisson regression. Since the number of people seeking shelter is count data, the Poisson distribution is convenient for modeling it because it will handle cases where the count is zero if that occurs (no one showed up at the shelter) and will enforce non-negativity.\nIt may be useful to think about this problem in the context of fractions, as in “what fraction of the exposed population will seek shelter?” This is also something easily incorporated within the Poisson regression framework.\nThe basic Poisson regression:\n\\[\n    log(E(Y|x)) = Bx\n\\] Poisson regression modeling counts as a fraction of exposed population:\n\\[\n    log(\\frac{E(Y|x)}{exposure}) = log(E(Y|x)) - log(exposure) = Bx - log(exposure)\n\\]\nI will choose the second option just to showcase how the implementation works. I will start with some data transformations, scaling the features by the exposed population so that they are fractions of the total population rather than counts. This normalizes them across observations. I will just use the marginal age variables here. In practice I would want to include the income variables and then also test whether the interaction variables from the joint distribution are useful, but none of that matters with the fake data I have right now. Then I can fit the model.\n\n# build modeling dataset\n# start with just the age categories\nmodel_data = df_intersections_sum[,grepl(\"age\",colnames(df_intersections_sum))]\n\n# normalize by the exposed population in each event\nmodel_data = sweep(model_data, 1, 1/event_pop, FUN=\"*\")\n\n# drop the first variable to avoid perfect multicollinearity\n# since they will all add up to 1 otherwise\nmodel_data = model_data[,-1]\n\n# add the target variable and exposure variable\nmodel_data$y = df_intersections_sum$shelter_demand\nmodel_data$exposure = event_pop\n\n# fit the model\nfit = glm(y ~ . - exposure + offset(log(exposure)), data = model_data, family=poisson(link=log))\nsummary(fit)\n\n\nCall:\nglm(formula = y ~ . - exposure + offset(log(exposure)), family = poisson(link = log), \n    data = model_data)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  -2.1469     0.1275 -16.844   &lt;2e-16 ***\nage_25_44    -0.1838     0.1637  -1.123    0.261    \nage_45_64    -0.1070     0.1116  -0.959    0.338    \nage_65_inf   -0.1886     0.1356  -1.391    0.164    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 195.42  on 247  degrees of freedom\nResidual deviance: 192.31  on 244  degrees of freedom\nAIC: 2031.1\n\nNumber of Fisher Scoring iterations: 3\n\n\nThe only part of the model that looks significant is the intercept, which is correct because I generated shelter demand as a constant 10% of the exposed population. In the real data, I might expect to see exposed populations made up of younger people or having lower incomes significantly affect shelter demand.\n\n\nPrediction Intervals\nThe Poisson regression I just constructed models expected shelter demand. In other words, the average number of people expected to seek shelter. The estimate produced by the model may over-predict or under-predict. ARC may care more about under-predicting than over-predicting. Depending on the resources available, ARC may prefer to over-predict demand and send more people to open unnecessary shelters as opposed to under-predicting demand and having to turn away people seeking shelter or not have shelter available for them.\nOne way to address this within the context of this model is a prediction interval. Unlike a confidence interval which only incorporates uncertainty about model parameter estimates, a prediction interval also incorporates uncertainty about the outcomes. Building a 95% prediction interval means that we would expect that the observed outcome falls within the interval 95% of the time. Using the upper bound of that interval may be a good way to estimate an upper limit of what shelter demand could be for a particular scenario.\n\n# what percentage of model predictions\n# under-estimate shelter demand?\nmean(model_data$y - predict(fit, type = \"response\") &gt; 0)\n\n[1] 0.5282258\n\n# bootstrap setup\nset.seed(42)\nn_bootstraps = 1000\nboot_results = matrix(0, nrow = nrow(model_data), ncol = n_bootstraps)\n\n# bootstrap prediction interval\nfor(i in 1:n_bootstraps){\n    # re-sample from modeling data with replacement\n    boot_idx = sample(nrow(model_data), replace = TRUE)\n    df_boot = model_data[boot_idx,]\n    # fit the model with bootstrapped data\n    fit_boot = glm(y ~ . - exposure + offset(log(exposure)), \n                   data = df_boot, family=poisson(link=log))\n    # predictions of actual data using bootstrapped model\n    # distributions from this step would give the confidence interval\n    y_pred_boot = predict(fit_boot, newdata = model_data, type = \"response\")\n    # now need to account for residual variance\n    # given that the data is assumed to be poisson distributed\n    # the prediction is the expected value or lambda\n    # so we can just sample from a poisson distribution with that lambda\n    boot_results[,i] = rpois(nrow(model_data), lambda = y_pred_boot)\n}\n\n# 95% prediction interval\nlower_bound = apply(boot_results, 1, quantile, probs = 0.025)\nupper_bound = apply(boot_results, 1, quantile, probs = 0.975)\n\n# what percentage of the time does\n# the upper bound of the 95% prediction interval\n# under-estimate shelter demand?\nmean(model_data$y - upper_bound &gt; 0)\n\n[1] 0.01612903"
  },
  {
    "objectID": "projects/ca_fire_evac/ca_fire_evac.html#conclusion",
    "href": "projects/ca_fire_evac/ca_fire_evac.html#conclusion",
    "title": "California Fire Evacuations",
    "section": "Conclusion",
    "text": "Conclusion\nI have shown a proposed approach for acquiring data from FEMA and the US Census to support making data-driven estimates of shelter demand during wildfire evacuations in California. While the full analysis is incomplete without the actual shelter data from ARC, I laid out some initial possibilities based on the information available."
  },
  {
    "objectID": "projects/impact_eval/impact_eval.html",
    "href": "projects/impact_eval/impact_eval.html",
    "title": "Impact Evaluation",
    "section": "",
    "text": "One of the volunteer projects I worked on for the American Red Cross (ARC) was to help develop a cost benefit analysis for one of their programs. The International Federation of the Red Cross and Red Crescent Societies (IFRC) has a nice summary dashboard they maintain with high level information about some of their specific interventions: https://go.ifrc.org/.\nOne of the programs that IFRC supports is early action or anticipatory action. Unlike a responsive program that would wait for a disaster to occur and then provide aid afterwards, an anticipatory action program aims to provide preventative aid before the disaster occurs. This can be effective for certain types of disasters where we can reasonably predict the event in advance.\nFor example, we might predict a coming flood using data about river levels, weather, upstream water flow, etc. If we wait until after the flood occurs, the flooding could spread water-borne diseases among the affected population and a relief effort might involve providing medicine or other forms of healthcare. Alternatively, an anticipatory action might be to provide chlorine tablets or other water treatment options before the flood occurs so that water-borne disease does not become prevalent after the flood. If effective, this type of preventative action can allow an organization to help more people with the available budget, since these types of preventative actions are often more cost efficient than treating problems after they become more serious."
  },
  {
    "objectID": "projects/impact_eval/impact_eval.html#introoverview",
    "href": "projects/impact_eval/impact_eval.html#introoverview",
    "title": "Impact Evaluation",
    "section": "",
    "text": "One of the volunteer projects I worked on for the American Red Cross (ARC) was to help develop a cost benefit analysis for one of their programs. The International Federation of the Red Cross and Red Crescent Societies (IFRC) has a nice summary dashboard they maintain with high level information about some of their specific interventions: https://go.ifrc.org/.\nOne of the programs that IFRC supports is early action or anticipatory action. Unlike a responsive program that would wait for a disaster to occur and then provide aid afterwards, an anticipatory action program aims to provide preventative aid before the disaster occurs. This can be effective for certain types of disasters where we can reasonably predict the event in advance.\nFor example, we might predict a coming flood using data about river levels, weather, upstream water flow, etc. If we wait until after the flood occurs, the flooding could spread water-borne diseases among the affected population and a relief effort might involve providing medicine or other forms of healthcare. Alternatively, an anticipatory action might be to provide chlorine tablets or other water treatment options before the flood occurs so that water-borne disease does not become prevalent after the flood. If effective, this type of preventative action can allow an organization to help more people with the available budget, since these types of preventative actions are often more cost efficient than treating problems after they become more serious."
  },
  {
    "objectID": "projects/impact_eval/impact_eval.html#setting-up",
    "href": "projects/impact_eval/impact_eval.html#setting-up",
    "title": "Impact Evaluation",
    "section": "Setting Up",
    "text": "Setting Up\n\nLoading Libraries\n\nlibrary(lmtest) # to adjust model using clustered error covariance\nlibrary(sandwich) # to compute clustered error covariance\nlibrary(dplyr) # for data manipulation\nlibrary(tidyr) # for data manipulation\nlibrary(ggplot2) # to make plots"
  },
  {
    "objectID": "projects/impact_eval/impact_eval.html#generating-the-data",
    "href": "projects/impact_eval/impact_eval.html#generating-the-data",
    "title": "Impact Evaluation",
    "section": "Generating the Data",
    "text": "Generating the Data\nI will generate my own fake data to use as an example for explaining some general ideas. Continuing with the flood example, let us say that we want to evaluate the effectiveness of an anticipatory action (intervention) that aimed to reduce healthcare expenditures by providing chlorine tablets immediately before a flood event to help reduce the incidence of water-borne disease. Based on the available budget, the treatment can only be provided to part of the affected population. Some time after the event, we survey the affected population to see what their actual healthcare expenditures were. This is the data that we use to evaluate the impact of our program.\n\n# set seed for consistency\nset.seed(42)\ndf = data.frame()\n\n# assume total population of 1000 households\nn_households = 1000\n\n# each household has 1-7 individuals\nhh_size = sample.int(7, n_households, replace = TRUE)\n\n# generate data for each individual\nfor(i in 1:n_households){\n    hh = data.frame(\n        hh_id = i, # household ID\n        hh_size = hh_size[i], # household size\n        age = sample.int(65, hh_size[i], replace=TRUE), # age from 1-65\n        female = ifelse(runif(hh_size[i]) &lt; 0.5, 1, 0)\n    )\n    df = rbind(df, hh)\n}\n\n# only 100 households receive the treatment\n# and treatment is not purely random\n# households that are larger, have more young or elderly,\n# or are more female are given some preference\ntreatment_select = df %&gt;% \n    group_by(hh_id, hh_size) %&gt;%\n    summarise(pct_young_old = mean((age &lt;= 12) | (age &gt;= 50)),\n              pct_F = mean(female)) %&gt;%\n    mutate(treatment_prob = hh_size + 2*pct_young_old + 2*pct_F)\n\nhh_id_treatment = sample(treatment_select$hh_id, 100, \n                         prob = treatment_select$treatment_prob)\n\ndf$treatment = ifelse(df$hh_id %in% hh_id_treatment, 1, 0)\n\n# simulate actual healthcare costs\n# hh_size increases costs because of increased exposure opportunity\n# age young or old increases costs due to weaker immune systems\n# treatment is effective and reduces costs\ndf$hc_cost = 50 + 5*df$hh_size + 20*((df$age &lt;= 12) | (df$age &gt;= 50)) - 20*df$treatment\n\n# add cluster error and individual error\ndf_cluster_error = data.frame(hh_id = 1:n_households, \n                              e_household = rnorm(n_households, sd = 5))\n\ndf = df %&gt;%\n    inner_join(df_cluster_error, by = \"hh_id\") %&gt;%\n    mutate(e_individual = rnorm(nrow(df), sd = 5)) %&gt;%\n    mutate(hc_cost = hc_cost + e_household + e_individual)"
  },
  {
    "objectID": "projects/impact_eval/impact_eval.html#average-treatment-effect",
    "href": "projects/impact_eval/impact_eval.html#average-treatment-effect",
    "title": "Impact Evaluation",
    "section": "Average Treatment Effect",
    "text": "Average Treatment Effect\nThe metric we want to estimate is the average treatment effect (ATE). The ATE is the expected effect of the treatment on the target outcome. In our example, this would be the dollar amount by which the treatment reduces healthcare expenses for each individual, on average. Estimating this impact is important for evaluating an intervention because it tells us how effective the intervention was at achieving the desired outcome, and hopefully can be generalized to other events and inform decisions about whether to use this same intervention in future scenarios.\n\nBasic Calculation\nThe definition for ATE is the average difference in outcomes for treated vs non-treated individuals. More formally:\n\\[\n    ATE = E[y_1 - y_0]\n\\] Or, writing this in terms of an estimate from a sample:\n\\[\n    \\widehat{ATE} = \\frac{1}{N} \\sum_i (y_1(i) - y_0(i))\n\\]\nThe problem in both of these definitions is that we never observe both \\(y_1(i)\\) and \\(y_0(i)\\), because any one individual either receives the treatment or does not, so we cannot observe what happened to that same individual under both scenarios. So instead, the practical solution is to examine the difference in means between the treated and untreated groups, which should be a good estimate of the ATE under certain assumptions.\n\\[\n    E[Y|X = 1] - E[Y|X = 0]\n\\]\n\n# calculate simple ATE by hand\nEY1 = mean(df$hc_cost[df$treatment == 1])\nEY0 = mean(df$hc_cost[df$treatment == 0])\nATE = EY1 - EY0\nATE\n\n[1] -16.81879\n\n\n\n\nLinear Regression\nA simple regression yields the same estimate for the ATE, since using a binary indicator for the treatment is effectively the same as just taking the mean for each group. This is a useful approach if you want to do anything beyond getting this single point estimate.\n\n# estimate via simple linear model\nfit = lm(hc_cost ~ treatment, data = df)\nsummary(fit)\n\n\nCall:\nlm(formula = hc_cost ~ treatment, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-46.734 -10.373  -0.274  10.843  46.409 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  82.8386     0.2561   323.4   &lt;2e-16 ***\ntreatment   -16.8188     0.7474   -22.5   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 14.98 on 3872 degrees of freedom\nMultiple R-squared:  0.1157,    Adjusted R-squared:  0.1154 \nF-statistic: 506.4 on 1 and 3872 DF,  p-value: &lt; 2.2e-16\n\n# 95% confidence interval for ATE\nconfint(fit, \"treatment\", level = 0.95)\n\n              2.5 %    97.5 %\ntreatment -18.28405 -15.35352\n\n# same thing using the standard errors directly\ncoef(fit)[2] + qt(0.025, df=fit$df.residual) * summary(fit)$coefficients[2,2]\n\ntreatment \n-18.28405 \n\ncoef(fit)[2] + qt(0.975, df=fit$df.residual) * summary(fit)$coefficients[2,2]\n\ntreatment \n-15.35352 \n\n\nThe estimated coefficient on the treatment variable matches the manual calculation, as expected. The estimated ATE of about -17 means that receiving the treatment reduces healthcare expenditures by an average of $17 per person. However, this estimate does not quite match the true effect of -20 which is the effect I used to generate the data, and the 95% confidence interval does not include the true effect either. This is because of some additional complications I added while generating the data which will need to be addressed."
  },
  {
    "objectID": "projects/impact_eval/impact_eval.html#additional-considerations",
    "href": "projects/impact_eval/impact_eval.html#additional-considerations",
    "title": "Impact Evaluation",
    "section": "Additional Considerations",
    "text": "Additional Considerations\nThe simple estimate for ATE above is useful to look at, but there are usually more factors that need to be considered in this type of analysis. These will differ substantially based on the situation. Here we will examine a few that are relevant to this example.\n\nCovariate Adjustment\nA confounding factor is a variable that influences both the dependent variable and independent variable. To estimate the effect of X on Y, we must suppress the effects of confounding variables that influence both X and Y. We say that X and Y are confounded by some other variable Z whenever Z causally influences both X and Y.\n\nIn this example, there are variables such as household size and age (Z) which influence both the treatment effect (X) and healthcare expenditures (Y). They influence treatment because treatment was not assigned randomly; the assignment was weighted to prefer larger households and households with young and elderly members. They have a direct effect on healthcare costs based on how those costs were simulated in this example.\nWe can account for these factors by explicitly estimating their effects in the model. Conditioning our estimate on these additional variables is sometimes called covariate adjustment. Another way to think about this is that the simple version of the model suffers from omitted variable bias and the previous estimate of the ATE was incorporating effects which were actually attributable to these confounding factors.\n\n# add indicator for young/elderly age group\ndf$age_young_old = ifelse((df$age &lt;= 12) | (df$age &gt;= 50), 1, 0)\n\n# estimate via simple linear model\nfit = lm(hc_cost ~ treatment + hh_size + age_young_old, data = df)\nsummary(fit)\n\n\nCall:\nlm(formula = hc_cost ~ treatment + hh_size + age_young_old, data = df)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-23.5940  -4.6384   0.0414   4.9515  24.1408 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    49.58349    0.35093  141.29   &lt;2e-16 ***\ntreatment     -19.71544    0.35279  -55.88   &lt;2e-16 ***\nhh_size         5.05749    0.06416   78.83   &lt;2e-16 ***\nage_young_old  20.12108    0.22854   88.04   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.046 on 3870 degrees of freedom\nMultiple R-squared:  0.8043,    Adjusted R-squared:  0.8042 \nF-statistic:  5303 on 3 and 3870 DF,  p-value: &lt; 2.2e-16\n\n# 95% confidence interval for ATE\nconfint(fit, \"treatment\", level = 0.95)\n\n              2.5 %    97.5 %\ntreatment -20.40712 -19.02376\n\n\nAfter adjusting for these confounding factors, our ATE estimate is much closer to the true value and the 95% confidence interval includes the true value.\n\n\nInverse Probability Treatment Weights\nAnother method for addressing this concern around confounding is through the use of propensity scores, or inverse probability treatment weights. This is a popular method for observational studies. Unlike a randomized controlled trial (RCT) where an experiment is designed in advance to have treatment and control groups balanced across other covariates of interest, an observational study has no control over the treatment assignment and will often have treatment and control groups which are not balanced across other covariates. This technique is essentially re-weighting the observational data to make it look more it came from a balanced RCT design, which is important for estimating ATE since the calculation assumes we are comparing similar individuals.\nThis method starts by building a model to predict a propensity score, which is the probability that the treatment was assigned to an individual. Observations are then re-weighted by dividing by this probability in order to achieve better balance.\n\n# estimate propensity score\nfit_ps = glm(treatment ~ hh_size + age_young_old + female, data = df, family = binomial())\nehat = predict(fit_ps, type = \"response\")\ndf$ipw = df$treatment/ehat + (1-df$treatment)/(1-ehat)\n\n# estimate ATE using weighted linear model\nfit = lm(hc_cost ~ treatment, data = df, weights = ipw)\nsummary(fit)\n\n\nCall:\nlm(formula = hc_cost ~ treatment, data = df, weights = ipw)\n\nWeighted Residuals:\n     Min       1Q   Median       3Q      Max \n-128.314  -12.129   -0.438   12.748   85.704 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  83.1895     0.3267  254.60   &lt;2e-16 ***\ntreatment   -18.9913     0.4627  -41.04   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 20.34 on 3872 degrees of freedom\nMultiple R-squared:  0.3032,    Adjusted R-squared:  0.303 \nF-statistic:  1684 on 1 and 3872 DF,  p-value: &lt; 2.2e-16\n\n# 95% confidence interval for ATE\nconfint(fit, \"treatment\", level = 0.95)\n\n              2.5 %    97.5 %\ntreatment -19.89851 -18.08407\n\n\nThis is an improvement over the un-adjusted estimate. One additional diagnostic we can examine here is the extent to which this re-weighting improved covariate balance. This is often done by comparing Standardized Mean Differences (SMD) for the covariates of interest before and after adjustment using these weights. A common rule of thumb for good balance is a SMD threshold of 0.1, although this rule is somewhat arbitrary.\n\n# calculate SMD for covariates of interest\ncov_bal_smd = df %&gt;%\n    group_by(treatment) %&gt;%\n    summarise(hh_size_mean = mean(hh_size),\n              hh_size_wmean = weighted.mean(hh_size, ipw),\n              hh_size_sd = sd(hh_size),\n              age_mean = mean(age_young_old),\n              age_wmean = weighted.mean(age_young_old, ipw),\n              female_mean = mean(female),\n              female_wmean = weighted.mean(female, ipw)) %&gt;%\n    summarise(hh_size_unadjusted = abs(diff(hh_size_mean)) / sqrt(sum(hh_size_sd^2)/2),\n              hh_size_adjusted = abs(diff(hh_size_wmean)) / sqrt(sum(hh_size_sd^2)/2),\n              age_unadjusted = abs(diff(age_mean)),\n              age_adjusted = abs(diff(age_wmean)),\n              female_unadjusted = abs(diff(female_mean)),\n              female_adjusted = abs(diff(female_wmean))) %&gt;%\n    pivot_longer(everything(), values_to = \"SMD\") %&gt;%\n    mutate(adjusted = ifelse(grepl(\"unadjusted\", name), \"unadjusted\", \"adjusted\")) %&gt;%\n    mutate(covariate = sub(\"_[^_]*$\", \"\", name))\n\n# make love plot\ncov_bal_smd %&gt;%\n    mutate(adjusted = as.factor(adjusted)) %&gt;%\n    ggplot(aes(x = SMD, y = covariate, color = adjusted, shape = adjusted)) +\n    geom_vline(xintercept = 0) +\n    geom_vline(xintercept = 0.1, lty = \"dashed\") +\n    geom_point(size = 3) +\n    labs(x = \"Absolute Standardized Mean Differences\", y = \"Covariate\") +\n    theme(legend.title = element_blank())\n\n\n\n\nWe can see that covariate balance as measured by SMD improves after re-weighting the data with the inverse probability weights. The balance for household size is under the threshold of 0.1 after re-weighting, and the balance for the other covariates has also improved although they were already under the 0.1 threshold.\n\n\nClustered Standard Errors\nThere are certain situations where we might expect to observe clustering in our data. In this fake example, we have created clustering in the errors by adding an error component at the household level as well as the individual level. The rationale is that people in the same household will tend to have similar experiences. For instance, if one person from a household gets sick they are likely to pass that sickness to the other members of the household.\nThis clustering is present in the treatment assignment as well. The treatment is assigned to an entire household, not to specific individuals. This type of study design is similar to a cluster randomized trial where treatment is assigned at a more aggregate level even though outcomes are measured at an individual level. There are a number of reasons why this design might be desirable. In this fake example, if we had a family of two parents and two young children and randomly decided to only give the treatment to the parents, it would not be reasonable to expect them to follow the study design and treat only their own drinking water and let their children drink dirty water. They would likely give their treatment to their children, causing spillover effects which would reduce the power of our estimates.\nWhen clustering exists in the data it means that the errors are not all independent since there is dependence within each cluster. Observations within the same cluster (household) are similar to each other. This means the standard errors estimated by the model are too small and we need to adjust them to make sure our estimates are not overconfident. The adjustment will increase the standard errors, resulting in wider confidence intervals for our ATE estimate.\n\n# first model using covariate adjustment\nfit1 = lm(hc_cost ~ treatment + hh_size + age_young_old, data = df)\nfit1_coef_cl = coeftest(fit1, vcov = vcovCL, cluster = ~hh_id)\nfit1_coef_cl\n\n\nt test of coefficients:\n\n               Estimate Std. Error t value  Pr(&gt;|t|)    \n(Intercept)    49.58349    0.48875 101.450 &lt; 2.2e-16 ***\ntreatment     -19.71544    0.67000 -29.426 &lt; 2.2e-16 ***\nhh_size         5.05749    0.10010  50.526 &lt; 2.2e-16 ***\nage_young_old  20.12108    0.21921  91.788 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# 95% confidence interval for ATE\ndf_clust_ci = data.frame(\n    model = \"covariate adjustment\",\n    clustered_std_err = \"no\",\n    ci_95_lower = coef(fit1)[2] + qt(0.025, df=fit1$df.residual) * summary(fit1)$coefficients[2,2],\n    ci_95_upper = coef(fit1)[2] + qt(0.975, df=fit1$df.residual) * summary(fit1)$coefficients[2,2]\n)\ndf_clust_ci = rbind(df_clust_ci, data.frame(\n    model = \"covariate adjustment\",\n    clustered_std_err = \"yes\",\n    ci_95_lower = coef(fit1)[2] + qt(0.025, df=fit1$df.residual) * fit1_coef_cl[2,2],\n    ci_95_upper = coef(fit1)[2] + qt(0.975, df=fit1$df.residual) * fit1_coef_cl[2,2]\n))\n\n# second model using inverse probability weights\nfit2 = lm(hc_cost ~ treatment, data = df, weights = ipw)\nfit2_coef_cl = coeftest(fit2, vcov = vcovCL, cluster = ~hh_id)\nfit2_coef_cl\n\n\nt test of coefficients:\n\n             Estimate Std. Error t value  Pr(&gt;|t|)    \n(Intercept)  83.18950    0.41622 199.871 &lt; 2.2e-16 ***\ntreatment   -18.99129    1.03750 -18.305 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# 95% confidence interval for ATE\ndf_clust_ci = rbind(df_clust_ci, data.frame(\n    model = \"inverse probability weights\",\n    clustered_std_err = \"no\",\n    ci_95_lower = coef(fit2)[2] + qt(0.025, df=fit2$df.residual) * summary(fit2)$coefficients[2,2],\n    ci_95_upper = coef(fit2)[2] + qt(0.975, df=fit2$df.residual) * summary(fit2)$coefficients[2,2]\n))\ndf_clust_ci = rbind(df_clust_ci, data.frame(\n    model = \"inverse probability weights\",\n    clustered_std_err = \"yes\",\n    ci_95_lower = coef(fit2)[2] + qt(0.025, df=fit2$df.residual) * fit2_coef_cl[2,2],\n    ci_95_upper = coef(fit2)[2] + qt(0.975, df=fit2$df.residual) * fit2_coef_cl[2,2]\n))\n\n# display table\nrownames(df_clust_ci) = NULL\nknitr::kable(df_clust_ci)\n\n\n\n\n\n\n\n\n\n\nmodel\nclustered_std_err\nci_95_lower\nci_95_upper\n\n\n\n\ncovariate adjustment\nno\n-20.40712\n-19.02376\n\n\ncovariate adjustment\nyes\n-21.02903\n-18.40185\n\n\ninverse probability weights\nno\n-19.89851\n-18.08407\n\n\ninverse probability weights\nyes\n-21.02538\n-16.95719\n\n\n\n\n\nAs expected, accounting for the clustering in the errors increases the estimate for the standard error of ATE and consequently widens the bounds of the associated 95% confidence interval. It makes our estimate less precise, but it is a more accurate representation of what our confidence in our estimate should be."
  },
  {
    "objectID": "projects/impact_eval/impact_eval.html#conclusion",
    "href": "projects/impact_eval/impact_eval.html#conclusion",
    "title": "Impact Evaluation",
    "section": "Conclusion",
    "text": "Conclusion\nWe discussed estimating ATE to evaluate the impact of an intervention and covered a few examples of the problems that can arise in this type of analysis. Of course, there are a lot of other problems that can arise and techniques available to address them which I have not covered here. The World Bank provides a general description of more of the common techniques for impact evaluations in their publication here which the interested reader can download for free. Also, the use case I examined was focused on using observational data, but another approach would be to try to run the analysis concurrently with the intervention and have an RCT study design. The Abdul Latif Jameel Poverty Action Lab (J-PAL) has a lot of good resources on the subject, as well as impact evaluations in general."
  },
  {
    "objectID": "projects/risk_parity_portfolio/risk_parity_portfolio.html",
    "href": "projects/risk_parity_portfolio/risk_parity_portfolio.html",
    "title": "Risk Parity Portfolio",
    "section": "",
    "text": "One algorithm under the umbrella of portfolio optimization that sounded interesting to me is called the risk parity portfolio. All of the portfolio optimization methods are about picking a set of weights for the different assets in a portfolio to optimize some objective function. In the case of the risk parity portfolio, the weights are selected so that each asset in the portfolio contributes an equal amount of risk. Risk is measured in terms of standard deviation of returns."
  },
  {
    "objectID": "projects/risk_parity_portfolio/risk_parity_portfolio.html#introoverview",
    "href": "projects/risk_parity_portfolio/risk_parity_portfolio.html#introoverview",
    "title": "Risk Parity Portfolio",
    "section": "",
    "text": "One algorithm under the umbrella of portfolio optimization that sounded interesting to me is called the risk parity portfolio. All of the portfolio optimization methods are about picking a set of weights for the different assets in a portfolio to optimize some objective function. In the case of the risk parity portfolio, the weights are selected so that each asset in the portfolio contributes an equal amount of risk. Risk is measured in terms of standard deviation of returns."
  },
  {
    "objectID": "projects/risk_parity_portfolio/risk_parity_portfolio.html#setting-up",
    "href": "projects/risk_parity_portfolio/risk_parity_portfolio.html#setting-up",
    "title": "Risk Parity Portfolio",
    "section": "Setting Up",
    "text": "Setting Up\n\nLoading Libraries\n\nlibrary(quantmod) # to download stock data from Yahoo Finance\nlibrary(dplyr) # to transform data\nlibrary(tidyr) # to convert from wide to long data format\nlibrary(ggplot2) # to draw charts\n\n\n\nLoading Data\nFor this example I will pick a few tickers with different risk profiles to show how the algorithm works. I will just use one year of data since this is mainly an exercise in understanding the algorithm, not trying to actually build a trading strategy and backtest over long periods.\nThe quantmod package contains a function called getSymbols() which lets the user pick one or more stock symbols and specify a time period, and then it downloads the relevant data from Yahoo Finance.\n\n# pick a few tickers and download the data from Yahoo Finance\nsymbols_vec = c(\"SPY\",\"TLT\",\"GLD\",\"AAPL\",\"NFLX\",\"NVDA\")\ngetSymbols(symbols_vec, from = \"2023-01-01\", to = \"2023-12-31\")\n\n# combine adjusted close prices into one dataset\n# a bit weird because quantmod likes to return each ticker\n# as a new object in the environment\ndf_returns = data.frame()\nfor(i in 1:length(symbols_vec)){\n    # keep only the adjusted returns data\n    cmd = paste0(symbols_vec[i], \"=\", symbols_vec[i], \"[,grepl('Adjusted',colnames(\", symbols_vec[i], \"))]\")\n    eval(parse(text = cmd))\n    # combine into one data frame\n    if(i == 1){\n        cmd = paste0(\"df_returns = \", symbols_vec[i])\n        eval(parse(text = cmd))\n    }else{\n        cmd = paste0(\"df_returns = merge(df_returns, \", symbols_vec[i], \")\")\n        eval(parse(text = cmd))\n    }\n}\n\n# clean up column names\ncolnames(df_returns) = gsub(\".Adjusted\", \"\", colnames(df_returns))\n\n# transform adjusted close prices to daily returns\ndf_returns[-1,] = apply(df_returns, 2, function(x){x[2:length(x)] / x[1:(length(x)-1)] - 1})\ndf_returns = df_returns[-1,]\ndf_returns = data.frame(Date = index(df_returns), df_returns)\n\nHere is how the data looks after this initial cleanup.\n\nknitr::kable(head(df_returns))\n\n\n\n\n\n\n\n\n\n\n\n\n\nDate\nSPY\nTLT\nGLD\nAAPL\nNFLX\nNVDA\n\n\n\n\n2023-01-04\n0.0077202\n0.0136999\n0.0094119\n0.0103143\n0.0490252\n0.0303180\n\n\n2023-01-05\n-0.0114134\n0.0041811\n-0.0124515\n-0.0106046\n0.0009373\n-0.0328158\n\n\n2023-01-06\n0.0229321\n0.0183964\n0.0187075\n0.0367939\n0.0188892\n0.0416404\n\n\n2023-01-09\n-0.0005668\n0.0053241\n0.0022451\n0.0040890\n-0.0012042\n0.0517532\n\n\n2023-01-10\n0.0070129\n-0.0165499\n0.0036760\n0.0044563\n0.0392486\n0.0179805\n\n\n2023-01-11\n0.0126477\n0.0162516\n-0.0006868\n0.0211122\n-0.0008549\n0.0057829"
  },
  {
    "objectID": "projects/risk_parity_portfolio/risk_parity_portfolio.html#benchmarks",
    "href": "projects/risk_parity_portfolio/risk_parity_portfolio.html#benchmarks",
    "title": "Risk Parity Portfolio",
    "section": "Benchmarks",
    "text": "Benchmarks\nIt may be helpful to look at a couple of other portfolio allocation methods to get a sense of how this method compares with other options. I will pick a couple of simple examples to compare with.\n\nEqually Weighted Portfolio\nThis method allocates an equal amount of money to each asset in the portfolio. It is extremely simple, but that is part of what makes it a useful benchmark. The difference between this method and the risk parity portfolio is that the equal weights here mean equally balance-weighted, not equally risk-weighted. Putting an equal amount of money into treasuries vs tech stocks would likely mean that the money in tech stocks contributes much more risk to the portfolio compared with the money invested in treasuries. In other words, while the amount of money invested is diversified across assets, the risk is not necessarily as diversified since one asset class might represent the majority of the risk.\n\n# weight vector for equal weight\nw1 = rep(1, ncol(df_returns)-1)\nw1 = w1/sum(w1)\nw1\n\n[1] 0.1666667 0.1666667 0.1666667 0.1666667 0.1666667 0.1666667\n\n\n\n\nMinimum Risk Portfolio\nThis portfolio optimization problem can be defined as minimizing the total variance of the portfolio subject to the condition that the weights must add up to one.\n\\[\n\\begin{align}\n    Minimize& \\hspace{2.5em} \\frac{1}{2} w^T \\Sigma w \\\\\n    \\text{subject to}& \\hspace{2.5em} 1^T w = 1\n\\end{align}\n\\]\nThis optimization problem can be solved using lagrange multipliers. Construct the langrangian as the combination of the objective function and the constraints, then the two partial derivatives can be set equal to zero and used to solve for the unknown quantities of interest.\n\\[\n\\begin{align}\n    L(x,\\lambda) = f(x) - \\lambda g(x) \\\\\n    \\frac{dL}{dx} = 0 \\hspace{2.5em} \\frac{dL}{d \\lambda} = 0\n\\end{align}\n\\] In the context of this problem this becomes:\n\\[\n\\begin{align}\n    L(w,\\lambda) &= \\frac{1}{2} w^T \\Sigma w - \\lambda (1^T w - 1) \\\\\n    \\frac{dL}{dw} &= \\Sigma w - \\lambda 1^T = 0 \\\\\n    w &= \\lambda \\Sigma^{-1} 1 \\\\\n    \\\\\n    \\frac{dL}{d \\lambda} &= 1^T w - 1 = 0 \\\\\n    1^T w &= 1\n\\end{align}\n\\] Multiplying the first equation through by 1 on the left side lets us use the constraint from the second equation which removes \\(w\\) and allows us to solve for \\(\\lambda\\).\n\\[\n\\begin{align}\n    w &= \\lambda \\Sigma^{-1} 1 \\\\\n    1^T w &= \\lambda 1^T \\Sigma^{-1} 1 \\\\\n    1 &= \\lambda 1^T \\Sigma^{-1} 1 \\\\\n    \\lambda &= \\frac{1}{1^T \\Sigma^{-1} 1}\n\\end{align}\n\\] Plugging \\(\\lambda\\) back into the equation for \\(w\\) solves for \\(w\\).\n\\[\n\\begin{align}\n    w &= \\lambda \\Sigma^{-1} 1 \\\\\n    &= \\frac{\\Sigma^{-1} 1}{1^T \\Sigma^{-1} 1}\n\\end{align}\n\\]\nThis is simple to implement, it just involves computing and then inverting the covariance matrix \\(\\Sigma\\) for the daily returns data.\n\n# get inverse of covariance matrix of returns\ncov_mat = cov(df_returns[,-1])\ncov_mat_inv = solve(cov_mat)\n\n# compute weights\nvec1 = rep(1, nrow(cov_mat_inv))\nlambda = vec1 %*% cov_mat_inv %*% vec1\nlambda = as.numeric(1/lambda)\nw2 = as.vector(lambda * cov_mat_inv %*% vec1)\nw2\n\n[1]  0.61758079  0.07622508  0.41066270 -0.02959958 -0.03462770 -0.04024129\n\n\nOne interesting thing to note here is that while the weights were constrained to sum to 1, they were not constrained to be non-negative. The negative weights here on the riskier assets mean that achieving the minimum variance portfolio requires taking short positions in those assets."
  },
  {
    "objectID": "projects/risk_parity_portfolio/risk_parity_portfolio.html#risk-parity-portfolio",
    "href": "projects/risk_parity_portfolio/risk_parity_portfolio.html#risk-parity-portfolio",
    "title": "Risk Parity Portfolio",
    "section": "Risk Parity Portfolio",
    "text": "Risk Parity Portfolio\nThis time the problem is not to minimize risk, it is to impose a constraint that each asset in the portfolio contributes equally to the total risk.\n\nTheory\nThe total variance of the portfolio was minimized in the objective function of the minimum variance portfolio. Standard deviation is just the square root of variance, so the total standard deviation of the portfolio is \\(\\sigma (w) = \\sqrt{w^T \\Sigma w}\\). This can be decomposed by taking the partial derivative with respect to the weight of each individual asset in the portfolio.\n\\[\n    \\sigma(w) = \\sum_{i=1}^N w_i \\frac{d \\sigma}{dw_i} = \\sum_{i=1}^N \\frac{w_i(\\Sigma w)_i}{\\sqrt{w^T \\Sigma w}}\n\\]\nIn other words, asset \\(i\\)’s contribution to the total portfolio standard deviation, or its Risk Contribution (RC) is:\n\\[\n    RC_i = \\frac{w_i(\\Sigma w)_i}{\\sqrt{w^T \\Sigma w}}\n\\]\nThe constraint that needs to be imposed is to have each asset have the same risk contribution, which should be an equal amount of the total risk of the portfolio.\n\\[\n    RC_i = \\frac{1}{N} \\sigma(w) = \\frac{1}{N} \\sqrt{w^T \\Sigma w}\n\\] Dividing both sides by \\(\\sigma(w)\\) normalizes by total portfolio risk, giving us:\n\\[\n\\begin{align}\n    \\frac{w_i(\\Sigma w)_i}{\\sqrt{w^T \\Sigma w}} &= \\frac{1}{N} \\sqrt{w^T \\Sigma w} \\\\\n    \\frac{w_i(\\Sigma w)_i}{w^T \\Sigma w} &= \\frac{1}{N}\n\\end{align}\n\\]\nNext comes a change of variables in two places. One is substituting \\(b_i = \\frac{1}{N}\\) which is a generalization and will allow us to specify something other than equal risk contributions if desired later. Another is \\(x_i = \\frac{w_i}{\\sqrt{w^T \\Sigma w}}\\), or said another way, \\(x\\) is just the weight vector \\(w\\) normalized by total portfolio standard deviation. After this change of variables, the last equation now looks like this:\n\\[\n\\begin{align}\n    \\frac{w_i(\\Sigma w)_i}{w^T \\Sigma w} &= \\frac{1}{N} \\\\\n    \\frac{w_i}{\\sqrt{w^T \\Sigma w}} \\frac{(\\Sigma w)_i}{\\sqrt{w^T \\Sigma w}} &= b_i \\\\\n    x_i (\\Sigma x)_i &= b_i \\\\\n    x_i (\\Sigma x)_i - b_i &= 0\n\\end{align}\n\\]\nThis can be written out further to separate \\(x_i\\) from other weights \\(x_j\\) to yield a quadratic equation.\n\\[\n\\begin{align}\n    x_i (\\Sigma x)_i - b_i &= 0 \\\\\n    \\sigma_{ii}^2 x_i^2 + x_i\\sum_{i=/=j}\\sigma_{ij}^2 x_j - b_i &= 0\n\\end{align}\n\\]\nThen the solution \\(x_i^*\\) comes from the quadratic formula \\(x^* = \\frac{-b + \\sqrt{b^2 - 4ac}}{2a}\\)\n\\[\n    x_i^* = \\frac{-\\sum_{i=/=j}\\sigma_{ij}^2 x_j + \\sqrt{(\\sum_{i=/=j}\\sigma_{ij}^2 x_j)^2 + 4 \\sigma_{ii}^2 b_i}}{2\\sigma_{ii}^2}\n\\]\nSince each weight \\(x_i\\) is written in terms of the others \\(x_j\\) we iteratively update the weights until convergence is reached.\n\n\nImplementation\n\n# write it as a function that takes a covariance matrix as input\nrpp &lt;- function(cov_mat, b = NULL, tolerance = 0.00000001, max_iter = 100){\n    # initialize weights\n    # as equal weights normalized by total portfolio risk\n    x = rep(1, ncol(cov_mat))\n    x = x/sqrt(sum(cov_mat))\n    sigma_x = cov_mat %*% x\n    # use equal risk weights by default\n    if(is.null(b)){\n        b = rep(1, ncol(cov_mat))\n        b = b/sum(b)\n    }\n    # update weights\n    for(n in 1:max_iter){\n        for(i in 1:nrow(cov_mat)){\n            z = sigma_x[i] - cov_mat[i,i]*x[i]\n            x_star = (-z + sqrt(z^2 + 4*cov_mat[i,i]*b[i])) / (2*cov_mat[i,i])\n            x[i] = x_star\n            sigma_x = cov_mat %*% x\n        }\n        # check for convergence, stop if converged\n        if(max(abs(x * as.vector(cov_mat %*% x) - b)) &lt; tolerance){\n            break\n        }\n    }\n    return(x/sum(x))\n}\n\n# get RPP weights\nw3 = rpp(cov_mat)\nw3\n\n[1] 0.20723268 0.19614619 0.30177870 0.14234206 0.08513825 0.06736213\n\n# check that the weights result in equal risk contributions\nw3 * as.vector(cov_mat %*% w3) / sqrt(as.numeric(w3 %*% cov_mat %*% w3))\n\n[1] 0.001235473 0.001235473 0.001235473 0.001235473 0.001235473 0.001235473"
  },
  {
    "objectID": "projects/risk_parity_portfolio/risk_parity_portfolio.html#performance",
    "href": "projects/risk_parity_portfolio/risk_parity_portfolio.html#performance",
    "title": "Risk Parity Portfolio",
    "section": "Performance",
    "text": "Performance\nNow we can compare these three methods and see how they differ in terms of returns and how the weights change over time. For simplicity I will assume that re-balancing happens daily with no transaction costs, and that the weights are calculated at the start of each month by looking backwards at the prior 3-month period.\n\n# pull out month as a feature\ndf_returns$month = as.integer(substr(df_returns$Date, 6, 7))\n\n# for each month get weights based on previous 3 months\ndf_backtest = data.frame()\ndf_w = data.frame()\nfor(i in 4:12){\n    # previous 3 months of returns\n    df = df_returns[(df_returns$month &gt;= i-3) & (df_returns$month &lt;= i-1),]\n    \n    # equal weights\n    w1 = rep(1, ncol(df)-2)\n    w1 = w1/sum(w1)\n    \n    # minimum variance weights\n    cov_mat = cov(df[,!(colnames(df) %in% c(\"Date\",\"month\"))])\n    cov_mat_inv = solve(cov_mat)\n    vec1 = rep(1, nrow(cov_mat_inv))\n    lambda = vec1 %*% cov_mat_inv %*% vec1\n    lambda = as.numeric(1/lambda)\n    w2 = as.vector(lambda * cov_mat_inv %*% vec1)\n    \n    # risk parity portfolio weights\n    w3 = rpp(cov_mat)\n    \n    # returns for the new month\n    df = df_returns[df_returns$month == i,]\n    dt = df$Date\n    df = df[,!(colnames(df) %in% c(\"Date\",\"month\"))]\n    r1 = rowSums(sweep(df, 2, w1, FUN=\"*\"))\n    r2 = rowSums(sweep(df, 2, w2, FUN=\"*\"))\n    r3 = rowSums(sweep(df, 2, w3, FUN=\"*\"))\n    rets = data.frame(Date = dt, w_eq = r1, w_min_var = r2, w_rpp = r3)\n    df_backtest = rbind(df_backtest, rets)\n    \n    # store weight vectors over time\n    w_month = data.frame(\n        Date = as.Date(paste0(\"2023-\",formatC(i, width=2, format=\"d\", flag=\"0\"),\"-01\")),\n        type = c(rep(\"eq\", 6), rep(\"min_var\", 6), rep(\"rpp\", 6)),\n        asset = rep(colnames(df), 3),\n        weight = c(w1, w2, w3)\n    )\n    df_w = rbind(df_w, w_month)\n}\n\n# plot of performance\ndf_backtest %&gt;%\n    mutate(Date = as.Date(Date)) %&gt;%\n    mutate(w_eq = cumprod(1 + w_eq) - 1) %&gt;%\n    mutate(w_min_var = cumprod(1 + w_min_var) - 1) %&gt;%\n    mutate(w_rpp = cumprod(1 + w_rpp) - 1) %&gt;%\n    pivot_longer(!Date, names_to = \"portfolio\", values_to = \"cum_ret\") %&gt;%\n    ggplot(aes(x = Date, y = cum_ret, color = portfolio)) +\n    geom_line() +\n    labs(y = \"Cumulative Returns\") +\n    theme(legend.position = \"bottom\")\n\n\n\n# plot of weights over time for the 3 different methods\ndf_w %&gt;%\n    ggplot(aes(x = Date, y = weight, fill = asset)) +\n    geom_bar(position = \"stack\", stat = \"identity\") +\n    facet_grid(type ~ .)"
  },
  {
    "objectID": "projects/risk_parity_portfolio/risk_parity_portfolio.html#conclusion",
    "href": "projects/risk_parity_portfolio/risk_parity_portfolio.html#conclusion",
    "title": "Risk Parity Portfolio",
    "section": "Conclusion",
    "text": "Conclusion\nThe risk parity portfolio represents an interesting way to look at the portfolio allocation problem, focusing on the constraints as the objective. I only scratched the surface of the implementation, as there are other conditions that could also be considered such as box constraints for the weights or adding other conditions to optimize for. The riskParityPortfolio R package has excellent vignettes to explain the material for the interested reader."
  }
]