[
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "California Fire Evacuations\n\n\n\n\n\nA proof of concept for the American Red Cross about making data driven predictions of shelter demand related to California wildfire evacuation orders\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nGenerative Art\n\n\n\n\n\nAn exploration of using R to produce artwork, from static images to animations\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nRisk Parity Portfolio\n\n\n\n\n\nAn exercise in understanding the mechanics of the risk parity portfolio by implementing the algorithm myself\n\n\n\n\n \n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/generative_art/generative_art.html",
    "href": "projects/generative_art/generative_art.html",
    "title": "Generative Art",
    "section": "",
    "text": "I came across a few blog posts about using R to generate art which piqued my interest. For readers unfamiliar with the idea, the aRtsy package is a great reference. It implements a large suite of different algorithms which are used to draw interesting patterns. I was inspired by the idea of using R for art instead of analysis and wanted to try it out myself.\nWhile I love the content in the aRtsy package, one limitation (at least at the time I’m writing this) is that the package only produces static images. My idea for a new contribution in this space would be to make a generative animation."
  },
  {
    "objectID": "projects/generative_art/generative_art.html#introoverview",
    "href": "projects/generative_art/generative_art.html#introoverview",
    "title": "Generative Art",
    "section": "",
    "text": "I came across a few blog posts about using R to generate art which piqued my interest. For readers unfamiliar with the idea, the aRtsy package is a great reference. It implements a large suite of different algorithms which are used to draw interesting patterns. I was inspired by the idea of using R for art instead of analysis and wanted to try it out myself.\nWhile I love the content in the aRtsy package, one limitation (at least at the time I’m writing this) is that the package only produces static images. My idea for a new contribution in this space would be to make a generative animation."
  },
  {
    "objectID": "projects/generative_art/generative_art.html#setting-up",
    "href": "projects/generative_art/generative_art.html#setting-up",
    "title": "Generative Art",
    "section": "Setting Up",
    "text": "Setting Up\n\nLoading Libraries\n\nlibrary(aRtsy) # to print out a few examples I like\nlibrary(gridExtra) # to arrange multiple images in one plot\nlibrary(ggplot2) # to draw my own pictures\nlibrary(gganimate) # to turn multiple static images into an animation"
  },
  {
    "objectID": "projects/generative_art/generative_art.html#inspiration-from-the-artsy-package",
    "href": "projects/generative_art/generative_art.html#inspiration-from-the-artsy-package",
    "title": "Generative Art",
    "section": "Inspiration from the aRtsy package",
    "text": "Inspiration from the aRtsy package\nI’ll start by printing out a few examples using the basic functions of the aRtsy package. This is partly to showcase how easy it is for a user to play with, and partly to have an excuse to add nice artwork to my site. While it is easy to colorPalette() function provided by the aRtsy package to pick colors at random, making your own color palette is just a matter of picking a few colors you like, so that’s what I’ll do here. I just googled “hex color picker” and found this site which helped me identify the hex values for a few colors I liked.\n\nmy_color_palette = c(\"#666699\", \"#a894d1\", \"#ab7ab8\", \"#406abf\", \"#1f42ad\", \"#3d0099\", \"#800040\")\n\nset.seed(5813)\np1 &lt;- canvas_collatz(colors = my_color_palette, background = \"#000000\")\nset.seed(1818)\np2 &lt;- canvas_ribbons(colors = my_color_palette, background = \"#000000\", triangle = FALSE)\nset.seed(4497)\np3 &lt;- canvas_flame(colors = my_color_palette, background = \"#000000\")\nset.seed(9819)\np4 &lt;- canvas_flow(colors = my_color_palette, background = \"#000000\")\n\ngrid.arrange(p1, p2, p3, p4, ncol = 2)\n\n\n\n\nOne of the most convenient aspects of the way these functions are designed is that they require very little input from the user to generate new random pictures. All it takes to generate a new image is to set a different seed. I will try to follow the same approach, designing a function to create a specific type of image and using a random seed to drive the contents generated for the images."
  },
  {
    "objectID": "projects/generative_art/generative_art.html#initial-concept",
    "href": "projects/generative_art/generative_art.html#initial-concept",
    "title": "Generative Art",
    "section": "Initial Concept",
    "text": "Initial Concept\nThe functions from the aRtsy package are each based on a specific algorithm, the output from which is transformed in such a way that it generates interesting images. I am not sure how to do that, so I will have to work backwards and start with an idea of what kind of images I want to generate, and then create an algorithm that accomplishes that task.\nSimilar to aRtsy, I will rely on the ggplot2 package to draw images. This means I can then use the gganimate package (which works well with ggplot2) to turn multiple images into an animation. The gganimate package creates transitions for the different shapes that ggplot2 can draw. That means if I can represent my desired animation as multiple frames, or sequential plots, then the gganimate package can blend those frames together into an animation.\n\ndf &lt;- data.frame(x = 1:9, y = 1:9, id = 1:9, t = 1:9)\nplot_list &lt;- lapply(1:9, function(x){\n    df_subset = df[df$t == x,]\n    p = ggplot(df_subset, aes(x = x, y = y)) +\n        geom_point(color = \"#406abf\") +\n        scale_x_continuous(limits = c(0, 10)) +\n        scale_y_continuous(limits = c(0, 10))\n    return(p)\n})\n\ndo.call(\"grid.arrange\", c(plot_list, ncol = 3))\n\n\n\n\n\np = ggplot(df, aes(x = x, y = y, group = id)) +\n    geom_point(color = \"#406abf\") +\n    scale_x_continuous(limits = c(0, 10)) +\n    scale_y_continuous(limits = c(0, 10)) +\n    transition_time(t) +\n    ease_aes(\"linear\")\n\nanimate(p, nframes = 90, fps = 30, width = 400, height = 400, units = \"px\")\n\n\n\n\nI will choose to use points because they are simple to work with and generating a lot of them at random might produce an interesting animation. In order to have something more interesting than just points moving around in the plot, I will introduce a polygon in the center. Randomness can enter this animation in a few ways, such as the number of sides this polygon has and its rotation, the positions of the points that will move through the plot, and the color scheme."
  },
  {
    "objectID": "projects/generative_art/generative_art.html#designing-the-algorithm",
    "href": "projects/generative_art/generative_art.html#designing-the-algorithm",
    "title": "Generative Art",
    "section": "Designing the Algorithm",
    "text": "Designing the Algorithm\nSince the intention is to incorporate randomness, I start by setting the seed so that the steps I am trying out can be reproduced for testing if necessary.\n\nset.seed(1)\n\nThere are a few pieces of the concept to design around which include the polygon, the points, and the transitions.\n\nThe Polygon\nAdding a polygon to the plot is probably most interesting if it takes up a lot of space and is at the center of the plot. If it is small and off to one side it won’t have much impact. My first idea is to start with the unit circle. Pick some number of points on the edge of the unit circle and then if you connect those points you should wind up with a shape inscribed in the unit circle. Randomness can be introduced by randomly picking how many sides this polygon should have. It should be an integer number which is at least 3 so that it has positive area, and no more than 10 because adding more and more sides will just make the polygons closer and closer to an actual circle which will mean less variation in the shapes which is less interesting.\nI start by picking the first point at random, which is just a random angle at which the point will exist on the unit circle. Then I randomly pick the number of sides the polygon will have. With those two pieces of information, the locations of the other points can be generated by taking equally spaced steps along the unit circle where the step size is based on the number of sides desired.\n\ntheta = 2*pi*runif(1)\nn_sides = sample(3:10, 1)\n\n# initialize all points as the same starting point\ndf_shape = data.frame(\n    x = rep(cos(theta), n_sides),\n    y = rep(sin(theta), n_sides)\n)\n\n# apply rotation for each point sequentially\nfor(i in 2:n_sides){\n    df_shape$x[i] = df_shape$x[i-1] * cos(2*pi/n_sides) - df_shape$y[i-1] * sin(2*pi/n_sides)\n    df_shape$y[i] = df_shape$x[i-1] * sin(2*pi/n_sides) + df_shape$y[i-1] * cos(2*pi/n_sides)\n}\n\nggplot() +\ngeom_polygon(aes(x = x, y = y), data = df_shape, fill = \"#406abf\")\n\n\n\n\n\n\nThe Points\nNext comes the points. The points will have their positions and velocities randomly generated. The time at which they appear in the plot will also be random. I will start with 1000 points and see how well that works, it can be be updated easily later if necessary. They will move from the top left of the plot towards the bottom right covering about 5% of the distance per frame, plus or minus some random normal noise.\n\nn_obs = 1000\ndf_pts = data.frame(\n    id = 1:n_obs,\n    t = sample.int(400, size = n_obs, replace = TRUE),\n    x = 0,\n    y = 2.8*runif(n_obs) - 1.4,\n    v_x =  0.1 + 0.02*rnorm(n_obs),\n    v_y = -0.1 + 0.02*rnorm(n_obs)\n)\n\n\n\nCalculating Intersections\nSince there are both points and a big polygon in the image, an obvious thing to do to make it more interesting is to change the behavior based on whether or not they overlap. This means figuring out when they are overlapping and when they are not.\nOne way to think about this is that each point has an x and y velocity, so it is traveling in a straight path, or a line. Similarly, the polygon in the center has a random number of sides between 3 and 10, and if you think of those sides as continuing beyond the edges of the polygon then they are just lines. So we can figure out where the point enters and exits the polygon by calculating where these lines intersect.\nIf we have two lines represented by an equation like \\(y = mx + b\\) then the intersection will be where the x values are equal and the y values are also equal. We set \\(y_1 = y_2\\) and \\(x_1 = x_2\\) and solve for \\(x\\) and \\(y\\).\n\\[\n\\begin{aligned}\n  y_1 - m_1x_1 - b_1 &= y_2 - m_2x_2 - b_2 \\\\\n  y - m_1x - b_1 &= y - m_2x - b_2 \\\\\n  (m_2 - m_1)x &= b_1 - b_2 \\\\\n  x &= (b_1 - b_2)/(m_2 - m_1)\n\\end{aligned}\n\\]\nThis gives the x-coordinate of the intersection point, which can be plugged back into the original equation for the line \\(y_1 = m_1x_1 + b_1\\) to get the y-coordinate.\nIf the polygon has 10 sides, there will now be 10 intersection points, one intersection of each point’s trajectory with each of the 10 lines. But the point should only pass through the polygon in 2 places. This is because the polygon is not actually 10 lines but 10 line segments, so many of the intersections that were just calculated lie outside the bounds of the polygon shape. These incorrect intersection points can be removed by adding a filter to condition on points that lie inside the unit circle.\n\\[\nx^2 + y^2 &lt;= r^2 \\hspace{35pt} (r = 1)\n\\]\nSince the polygon was originally designed to be inscribed within the unit circle, that constraint can be used here to validate which intersection points are the correct ones. Furthermore, I can identify which of the two valid intersection points is the entry vs exit of point’s path through the polygon based on which has the lower x-value, since the velocity in the x direction is positive.\n\n# add the first row of the polygon data frame to the end\n# to make it convenient to calculate parameters for polygon lines\ndf_shape_loop = df_shape\ndf_shape_loop = rbind(df_shape_loop, df_shape[1,])\n\n# initialize data frame to hold candidate intersection point\n# x-coordinates, will add a new column for each intersection\ndf_x_intersect = rep(NA, nrow(df_pts))\n\n# parameters for the lines from the points\n# can be calculated outside the loop\n# m1 and b1 are both vectors\nm1 = df_pts$v_y / df_pts$v_x\nb1 = df_pts$y - m1*df_pts$x\n\n# calculate one intersection for each line (segment) from the polygon\nfor(i in 2:nrow(df_shape_loop)){\n    # m2 and b2 are scalars here\n    m2 = (df_shape_loop$y[i] - df_shape_loop$y[i-1]) / (df_shape_loop$x[i] - df_shape_loop$x[i-1])\n    b2 = df_shape_loop$y[i] - m2*df_shape_loop$x[i]\n    x_intersect = (b1 - b2)/(m2 - m1)\n    df_x_intersect = cbind(df_x_intersect, x_intersect)\n}\n\n# calculate corresponding y-coordinate for each intersection\ndf_y_intersect = sweep(df_x_intersect, 1, m1, FUN=\"*\")\ndf_y_intersect = sweep(df_y_intersect, 1, b1, FUN=\"+\")\n\n# check which 2 out of the 3+ intersections are valid\n# all valid intersection must lie within the unit circle\ncheck_valid = (sqrt(df_x_intersect^2 + df_y_intersect^2) &lt;= 1)\ncheck_valid[check_valid == 0] = NA\n\n# replace invalid intersection points with NA\ndf_x_intersect = df_x_intersect*check_valid\n\n# calculate entry and exit points\nx_entry = apply(df_x_intersect, 1, min, na.rm = TRUE)\nx_entry = ifelse(is.finite(x_entry), x_entry, NA)\nx_exit  = apply(df_x_intersect, 1, max, na.rm = TRUE)\nx_exit  = ifelse(is.finite(x_exit), x_exit, NA)\ny_entry = m1*x_entry + b1\ny_exit  = m1*x_exit + b1\n\n# consolidate entry and exit points into a data frame\ndf_entry_exit = as.data.frame(cbind(x_entry, y_entry, x_exit, y_exit))\n# cut speed of point in half inside the polygon\ndf_entry_exit$n_frames = ceiling((df_entry_exit$x_exit - df_entry_exit$x_entry)/(df_pts$v_x/2))\ndf_entry_exit$v_x = (df_entry_exit$x_exit - df_entry_exit$x_entry)/df_entry_exit$n_frames\ndf_entry_exit$v_y = (df_entry_exit$y_exit - df_entry_exit$y_entry)/df_entry_exit$n_frames\n\nNot all of the randomly generated points will actually come into contact with the polygon. This will require handling separately. If a point will never come into contact with the polygon, it will stay unchanged and be flagged as not in contact. If a point does contact the polygon at some point, I will change it’s initial value to be the point where it first contacts the polygon and flag it as in contact.\n\ndf_pts$x = ifelse(is.na(df_entry_exit$x_entry), 0, df_entry_exit$x_entry)\ndf_pts$y = ifelse(is.na(df_entry_exit$y_entry), df_pts$y, df_entry_exit$y_entry)\ndf_pts$contact = ifelse(is.na(df_entry_exit$x_entry), 0, 1)\n\n\n\nCalculating Movement\nNow that all the points have been initialized I need to calculate how their positions will change over time. As the time step increases by one, the x and y positions should update based on the v_x and v_y velocity parameters. Since points will start either in the center of the plot or where they first make contact with the polygon, I will work out their motion backwards from their initialized location. I can then handle motion while in contact with the polygon as a separate case, and then motion forwards again after exiting the polygon.\nThe first thing to do is to save the initialized state in a new variable so it can be used again later. After that, the data frame that contains the points will be updated to add their positions moving backwards in time.\n\n# save initialized state\ndf_init = df_pts\n\n# work backwards 15 frames\n# to get position of points before contact\n# with polygon or center of canvas\ndf_backwards = df_init\ndf_backwards$contact = 0\n\nfor(i in 1:15){\n    df_backwards$t = df_backwards$t - 1\n    df_backwards$x = df_backwards$x - df_backwards$v_x\n    df_backwards$y = df_backwards$y - df_backwards$v_y\n    # append to points data\n    df_pts = rbind(df_pts, df_backwards)\n}\n\nNext comes movement within the polygon. Not all point come into contact with the polygon, so this should only be done for the points where contact occurs.\n\n# get the subset of initialized points which\n# make contact with the polygon\ndf_within = df_init\ndf_within$v_x = df_entry_exit$v_x\ndf_within$v_y = df_entry_exit$v_y\ndf_within$n_frames = df_entry_exit$n_frames\ndf_within = df_within[df_within$contact == 1,]\n\n# add one frame at contact position\n# basically, the point will appear to pause\n# when it hits the polygon\ndf_within$t = df_within$t + 1\ndf_pts = rbind(df_pts, df_within[,-which(colnames(df_within)==\"n_frames\")])\n\n# different points will spend different amounts of time\n# traversing the polygon, so only keep iterating\n# for the points that have not reached the other side yet\nfor(i in 1:max(df_within$n_frames)){\n    df_within = df_within[df_within$n_frames &gt;= i,]\n    df_within$t = df_within$t + 1\n    df_within$x = df_within$x + df_within$v_x\n    df_within$y = df_within$y + df_within$v_y\n    # append to points data\n    df_pts = rbind(df_pts, df_within[,-which(colnames(df_within)==\"n_frames\")])\n}\n\nFinally there is the forwards movement either from the center of the plot to the right or from exiting the polygon to the right.\n\n# reset locations for points that contact the polygon\n# to be at the location where they exit, and also add 2\n# to their time index to create a delay on exit\ndf_forwards = df_init\ndf_forwards$x = ifelse(is.na(df_entry_exit$x_exit), df_init$x, df_entry_exit$x_exit)\ndf_forwards$y = ifelse(is.na(df_entry_exit$y_exit), df_init$y, df_entry_exit$y_exit)\ndf_forwards$t = ifelse(is.na(df_entry_exit$n_frames), df_forwards$t, df_forwards$t + df_entry_exit$n_frames + 2)\ndf_forwards$contact = 0\n\n# work forwards 15 frames past exit from polygon\n# to get position of points as they exit the canvas\nfor(i in 1:15){\n    df_forwards$t = df_forwards$t + 1\n    df_forwards$x = df_forwards$x + df_forwards$v_x\n    df_forwards$y = df_forwards$y + df_forwards$v_y\n    # append to points data\n    df_pts = rbind(df_pts, df_forwards)\n}\n\n\n\nCleanup\nWhile calculating point positions, I picked a number of time steps to make sure that motion within the bounds of the plot is being captured. If the plot x-axis covers [-1.25, 1.25] and average velocity in the x direction is 0.1 then it should take around (1.2 + 1.2)/0.1 = 24 time steps to get from side of the plot to the other, or 12 steps backwards from the center of the plot, and 12 steps forwards. In previous sections I calculated 15 time steps in each direction to be conservative due to the randomness added to the point velocities. In addition to this, motion within the polygon is handled separately. There will also be cases where points only pass through small sections of the plot such as at the top right or bottom left. This means there are many point locations that have been calculated which are outside the bounds of the plot and should be thrown away.\n\ndf_pts = df_pts[(df_pts$x &gt;= -1.2) & (df_pts$x &lt;= 1.2),]\ndf_pts = df_pts[(df_pts$y &gt;= -1.2) & (df_pts$y &lt;= 1.2),]"
  },
  {
    "objectID": "projects/generative_art/generative_art.html#first-animation",
    "href": "projects/generative_art/generative_art.html#first-animation",
    "title": "Generative Art",
    "section": "First Animation",
    "text": "First Animation\nAll the pieces are in place. Now it’s time to combine all these time and position values into an animation. Here’s a first pass at fitting it all together.\n\nmargin = 0\np = ggplot() +\n    geom_polygon(aes(x = x, y = y), data = df_shape, fill = my_color_palette[4]) +\n    geom_point(aes(x = x, y = y, group = id, color = as.factor(contact)), data = df_pts) +\n    scale_color_manual(values = my_color_palette[1:2]) +\n    scale_x_continuous(limits = c(-1.2, 1.2)) +\n    scale_y_continuous(limits = c(-1.2, 1.2)) +\n    theme(\n        axis.title = ggplot2::element_blank(),\n        axis.text = ggplot2::element_blank(),\n        axis.ticks = ggplot2::element_blank(),\n        axis.line = ggplot2::element_blank(),\n        legend.position = \"none\",\n        plot.background = ggplot2::element_rect(fill = \"#000000\", colour = \"#000000\"),\n        panel.border = ggplot2::element_blank(),\n        panel.grid = ggplot2::element_blank(),\n        plot.margin = ggplot2::unit(rep(margin, 4), \"lines\"),\n        strip.background = ggplot2::element_blank(),\n        strip.text = ggplot2::element_blank(),\n        panel.background = ggplot2::element_blank()\n    ) +\n    transition_time(t) +\n    ease_aes(\"linear\")\n\nanimate(p, nframes = 1500, fps = 30, width = 400, height = 400, units = \"px\")\n\n\n\n\n\nA Smooth Loop\nThis looks pretty good, but there are clear points where the animation is beginning and ending. One more change could be made here to make the animation loop more smoothly or seamlessly.\n\n# take points from just after the start of the animation\n# and use them to replace points towards the end\npts_start  = unique(df_pts$id[df_pts$t == 51])\npts_remove = unique(df_pts$id[df_pts$t &gt;= 350])\ndf_loop    = df_pts[df_pts$id %in% pts_start,]\ndf_loop$t  = df_loop$t + 299\ndf_loop$id = df_loop$id + n_obs\ndf_pts     = df_pts[!(df_pts$id %in% pts_remove),]\ndf_pts     = rbind(df_pts, df_loop)\n\n# limit data shown to the window that starts and ends\n# with the copied points\ndf_plot = df_pts[(df_pts$t &gt;= 51) & (df_pts$t &lt;= 350),]\n\nNow the animation should look like a continuous loop with no obvious starting or stopping point.\n\nmargin = 0\np = ggplot() +\n    geom_polygon(aes(x = x, y = y), data = df_shape, fill = my_color_palette[4]) +\n    geom_point(aes(x = x, y = y, group = id, color = as.factor(contact)), data = df_plot) +\n    scale_color_manual(values = my_color_palette[1:2]) +\n    scale_x_continuous(limits = c(-1.2, 1.2)) +\n    scale_y_continuous(limits = c(-1.2, 1.2)) +\n    theme(\n        axis.title = ggplot2::element_blank(),\n        axis.text = ggplot2::element_blank(),\n        axis.ticks = ggplot2::element_blank(),\n        axis.line = ggplot2::element_blank(),\n        legend.position = \"none\",\n        plot.background = ggplot2::element_rect(fill = \"#000000\", colour = \"#000000\"),\n        panel.border = ggplot2::element_blank(),\n        panel.grid = ggplot2::element_blank(),\n        plot.margin = ggplot2::unit(rep(margin, 4), \"lines\"),\n        strip.background = ggplot2::element_blank(),\n        strip.text = ggplot2::element_blank(),\n        panel.background = ggplot2::element_blank()\n    ) +\n    transition_time(t) +\n    ease_aes(\"linear\")\n\nanimate(p, nframes = 1500, fps = 30, width = 400, height = 400, units = \"px\")"
  },
  {
    "objectID": "projects/generative_art/generative_art.html#final-function",
    "href": "projects/generative_art/generative_art.html#final-function",
    "title": "Generative Art",
    "section": "Final Function",
    "text": "Final Function\nThe last piece of the puzzle is to wrap all of this up into one function so that it can be used the same way as the functions in the aRtsy package. It will take in 3 optional inputs. One is the seed, which can be set outside the function like in aRtsy, or can be passed as an argument. Another is a color palette, if the user doesn’t provide one that is fine, it should just pick colors randomly. Last is a background color, with similar behavior where it is picked randomly if not specified.\n\ndraw_polygon_animation = function(seed = NULL, color_palette = NULL, background = NULL){\n    # set seed here if it was passed as an argument\n    if(!is.null(seed)){ set.seed(seed) }\n    \n    # if color palette is given use that\n    # otherwise pick randomly from default colors\n    color_vec = grDevices::colors()[grep('gr(a|e)y', grDevices::colors(), invert = T)]\n    if(!is.null(color_palette)){ color_vec = color_palette }\n    \n    # if background color is provided use it as the first color\n    # otherwise pick randomly\n    colors4 = sample(color_vec, 4)\n    if(!is.null(background)){ colors4[1] = background }\n    \n    # picking start rotation and sides for polygon randomly\n    theta = 2*pi*runif(1)\n    n_sides = sample(3:10, 1)\n\n    # initialize all polygon vertices as the same starting point\n    df_shape = data.frame(\n        x = rep(cos(theta), n_sides),\n        y = rep(sin(theta), n_sides)\n    )\n\n    # apply rotation for each point sequentially\n    for(i in 2:n_sides){\n        df_shape$x[i] = df_shape$x[i-1] * cos(2*pi/n_sides) - df_shape$y[i-1] * sin(2*pi/n_sides)\n        df_shape$y[i] = df_shape$x[i-1] * sin(2*pi/n_sides) + df_shape$y[i-1] * cos(2*pi/n_sides)\n    }\n    \n    # initialize points\n    n_obs = 1000\n    df_pts = data.frame(\n        id = 1:n_obs,\n        t = sample.int(400, size = n_obs, replace = TRUE),\n        x = 0,\n        y = 2.8*runif(n_obs) - 1.4,\n        v_x =  0.1 + 0.02*rnorm(n_obs),\n        v_y = -0.1 + 0.02*rnorm(n_obs)\n    )\n    \n    # add the first row of the polygon data frame to the end\n    # to make it convenient to calculate parameters for polygon lines\n    df_shape_loop = df_shape\n    df_shape_loop = rbind(df_shape_loop, df_shape[1,])\n    \n    # initialize data frame to hold candidate intersection point\n    # x-coordinates, will add a new column for each intersection\n    df_x_intersect = rep(NA, nrow(df_pts))\n    \n    # parameters for the lines from the points\n    # can be calculated outside the loop\n    # m1 and b1 are both vectors\n    m1 = df_pts$v_y / df_pts$v_x\n    b1 = df_pts$y - m1*df_pts$x\n    \n    # calculate one intersection for each line (segment) from the polygon\n    for(i in 2:nrow(df_shape_loop)){\n        # m2 and b2 are scalars here\n        m2 = (df_shape_loop$y[i] - df_shape_loop$y[i-1]) / (df_shape_loop$x[i] - df_shape_loop$x[i-1])\n        b2 = df_shape_loop$y[i] - m2*df_shape_loop$x[i]\n        x_intersect = (b1 - b2)/(m2 - m1)\n        df_x_intersect = cbind(df_x_intersect, x_intersect)\n    }\n    \n    # calculate corresponding y-coordinate for each intersection\n    df_y_intersect = sweep(df_x_intersect, 1, m1, FUN=\"*\")\n    df_y_intersect = sweep(df_y_intersect, 1, b1, FUN=\"+\")\n    \n    # check which 2 out of the 3+ intersections are valid\n    # all valid intersection must lie within the unit circle\n    check_valid = (sqrt(df_x_intersect^2 + df_y_intersect^2) &lt;= 1)\n    check_valid[check_valid == 0] = NA\n    \n    # replace invalid intersection points with NA\n    df_x_intersect = df_x_intersect*check_valid\n    \n    # calculate entry and exit points\n    x_entry = apply(df_x_intersect, 1, min, na.rm = TRUE)\n    x_entry = ifelse(is.finite(x_entry), x_entry, NA)\n    x_exit  = apply(df_x_intersect, 1, max, na.rm = TRUE)\n    x_exit  = ifelse(is.finite(x_exit), x_exit, NA)\n    y_entry = m1*x_entry + b1\n    y_exit  = m1*x_exit + b1\n    \n    # consolidate entry and exit points into a data frame\n    df_entry_exit = as.data.frame(cbind(x_entry, y_entry, x_exit, y_exit))\n    # cut speed of point in half inside the polygon\n    df_entry_exit$n_frames = ceiling((df_entry_exit$x_exit - df_entry_exit$x_entry)/(df_pts$v_x/2))\n    df_entry_exit$v_x = (df_entry_exit$x_exit - df_entry_exit$x_entry)/df_entry_exit$n_frames\n    df_entry_exit$v_y = (df_entry_exit$y_exit - df_entry_exit$y_entry)/df_entry_exit$n_frames\n    \n    # reset starting locations\n    df_pts$x = ifelse(is.na(df_entry_exit$x_entry), 0, df_entry_exit$x_entry)\n    df_pts$y = ifelse(is.na(df_entry_exit$y_entry), df_pts$y, df_entry_exit$y_entry)\n    df_pts$contact = ifelse(is.na(df_entry_exit$x_entry), 0, 1)\n    \n    # save initialized state\n    df_init = df_pts\n    \n    # work backwards 15 frames\n    # to get position of points before contact\n    # with polygon or center of canvas\n    df_backwards = df_init\n    df_backwards$contact = 0\n    \n    for(i in 1:15){\n        df_backwards$t = df_backwards$t - 1\n        df_backwards$x = df_backwards$x - df_backwards$v_x\n        df_backwards$y = df_backwards$y - df_backwards$v_y\n        # append to points data\n        df_pts = rbind(df_pts, df_backwards)\n    }\n    \n    # get the subset of initialized points which\n    # make contact with the polygon\n    df_within = df_init\n    df_within$v_x = df_entry_exit$v_x\n    df_within$v_y = df_entry_exit$v_y\n    df_within$n_frames = df_entry_exit$n_frames\n    df_within = df_within[df_within$contact == 1,]\n    \n    # add one frame at contact position\n    # basically, the point will appear to pause\n    # when it hits the polygon\n    df_within$t = df_within$t + 1\n    df_pts = rbind(df_pts, df_within[,-which(colnames(df_within)==\"n_frames\")])\n    \n    # different points will spend different amounts of time\n    # traversing the polygon, so only keep iterating\n    # for the points that have not reached the other side yet\n    for(i in 1:max(df_within$n_frames)){\n        df_within = df_within[df_within$n_frames &gt;= i,]\n        df_within$t = df_within$t + 1\n        df_within$x = df_within$x + df_within$v_x\n        df_within$y = df_within$y + df_within$v_y\n        # append to points data\n        df_pts = rbind(df_pts, df_within[,-which(colnames(df_within)==\"n_frames\")])\n    }\n    \n    # reset locations for points that contact the polygon\n    # to be at the location where they exit, and also add 2\n    # to their time index to create a delay on exit\n    df_forwards = df_init\n    df_forwards$x = ifelse(is.na(df_entry_exit$x_exit), df_init$x, df_entry_exit$x_exit)\n    df_forwards$y = ifelse(is.na(df_entry_exit$y_exit), df_init$y, df_entry_exit$y_exit)\n    df_forwards$t = ifelse(is.na(df_entry_exit$n_frames), df_forwards$t, df_forwards$t + df_entry_exit$n_frames + 2)\n    df_forwards$contact = 0\n    \n    # work forwards 15 frames past exit from polygon\n    # to get position of points as they exit the canvas\n    for(i in 1:15){\n        df_forwards$t = df_forwards$t + 1\n        df_forwards$x = df_forwards$x + df_forwards$v_x\n        df_forwards$y = df_forwards$y + df_forwards$v_y\n        # append to points data\n        df_pts = rbind(df_pts, df_forwards)\n    }\n    \n    # remove points located outside of plot window\n    df_pts = df_pts[(df_pts$x &gt;= -1.2) & (df_pts$x &lt;= 1.2),]\n    df_pts = df_pts[(df_pts$y &gt;= -1.2) & (df_pts$y &lt;= 1.2),]\n    \n    # make the loop continuous\n    # take points from just after the start of the animation\n    # and use them to replace points towards the end\n    pts_start  = unique(df_pts$id[df_pts$t == 51])\n    pts_remove = unique(df_pts$id[df_pts$t &gt;= 350])\n    df_loop    = df_pts[df_pts$id %in% pts_start,]\n    df_loop$t  = df_loop$t + 299\n    df_loop$id = df_loop$id + n_obs\n    df_pts     = df_pts[!(df_pts$id %in% pts_remove),]\n    df_pts     = rbind(df_pts, df_loop)\n    \n    # limit data shown to the window that starts and ends\n    # with the copied points\n    df_plot = df_pts[(df_pts$t &gt;= 51) & (df_pts$t &lt;= 350),]\n    \n    # animation\n    margin = 0\n    p = ggplot() +\n        geom_polygon(aes(x = x, y = y), data = df_shape, fill = colors4[2]) +\n        geom_point(aes(x = x, y = y, group = id, color = as.factor(contact)), data = df_plot) +\n        scale_color_manual(values = colors4[3:4]) +\n        scale_x_continuous(limits = c(-1.2, 1.2)) +\n        scale_y_continuous(limits = c(-1.2, 1.2)) +\n        theme(\n            axis.title = ggplot2::element_blank(),\n            axis.text = ggplot2::element_blank(),\n            axis.ticks = ggplot2::element_blank(),\n            axis.line = ggplot2::element_blank(),\n            legend.position = \"none\",\n            plot.background = ggplot2::element_rect(fill = colors4[1], color = colors4[1]),\n            panel.border = ggplot2::element_blank(),\n            panel.grid = ggplot2::element_blank(),\n            plot.margin = ggplot2::unit(rep(margin, 4), \"lines\"),\n            strip.background = ggplot2::element_blank(),\n            strip.text = ggplot2::element_blank(),\n            panel.background = ggplot2::element_blank()\n        ) +\n        transition_time(t) +\n        ease_aes(\"linear\")\n    \n    animate(p, nframes = 1500, fps = 30, width = 400, height = 400, units = \"px\")\n}"
  },
  {
    "objectID": "projects/generative_art/generative_art.html#conclusion",
    "href": "projects/generative_art/generative_art.html#conclusion",
    "title": "Generative Art",
    "section": "Conclusion",
    "text": "Conclusion\nThis was a fun creative exercise exploring a topic very different from the data analysis I normally use R for. Despite being about making art, it was still a very technical exercise and the concept I came up with used a lot of basic algebra and geometry concepts. Hopefully aspects of this will prove useful in future endeavors.\n\ndraw_polygon_animation(seed = 2172, color_palette = my_color_palette, background = \"#000000\")"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Nicholas Burk",
    "section": "",
    "text": "n = 20 // number of layers \nm = 200 // number of samples per layer\nk = 10 // number of bumps per layer\n\nbumps = {\n  // Inspired by Lee Byron’s test data generator.\n  function bump(a, n) {\n    const x = 1 / (0.1 + Math.random());\n    const y = 2 * Math.random() - 0.5;\n    const z = 10 / (0.1 + Math.random());\n    for (let i = 0; i &lt; n; ++i) {\n      const w = (i / n - y) * z;\n      a[i] += x * Math.exp(-w * w);\n    }\n  }\n  return function bumps(n, m) {\n    const a = [];\n    for (let i = 0; i &lt; n; ++i) a[i] = 0;\n    for (let i = 0; i &lt; m; ++i) bump(a, n);\n    return a;\n  };\n}\n\nchart = {\n  //const width = 928;\n  const width = 1200;\n  const height = 500;\n\n  const x = d3.scaleLinear([0, m - 1], [0, width]);\n  const y = d3.scaleLinear([0, 1], [height, 0]);\n  const z = d3.interpolateCool;\n\n  const area = d3.area()\n    .x((d, i) =&gt; x(i))\n    .y0(d =&gt; y(d[0]))\n    .y1(d =&gt; y(d[1]));\n\n  const stack = d3.stack()\n    .keys(d3.range(n))\n    .offset(d3.stackOffsetWiggle)\n    .order(d3.stackOrderNone);\n\n  function randomize() {\n    const layers = stack(d3.transpose(Array.from({length: n}, () =&gt; bumps(m, k))));\n    y.domain([\n      d3.min(layers, l =&gt; d3.min(l, d =&gt; d[0])),\n      d3.max(layers, l =&gt; d3.max(l, d =&gt; d[1]))\n    ]);\n    return layers;\n  }\n  \n  const svg = d3.create(\"svg\")\n      .attr(\"viewBox\", [0, 0, width, height])\n      .attr(\"width\", width)\n      .attr(\"height\", height)\n      .attr(\"style\", \"max-width: 100%; height: auto;\");\n\n  const path = svg.selectAll(\"path\")\n    .data(randomize)\n    .join(\"path\")\n      .attr(\"d\", area)\n      .attr(\"fill\", () =&gt; z(Math.random()));\n\n  while (true) {\n    yield svg.node();\n\n    await path\n      .data(randomize)\n      .transition()\n        .delay(1000)\n        .duration(1500)\n        .attr(\"d\", area)\n      .end();\n  }\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n  \n  \nHi! I’m Nicholas Burk. My background is in math and economics, and I have spent most of my career working in the financial industry in various roles in the Federal Reserve System, Union Bank, and Fannie Mae. My work has ranged from exploratory data analysis to building and testing statistical models. I have also done some data science work for the Amercian Red Cross as a virtual volunteer.\nThe projects section contains posts about some side projects I have worked on unrelated to my job. These are usually short term passion projects related to a topic of interest or volunteer work."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "My background is in math and economics, and I have spent most of my career working in the financial industry doing analytical work ranging from exploratory data analysis to building and testing statistical models."
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About Me",
    "section": "Education",
    "text": "Education\nGeorgia Institute of Technology | M.S. in Analytics\nUniversity of California, San Diego | B.S. in Mathematics, B.S. in Economics"
  },
  {
    "objectID": "about.html#employment",
    "href": "about.html#employment",
    "title": "About Me",
    "section": "Employment",
    "text": "Employment\nFederal Reserve Bank of San Francisco\nReserves Data Specialist | 2019-2023\n\nAssisted in the implementation of monetary policy; operational areas include reserve requirements, interest payments, deficiency charges, TDF operations, etc. Required in-depth knowledge of the functionality of and data flows between a network of internal applications.\nResearched questions from stakeholders to help guide policy decisions. Examples include exploratory analysis of usage of FRB products which guides updates to Regulation D and service offerings, extract data from external sources such as Census and FEMA and combine with internal data for mapping and spatial modeling (tests for spatial dependence, KNN, spatial regression, etc.).\nProduct owner for Reserves business line applications; wrote requirements and user stories and worked with developers to build new functionality, proposed algorithm designs, tested that production implementation matches expectations.\nResearch and development applying statistical and machine learning techniques to ad-hoc problems. Examples include applying spectral analysis to identify cyclical patterns in account activity, LDA text analysis to determine how well business-proposed classifications align with data-driven classifications, simulation to bootstrap confidence intervals and prediction intervals in a variety of cases as well as simulating infrequent business problems to test how well detection algorithms perform, end-to-end model development (logistic regression with and without regularization penalties, random forest, etc.).\n\nMUFG Union Bank\nModel Risk Management Quantitative Analyst | 2017-2019\n\nAs the lead validator on my team, I managed one direct report, participated in the hiring, onboarding, and training for all new employees on the team, and served as project manager for validation projects contracted out to external vendors.\nTested new or updated models to ensure the development process follows appropriate methodology. All testing performed independently; model owner hands over development code and documentation and I use R, Python, to run tests of model assumptions, validate performance, build challenger models, deliver resulting observations.\nResponsible for a wide variety of credit risk models, including rating models, CCAR/DFAST stress testing models, CECL use cases, and more.\nChallenged current models by building alternative models to showcase different methodology and ensure state of the art techniques are considered. For example, building a random forest or neural network model to compare with a logistic regression.\nMonitored performance of existing models on an ongoing basis to identify and mitigate risks around deteriorating model performance. Involved enforcing existing model governance as well as changing governance plans to ensure they are appropriate for the model.\n\nFederal Reserve Bank of San Francisco\nFinancial Institutions Analyst | 2014-2017\n\nCollected and processed data from multiple sources at multiple frequencies, ensuring both timeliness and accuracy of the data. Built models for changepoint detection and structural breaks to make the quality assurance process more dynamic and data-driven.\nDeveloped tools using R, SQL, and MS Office to automate time-intensive processes and aid other analysts in their work. Worked with developers to implement my tools into production for the entire FRS.\nBuilt models for risk identification and exploratory analysis, leveraging external data sources as required to improve performance or yield interesting insights.\n\nBoard of Governors of the Federal Reserve System\nResearch Assistant | 2011-2014\n\nProduced memoranda and briefings to the Board, used in the implementation of monetary policy and in data books and testimony for Board members.\nMaintained and improved systems that collect real-time economic data.\nAssisted Board economists in a variety of projects, most notably the nowcasting project."
  },
  {
    "objectID": "about.html#volunteering",
    "href": "about.html#volunteering",
    "title": "About Me",
    "section": "Volunteering",
    "text": "Volunteering\nAmerican Red Cross\nVirtual Volunteer\n\nConsulted on causal analysis studies for the IFRC anticipatory action programs. Covered topics including study design, exploratory analysis, and communicating quantitative findings with non-technical stakeholders.\nDeveloped proof-of-concept for consistently estimating demand for shelter in relation to fires in California."
  },
  {
    "objectID": "projects/ca_fire_evac/ca_fire_evac.html",
    "href": "projects/ca_fire_evac/ca_fire_evac.html",
    "title": "California Fire Evacuations",
    "section": "",
    "text": "One of the volunteer projects I worked on for the American Red Cross (ARC) was for a chapter in northern California which wanted to improve predictions for shelter demand due to fire-related evacuations. When a wildfire happens, the local authorities may issue an evacuation order to tell people in the affected area to evacuate.\nARC helps by activating and staffing shelters near the affected area so that people who have to flee from the fire have a place to spend the night. These shelters are usually places like stadiums or schools with gymnasiums which have agreements with ARC to use their space as a temporary shelter in emergency situations. A volunteer from ARC will go to the shelter location to make sure it is open and ready to receive people in need.\nARC needs to decide which potential shelter locations to activate for any given evacuation order. There are a lot of factors that go into making this decision, but one of them is an estimate of demand for shelter. How many people will actually want to make use of the shelter that ARC will provide?"
  },
  {
    "objectID": "projects/ca_fire_evac/ca_fire_evac.html#introoverview",
    "href": "projects/ca_fire_evac/ca_fire_evac.html#introoverview",
    "title": "California Fire Evacuations",
    "section": "",
    "text": "One of the volunteer projects I worked on for the American Red Cross (ARC) was for a chapter in northern California which wanted to improve predictions for shelter demand due to fire-related evacuations. When a wildfire happens, the local authorities may issue an evacuation order to tell people in the affected area to evacuate.\nARC helps by activating and staffing shelters near the affected area so that people who have to flee from the fire have a place to spend the night. These shelters are usually places like stadiums or schools with gymnasiums which have agreements with ARC to use their space as a temporary shelter in emergency situations. A volunteer from ARC will go to the shelter location to make sure it is open and ready to receive people in need.\nARC needs to decide which potential shelter locations to activate for any given evacuation order. There are a lot of factors that go into making this decision, but one of them is an estimate of demand for shelter. How many people will actually want to make use of the shelter that ARC will provide?"
  },
  {
    "objectID": "projects/ca_fire_evac/ca_fire_evac.html#setting-up",
    "href": "projects/ca_fire_evac/ca_fire_evac.html#setting-up",
    "title": "California Fire Evacuations",
    "section": "Setting Up",
    "text": "Setting Up\n\nLoading Libraries\n\nlibrary(tigris) # to download CA shapefile from the Census\nlibrary(jsonlite) # to parse responses from the OpenFEMA API\nlibrary(sf) # for manipulating shapefile geometry information\nlibrary(ggplot2) # to make plots\nlibrary(tidycensus) # to get US Census data\nlibrary(dplyr) # for data manipulation\nlibrary(tidyr) # for data manipulation"
  },
  {
    "objectID": "projects/ca_fire_evac/ca_fire_evac.html#getting-the-data",
    "href": "projects/ca_fire_evac/ca_fire_evac.html#getting-the-data",
    "title": "California Fire Evacuations",
    "section": "Getting the Data",
    "text": "Getting the Data\nThe first thing I needed to do is find data related to the problem. For data about evacuation orders related to fires in CA, the OpenFEMA datasets looked like a good option. In particular, the Integrated Public Alert and Warning System (IPAWS) has an API which lets users query historical alert data. This is useful for this problem about CA fires because many counties in CA began adopting IPAWS to distribute their alerts starting around 2013, so it should contain data about the evacuation alerts of interest.\nAnother data source is the US Census. The evacuation order has information about what geographic region is affected, but no information about the people in that region or their demographics. That information will have to come from the census.\n\nGet Data from the OpenFEMA API\nThe IPAWS API lets users query based on certain available fields as listed in their documentation. However, it is a little bit tricky to get what I’m looking for because there is no “state” field to use for filtering so I cannot ask for results where “state=CA”. Instead the user can pass in a geometry in the shape of a polygon as a search area, so it is necessary for me to first represent CA as a polygon in Well Known Text (WKT) format to pass to the API as a filter.\n\n# get CA shapefile from the census\nca = tigris::states(cb = TRUE, resolution = \"20m\")\nca = ca[ca$NAME == \"California\",]\n\n# my approximate guess for longitude and latitude\n# for a polygon covering CA\nca_approx = data.frame(\n    lon = c(-124.2196,-119.9818,-119.9922,-113.9484,-114.3979,-117.2380,-120.1382,-124.5953),\n    lat = c(42.0019,42.0089,39.0026,34.6875,32.6730,32.4818,33.0436,40.3261)\n)\n\n# same as above, just transposed in format\n# to align with what the API expects\n# POLYGON((-124.2196 42.0019,-119.9818 42.0089,-119.9922 39.0026,-113.9498 34.6875,-114.3979 32.6730,-117.2380 32.4818,-120.1382 33.0436,-124.5953 40.3261))\n\n# verify coverage with plot\nggplot() +\n    geom_sf(data = ca) +\n    geom_polygon(aes(x = lon, y = lat), data = ca_approx,\n                 color = \"#F8766D\", fill = \"#F8766D\", alpha = 0.3) +\n    theme(axis.title = element_blank())\n\n\nMy simple polygon covers California as desired. It is okay that it covers some space beyond California because I am only using it to filter results from IPAWS. It may pick up a few cases outside of California, but these will be eliminated later when I merge the IPAWS data with the US Census data since they will not match up with any CA census geometries.\nNow I can start making requests from the IPAWS API. I will skip all the trial and error I had to go through to get it to work and just show the final code that worked for me. Comments below in the relevant sections discuss some of these decisions.\n\n# request to OpenFEMA API\n# give back status='Actual\" (as opposed to test alerts)\n# and state=CA (geo polygon information interects my approximated CA polygon)\napi_query_base = \"https://www.fema.gov/api/open/v1/IpawsArchivedAlerts?$filter=status eq 'Actual' and geo.intersects(searchGeometry, geography 'POLYGON((-124.2196 42.0019,-119.9818 42.0089,-119.9922 39.0026,-113.9498 34.6875,-114.3979 32.6730,-117.2380 32.4818,-120.1382 33.0436,-124.5953 40.3261))') and sent gt 'yyyy-01-01T00:00:01.000z' and sent lt 'yyyy-12-31T23:59:59.000z'&$orderby=sent&$inlinecount=allpages&$skip=\"\n\n# OpenFEMA API returns 1000 records max\n# so need to page through sets of 1000\n# https://www.fema.gov/about/openfema/api\nresults_df = data.frame()\nresults_poly = list()\n\n# IPAWS data is available starting in June 2012\n# I am querying one year of data at a time\n# because I keep running into errors where the API\n# doesn't want to return results when the page count\n# gets too high\nfor(year in 2012:2020){\n    \n    # set up counters to page through data\n    last_page = FALSE\n    skip_records = 0\n    \n    while(!last_page){\n        # query API for up to 1000 records\n        api_query = paste0(api_query_base, skip_records)\n        api_query = gsub(\"yyyy\", year, api_query)\n        json_result = readLines(api_query)\n        parsed_list = fromJSON(json_result)\n        n_records = nrow(parsed_list[[2]])\n        \n        # is this the last page of results?\n        # if so end loop\n        if(n_records &lt; 1000){ last_page = TRUE }\n        \n        # convert results to single data frame\n        # plus list of polygons\n        for(i in 1:n_records){\n            # get metadata for each alert\n            df = data.frame(\n                sent = parsed_list[[2]][i,]$sent,\n                status = parsed_list[[2]][i,]$status,\n                identifier = parsed_list[[2]][i,]$identifier,\n                category = paste0(parsed_list[[2]][i,]$info[[1]]$category[[1]], collapse = \", \"),\n                areaDesc = parsed_list[[2]][i,]$info[[1]]$area[[1]]$areaDesc,\n                responseType = paste0(parsed_list[[2]][i,]$info[[1]]$responseType[[1]], collapse = \", \"),\n                urgency = paste0(unique(parsed_list[[2]][i,]$info[[1]]$urgency), collapse = \", \"),\n                severity = paste0(unique(parsed_list[[2]][i,]$info[[1]]$severity), collapse = \", \"),\n                certainty = paste0(unique(parsed_list[[2]][i,]$info[[1]]$certainty), collapse = \", \"),\n                effective = ifelse(is.null(parsed_list[[2]][i,]$info[[1]]$effective), \"\", paste0(unique(parsed_list[[2]][i,]$info[[1]]$effective), collapse = \", \")),\n                onset = ifelse(is.null(parsed_list[[2]][i,]$info[[1]]$onset), \"\", paste0(unique(parsed_list[[2]][i,]$info[[1]]$onset), collapse = \", \")),\n                expires = ifelse(is.null(parsed_list[[2]][i,]$info[[1]]$expires), \"\", paste0(unique(parsed_list[[2]][i,]$info[[1]]$expires), collapse = \", \")),\n                description = ifelse(is.null(parsed_list[[2]][i,]$info[[1]]$description), \"\", paste0(unique(parsed_list[[2]][i,]$info[[1]]$description), collapse = \", \")),\n                instruction = ifelse(is.null(parsed_list[[2]][i,]$info[[1]]$instruction), \"\", paste0(unique(parsed_list[[2]][i,]$info[[1]]$instruction), collapse = \", \"))\n            )\n            \n            # sometimes evacuation orders for fires might not be coded as category='Fire'\n            # or as responseType='Evacuate' so filter more broadly\n            if((df$category != \"Met\") & (grepl(\"Evacuate\",df$responseType) | (grepl(\"fire\",df$description,ignore.case = TRUE) & grepl(\"evacuat\",df$description,ignore.case = TRUE)) | (grepl(\"fire\",df$instruction,ignore.case = TRUE) & grepl(\"evacuat\",df$instruction,ignore.case = TRUE)))){\n                results_df = rbind(results_df, df)\n                results_poly[[length(results_poly)+1]] = parsed_list[[2]][i,]$info[[1]]$area[[1]]$polygon$coordinates\n            }\n            \n        }\n        # print out progress\n        print(paste0(\"Year: \", year, \", Records Processed: \", skip_records + n_records))\n        # increment page skip count\n        skip_records = skip_records + 1000\n    }\n}\n\n# reformat polygon matrix results to WKT text fields\n# so they can be represented in a column vector\npoly_vec = do.call(\"c\", lapply(results_poly, function(x){\n    paste0(apply(x[[1]], 2, function(y){paste0(y,collapse = \" \")}), collapse = \",\")\n}))\nresults_df$areaPolygon = sub(\"^[^,]*,\", \"\", poly_vec)\nresults_df$areaPolygon = paste0(\"POLYGON((\", poly_vec, \"))\")\n\n\n\nGet Data from the US Census\nThere is a lot of demographic information available via the US Census API, so it helps to have some idea of what to get in advance. In my conversations with ARC, they said age and income are the ones they would consider most important. People who are very old are less likely to leave their home in the event of an evacuation order and consequently less likely to seek shelter from ARC. Similarly, people with higher income may have a second home they can travel to or choose to find paid accommodations with a higher level of comfort than the free but sparse shelter option that ARC is providing.\nI found this information in the American Community Survey (ACS) 5-year data. Here is a link to the census page for this data source. The 5-year data is great because it has information at the block-group level, which is a really low level geography. Smaller geographic units are better because they should allow for closer alignment with the arbitrary areas of the evacuation orders than larger units which may have large areas that are not in the region of interest.\nThe data dictionary is large and hard to wade through. I found what I was looking for under the variable B19037, which provides what is basically the joint distribution of age and income. However, it is not easy to separate age and income from the way the data is given, so I created the mapping manually since the number of cases is small enough and this is the only data I needed.\n\n# read in the mapping I created manually in a csv file\nB19037_mapping = read.csv(\"census_B19037_definitions.csv\")\n\n# display mapping table\nknitr::kable(head(B19037_mapping))\n\n\n\n\nvariable\nage_householder\nincome\n\n\n\n\nB19037_003\n0_24\n0_10K\n\n\nB19037_004\n0_24\n10K_15K\n\n\nB19037_005\n0_24\n15K_20K\n\n\nB19037_006\n0_24\n20K_25K\n\n\nB19037_007\n0_24\n25K_30K\n\n\nB19037_008\n0_24\n30K_35K\n\n\n\n\n\nUsing the census API requires the user to have an API key. This is a really easy thing to apply for and should only take a couple minutes to both apply for it and receive it. Here is the page to apply: https://api.census.gov/data/key_signup.html\n\n# store census API key for use in query\n# using tidycensus package functions\nCENSUS_API_KEY = \"YOUR API KEY HERE\"\ncensus_api_key(CENSUS_API_KEY)\n\n# download the data\nacs5_B19307 = get_acs(geography = \"block group\",\n                      variables = B19307_mapping$variable,\n                      year = 2019,\n                      state = \"CA\",\n                      geometry = TRUE)"
  },
  {
    "objectID": "projects/ca_fire_evac/ca_fire_evac.html#data-transformations",
    "href": "projects/ca_fire_evac/ca_fire_evac.html#data-transformations",
    "title": "California Fire Evacuations",
    "section": "Data Transformations",
    "text": "Data Transformations\nHaving the joint distribution by default is great, but it wouldn’t hurt to decompose age and income into their marginal distributions too. I might as well do that now to prepare for later stages where I might want to use them independently.\n\n# compute marginal distribution for age\ndf_age = as.data.frame(acs5_B19307) %&gt;%\n    merge(B19307_mapping, by = \"variable\", all.x = TRUE) %&gt;%\n    mutate(age_householder = paste0(\"age_\", age_householder)) %&gt;%\n    group_by(GEOID, age_householder) %&gt;%\n    summarise(age_count = sum(estimate)) %&gt;%\n    ungroup() %&gt;%\n    pivot_wider(names_from = age_householder, values_from = age_count)\n\n# compute marginal distribution for income\ndf_income = as.data.frame(acs5_B19307) %&gt;%\n    merge(B19307_mapping, by = \"variable\", all.x = TRUE) %&gt;%\n    group_by(GEOID, income) %&gt;%\n    summarise(income_count = sum(estimate)) %&gt;%\n    ungroup() %&gt;%\n    pivot_wider(names_from = income, values_from = income_count)\n\n# having trouble pivoting original data\n# in the presence of the geometry feature\n# so remove the geometry and do it separately here\ndf_joint = as.data.frame(acs5_B19307) %&gt;%\n    select(GEOID, variable, estimate) %&gt;%\n    pivot_wider(names_from = variable, values_from = estimate)\n\n# join marginal distributions into original data\ndf_acs5 = acs5_B19307 %&gt;%\n    select(GEOID, geometry) %&gt;%\n    filter(!duplicated(GEOID)) %&gt;%\n    inner_join(df_age, by = \"GEOID\") %&gt;%\n    inner_join(df_income, by = \"GEOID\") %&gt;%\n    inner_join(df_joint, by = \"GEOID\") %&gt;%\n    mutate(block_group_area = st_area(.))\n\n\nCombining the Data\nNow that I have data from IPAWS on fire-related evacuation orders in CA, and data on population counts by age and income in CA at the census block group level, I would like to combine that information. This can be done by checking for overlaps between the geometries of the two data sources.\n\n# put IPAWS alert data in a form the sf package likes\n# specifying where to find the geometry information\n# and in what form (WKT)\ndf_alerts = st_as_sf(results_df, wkt = \"areaPolygon\")\nst_crs(df_alerts) = st_crs(df_acs5)\n\n# start by computing just the first case as an example\ndf_intersections = df_alerts[1,] %&gt;%\n    select(identifier, areaPolygon) %&gt;%\n    st_intersection(df_acs5) %&gt;%\n    mutate(intersect_area = st_area(.)) %&gt;%\n    mutate(pct_overlap = as.numeric(intersect_area/block_group_area)) %&gt;%\n    st_drop_geometry()\n\n# plot the census block groups and the\n# evacuation area to show the overlap\nggplot() + \n    geom_sf(data=df_acs5[df_acs5$GEOID %in% df_intersections$GEOID,]) +\n    geom_sf(data=df_alerts[1,],color = \"#F8766D\", fill = \"#F8766D\", alpha = 0.3)\n\n\nEven though the number of evacuation orders is small (just a couple hundred) the time it takes to compute all the intersections and areas is several hours.\n\nfor(i in 2:nrow(df_alerts)){\n    # compute for one evacuation order at a time\n    temp = df_alerts[i,] %&gt;%\n        select(identifier, areaPolygon) %&gt;%\n        st_intersection(df_acs5) %&gt;%\n        mutate(intersect_area = st_area(.)) %&gt;%\n        mutate(pct_overlap = as.numeric(intersect_area/block_group_area))  %&gt;%\n        st_drop_geometry()\n    \n    # merge into larger data frame\n    df_intersections = rbind(df_intersections, temp)\n}\n\n\n\nAggregate by Evacuation Event\nThis combined data shows how the area covered by the evacuation order overlaps with the defined census block groups. This data needs to be aggregated at the level of evacuation orders so that one evacuation order has one set of corresponding variables. Here are a couple of simple options.\n\nWeight the census data by the percentage of the area that overlaps with the evacuation order area. If there is more overlap, more of the data from that census block group will be used. This sounds logical, but it assumes that the population of individuals is uniformly distributed throughout that block group, which does not have to be true.\nGive 100% weight to all census block groups that intersect with the evacuation order area. This ensures no individuals are incorrectly excluded if the population of a block group happens to be concentrated in a small area within that block group. However, it may lead to overestimating the relevant population for the evacuation order.\n\nSince the right choice is not immediately obvious, I will just group the data both ways and figure out which way is preferable at a later stage.\n\n# option 1: census features are weighted average\n# based on the intersection of their block group area\n# with the evacuation order area\ndf_intersections_avg = df_intersections %&gt;%\n    select(-GEOID) %&gt;%\n    group_by(identifier) %&gt;%\n    summarise_all(~ sum(.x * pct_overlap)) %&gt;%\n    select(-pct_overlap) %&gt;%\n    inner_join(results_df[,c(\"identifier\",\"sent\",\"areaDesc\")], by = \"identifier\")\n\n# option 2: census features are summed together\n# (given 100% weight) as long as their block group area\n# has any overlap with the evacuation order area\ndf_intersections_sum = df_intersections %&gt;%\n    select(-GEOID, -pct_overlap) %&gt;%\n    group_by(identifier) %&gt;%\n    summarise_all(~ sum(.x)) %&gt;%\n    inner_join(results_df[,c(\"identifier\",\"sent\",\"areaDesc\")], by = \"identifier\")"
  },
  {
    "objectID": "projects/ca_fire_evac/ca_fire_evac.html#predicting-shelter-demand",
    "href": "projects/ca_fire_evac/ca_fire_evac.html#predicting-shelter-demand",
    "title": "California Fire Evacuations",
    "section": "Predicting Shelter Demand",
    "text": "Predicting Shelter Demand\nThe next step is to use the data gathered around each evacuation event to predict how many people will seek shelter from ARC. This has to be kept very general because I do not have access to the data required to make meaningful progress beyond this point.\n\nGet Data on ARC Shelter Demand\nARC should have some basic data about the number of people who took shelter at ARC shelters during historical wildfire evacuation events. As a volunteer working on this proof of concept, this data was not made available to me at this stage of the analysis. As such, I have to make up some fake data to use here.\nGenerating fake data about shelter demand means none of the following results are meaningful and certain decisions about the analysis cannot be made here. For example, I discussed above two options for how data could be aggregated for unique evacuation events and the way to pick which method to use would be to see which produced the better fitting model. Since I don’t have real data, I cannot evaluate which method will be better, so I will arbitrarily pick the second method just to have some data to use here.\nI will assume the number of people who seek shelter is drawn from a random Poisson distribution (so that it is always an integer number of people), and is about 10% of the impacted population.\n\n# calculate total population from age variables\nn_events = nrow(df_intersections_sum)\nevent_pop = apply(df_intersections_sum[,grepl(\"age\",colnames(df_intersections_sum))], 1, sum)\n\n# add affected population and random poisson shelter demand\ndf_intersections_sum$event_pop = event_pop\ndf_intersections_sum$shelter_demand = rpois(n_events, lambda = 0.1*event_pop)\n\nThis also skips several other complications that are likely to arise with the actual data, such as figuring out how to match the shelter data with the right evacuation event.\n\n\nA Simple Model\nMy first instinct when thinking about this problem is to try a Poisson regression. Since the number of people seeking shelter is count data, the Poisson distribution is convenient for modeling it because it will handle cases where the count is zero if that occurs (no one showed up at the shelter) and will enforce non-negativity.\nIt may be useful to think about this problem in the context of fractions, as in “what fraction of the exposed population will seek shelter?” This is also something easily incorporated within the Poisson regression framework.\nThe basic Poisson regression:\n\\[\n    log(E(Y|x)) = Bx\n\\] Poisson regression modeling counts as a fraction of exposed population:\n\\[\n    log(\\frac{E(Y|x)}{exposure}) = log(E(Y|x)) - log(exposure) = Bx - log(exposure)\n\\]\nI will choose the second option just to showcase how the implementation works. I will start with some data transformations, scaling the features by the exposed population so that they are fractions of the total population rather than counts. This normalizes them across observations. I will just use the marginal age variables here. In practice I would want to include the income variables and then also test whether the interaction variables from the joint distribution are useful, but none of that matters with the fake data I have right now. Then I can fit the model.\n\n# build modeling dataset\n# start with just the age categories\nmodel_data = df_intersections_sum[,grepl(\"age\",colnames(df_intersections_sum))]\n\n# normalize by the exposed population in each event\nmodel_data = sweep(model_data, 1, 1/event_pop, FUN=\"*\")\n\n# drop the first variable to avoid perfect multicollinearity\n# since they will all add up to 1 otherwise\nmodel_data = model_data[,-1]\n\n# add the target variable and exposure variable\nmodel_data$y = df_intersections_sum$shelter_demand\nmodel_data$exposure = event_pop\n\n# fit the model\nfit = glm(y ~ . - exposure + offset(log(exposure)), data = model_data, family=poisson(link=log))\nsummary(fit)\n\n\nCall:\nglm(formula = y ~ . - exposure + offset(log(exposure)), family = poisson(link = log), \n    data = model_data)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  -2.1469     0.1275 -16.844   &lt;2e-16 ***\nage_25_44    -0.1838     0.1637  -1.123    0.261    \nage_45_64    -0.1070     0.1116  -0.959    0.338    \nage_65_inf   -0.1886     0.1356  -1.391    0.164    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 195.42  on 247  degrees of freedom\nResidual deviance: 192.31  on 244  degrees of freedom\nAIC: 2031.1\n\nNumber of Fisher Scoring iterations: 3\n\n\nThe only part of the model that looks significant is the intercept, which is correct because I generated shelter demand as a constant 10% of the exposed population. In the real data, I might expect to see exposed populations made up of younger people or having lower incomes significantly affect shelter demand.\n\n\nPrediction Intervals\nThe Poisson regression I just constructed models expected shelter demand. In other words, the average number of people expected to seek shelter. The estimate produced by the model may over-predict or under-predict. ARC may care more about under-predicting than over-predicting. Depending on the resources available, ARC may prefer to over-predict demand and send more people to open unnecessary shelters as opposed to under-predicting demand and having to turn away people seeking shelter or not have shelter available for them.\nOne way to address this within the context of this model is a prediction interval. Unlike a confidence interval which only incorporates uncertainty about model parameter estimates, a prediction interval also incorporates uncertainty about the outcomes. Building a 95% prediction interval means that we would expect that the observed outcome falls within the interval 95% of the time. Using the upper bound of that interval may be a good way to estimate an upper limit of what shelter demand could be for a particular scenario.\n\n# what percentage of model predictions\n# under-estimate shelter demand?\nmean(model_data$y - predict(fit, type = \"response\") &gt; 0)\n\n[1] 0.5282258\n\n# bootstrap setup\nset.seed(42)\nn_bootstraps = 1000\nboot_results = matrix(0, nrow = nrow(model_data), ncol = n_bootstraps)\n\n# bootstrap prediction interval\nfor(i in 1:n_bootstraps){\n    # re-sample from modeling data with replacement\n    boot_idx = sample(nrow(model_data), replace = TRUE)\n    df_boot = model_data[boot_idx,]\n    # fit the model with bootstrapped data\n    fit_boot = glm(y ~ . - exposure + offset(log(exposure)), \n                   data = df_boot, family=poisson(link=log))\n    # predictions of actual data using bootstrapped model\n    # distributions from this step would give the confidence interval\n    y_pred_boot = predict(fit_boot, newdata = model_data, type = \"response\")\n    # now need to account for residual variance\n    # given that the data is assumed to be poisson distributed\n    # the prediction is the expected value or lambda\n    # so we can just sample from a poisson distribution with that lambda\n    boot_results[,i] = rpois(nrow(model_data), lambda = y_pred_boot)\n}\n\n# 95% prediction interval\nlower_bound = apply(boot_results, 1, quantile, probs = 0.025)\nupper_bound = apply(boot_results, 1, quantile, probs = 0.975)\n\n# what percentage of the time does\n# the upper bound of the 95% prediction interval\n# under-estimate shelter demand?\nmean(model_data$y - upper_bound &gt; 0)\n\n[1] 0.01612903"
  },
  {
    "objectID": "projects/ca_fire_evac/ca_fire_evac.html#conclusion",
    "href": "projects/ca_fire_evac/ca_fire_evac.html#conclusion",
    "title": "California Fire Evacuations",
    "section": "Conclusion",
    "text": "Conclusion\nI have shown a proposed approach for acquiring data from FEMA and the US Census to support making data-driven estimates of shelter demand during wildfire evacuations in California. While the full analysis is incomplete without the actual shelter data from ARC, I laid out some initial possibilities based on the information available."
  },
  {
    "objectID": "projects/risk_parity_portfolio/risk_parity_portfolio.html",
    "href": "projects/risk_parity_portfolio/risk_parity_portfolio.html",
    "title": "Risk Parity Portfolio",
    "section": "",
    "text": "One algorithm under the umbrella of portfolio optimization that sounded interesting to me is called the risk parity portfolio. All of the portfolio optimization methods are about picking a set of weights for the different assets in a portfolio to optimize some objective function. In the case of the risk parity portfolio, the weights are selected so that each asset in the portfolio contributes an equal amount of risk. Risk is measured in terms of standard deviation of returns."
  },
  {
    "objectID": "projects/risk_parity_portfolio/risk_parity_portfolio.html#introoverview",
    "href": "projects/risk_parity_portfolio/risk_parity_portfolio.html#introoverview",
    "title": "Risk Parity Portfolio",
    "section": "",
    "text": "One algorithm under the umbrella of portfolio optimization that sounded interesting to me is called the risk parity portfolio. All of the portfolio optimization methods are about picking a set of weights for the different assets in a portfolio to optimize some objective function. In the case of the risk parity portfolio, the weights are selected so that each asset in the portfolio contributes an equal amount of risk. Risk is measured in terms of standard deviation of returns."
  },
  {
    "objectID": "projects/risk_parity_portfolio/risk_parity_portfolio.html#setting-up",
    "href": "projects/risk_parity_portfolio/risk_parity_portfolio.html#setting-up",
    "title": "Risk Parity Portfolio",
    "section": "Setting Up",
    "text": "Setting Up\n\nLoading Libraries\n\nlibrary(quantmod) # to download stock data from Yahoo Finance\nlibrary(dplyr) # to transform data\nlibrary(tidyr) # to convert from wide to long data format\nlibrary(ggplot2) # to draw charts\n\n\n\nLoading Data\nFor this example I will pick a few tickers with different risk profiles to show how the algorithm works. I will just use one year of data since this is mainly an exercise in understanding the algorithm, not trying to actually build a trading strategy and backtest over long periods.\nThe quantmod package contains a function called getSymbols() which lets the user pick one or more stock symbols and specify a time period, and then it downloads the relevant data from Yahoo Finance.\n\n# pick a few tickers and download the data from Yahoo Finance\nsymbols_vec = c(\"SPY\",\"TLT\",\"GLD\",\"AAPL\",\"NFLX\",\"NVDA\")\ngetSymbols(symbols_vec, from = \"2023-01-01\", to = \"2023-12-31\")\n\n# combine adjusted close prices into one dataset\n# a bit weird because quantmod likes to return each ticker\n# as a new object in the environment\ndf_returns = data.frame()\nfor(i in 1:length(symbols_vec)){\n    # keep only the adjusted returns data\n    cmd = paste0(symbols_vec[i], \"=\", symbols_vec[i], \"[,grepl('Adjusted',colnames(\", symbols_vec[i], \"))]\")\n    eval(parse(text = cmd))\n    # combine into one data frame\n    if(i == 1){\n        cmd = paste0(\"df_returns = \", symbols_vec[i])\n        eval(parse(text = cmd))\n    }else{\n        cmd = paste0(\"df_returns = merge(df_returns, \", symbols_vec[i], \")\")\n        eval(parse(text = cmd))\n    }\n}\n\n# clean up column names\ncolnames(df_returns) = gsub(\".Adjusted\", \"\", colnames(df_returns))\n\n# transform adjusted close prices to daily returns\ndf_returns[-1,] = apply(df_returns, 2, function(x){x[2:length(x)] / x[1:(length(x)-1)] - 1})\ndf_returns = df_returns[-1,]\ndf_returns = data.frame(Date = index(df_returns), df_returns)\n\nHere is how the data looks after this initial cleanup.\n\nknitr::kable(head(df_returns))\n\n\n\n\n\n\n\n\n\n\n\n\n\nDate\nSPY\nTLT\nGLD\nAAPL\nNFLX\nNVDA\n\n\n\n\n2023-01-04\n0.0077202\n0.0136999\n0.0094119\n0.0103143\n0.0490252\n0.0303180\n\n\n2023-01-05\n-0.0114134\n0.0041811\n-0.0124515\n-0.0106046\n0.0009373\n-0.0328158\n\n\n2023-01-06\n0.0229321\n0.0183964\n0.0187075\n0.0367939\n0.0188892\n0.0416404\n\n\n2023-01-09\n-0.0005668\n0.0053241\n0.0022451\n0.0040890\n-0.0012042\n0.0517532\n\n\n2023-01-10\n0.0070129\n-0.0165499\n0.0036760\n0.0044563\n0.0392486\n0.0179805\n\n\n2023-01-11\n0.0126477\n0.0162516\n-0.0006868\n0.0211122\n-0.0008549\n0.0057829"
  },
  {
    "objectID": "projects/risk_parity_portfolio/risk_parity_portfolio.html#benchmarks",
    "href": "projects/risk_parity_portfolio/risk_parity_portfolio.html#benchmarks",
    "title": "Risk Parity Portfolio",
    "section": "Benchmarks",
    "text": "Benchmarks\nIt may be helpful to look at a couple of other portfolio allocation methods to get a sense of how this method compares with other options. I will pick a couple of simple examples to compare with.\n\nEqually Weighted Portfolio\nThis method allocates an equal amount of money to each asset in the portfolio. It is extremely simple, but that is part of what makes it a useful benchmark. The difference between this method and the risk parity portfolio is that the equal weights here mean equally balance-weighted, not equally risk-weighted. Putting an equal amount of money into treasuries vs tech stocks would likely mean that the money in tech stocks contributes much more risk to the portfolio compared with the money invested in treasuries. In other words, while the amount of money invested is diversified across assets, the risk is not necessarily as diversified since one asset class might represent the majority of the risk.\n\n# weight vector for equal weight\nw1 = rep(1, ncol(df_returns)-1)\nw1 = w1/sum(w1)\nw1\n\n[1] 0.1666667 0.1666667 0.1666667 0.1666667 0.1666667 0.1666667\n\n\n\n\nMinimum Risk Portfolio\nThis portfolio optimization problem can be defined as minimizing the total variance of the portfolio subject to the condition that the weights must add up to one.\n\\[\n\\begin{align}\n    Minimize& \\hspace{2.5em} \\frac{1}{2} w^T \\Sigma w \\\\\n    \\text{subject to}& \\hspace{2.5em} 1^T w = 1\n\\end{align}\n\\]\nThis optimization problem can be solved using lagrange multipliers. Construct the langrangian as the combination of the objective function and the constraints, then the two partial derivatives can be set equal to zero and used to solve for the unknown quantities of interest.\n\\[\n\\begin{align}\n    L(x,\\lambda) = f(x) - \\lambda g(x) \\\\\n    \\frac{dL}{dx} = 0 \\hspace{2.5em} \\frac{dL}{d \\lambda} = 0\n\\end{align}\n\\] In the context of this problem this becomes:\n\\[\n\\begin{align}\n    L(w,\\lambda) &= \\frac{1}{2} w^T \\Sigma w - \\lambda (1^T w - 1) \\\\\n    \\frac{dL}{dw} &= \\Sigma w - \\lambda 1^T = 0 \\\\\n    w &= \\lambda \\Sigma^{-1} 1 \\\\\n    \\\\\n    \\frac{dL}{d \\lambda} &= 1^T w - 1 = 0 \\\\\n    1^T w &= 1\n\\end{align}\n\\] Multiplying the first equation through by 1 on the left side lets us use the constraint from the second equation which removes \\(w\\) and allows us to solve for \\(\\lambda\\).\n\\[\n\\begin{align}\n    w &= \\lambda \\Sigma^{-1} 1 \\\\\n    1^T w &= \\lambda 1^T \\Sigma^{-1} 1 \\\\\n    1 &= \\lambda 1^T \\Sigma^{-1} 1 \\\\\n    \\lambda &= \\frac{1}{1^T \\Sigma^{-1} 1}\n\\end{align}\n\\] Plugging \\(\\lambda\\) back into the equation for \\(w\\) solves for \\(w\\).\n\\[\n\\begin{align}\n    w &= \\lambda \\Sigma^{-1} 1 \\\\\n    &= \\frac{\\Sigma^{-1} 1}{1^T \\Sigma^{-1} 1}\n\\end{align}\n\\]\nThis is simple to implement, it just involves computing and then inverting the covariance matrix \\(\\Sigma\\) for the daily returns data.\n\n# get inverse of covariance matrix of returns\ncov_mat = cov(df_returns[,-1])\ncov_mat_inv = solve(cov_mat)\n\n# compute weights\nvec1 = rep(1, nrow(cov_mat_inv))\nlambda = vec1 %*% cov_mat_inv %*% vec1\nlambda = as.numeric(1/lambda)\nw2 = as.vector(lambda * cov_mat_inv %*% vec1)\nw2\n\n[1]  0.61758079  0.07622508  0.41066270 -0.02959958 -0.03462770 -0.04024129\n\n\nOne interesting thing to note here is that while the weights were constrained to sum to 1, they were not constrained to be non-negative. The negative weights here on the riskier assets mean that achieving the minimum variance portfolio requires taking short positions in those assets."
  },
  {
    "objectID": "projects/risk_parity_portfolio/risk_parity_portfolio.html#risk-parity-portfolio",
    "href": "projects/risk_parity_portfolio/risk_parity_portfolio.html#risk-parity-portfolio",
    "title": "Risk Parity Portfolio",
    "section": "Risk Parity Portfolio",
    "text": "Risk Parity Portfolio\nThis time the problem is not to minimize risk, it is to impose a constraint that each asset in the portfolio contributes equally to the total risk.\n\nTheory\nThe total variance of the portfolio was minimized in the objective function of the minimum variance portfolio. Standard deviation is just the square root of variance, so the total standard deviation of the portfolio is \\(\\sigma (w) = \\sqrt{w^T \\Sigma w}\\). This can be decomposed by taking the partial derivative with respect to the weight of each individual asset in the portfolio.\n\\[\n    \\sigma(w) = \\sum_{i=1}^N w_i \\frac{d \\sigma}{dw_i} = \\sum_{i=1}^N \\frac{w_i(\\Sigma w)_i}{\\sqrt{w^T \\Sigma w}}\n\\]\nIn other words, asset \\(i\\)’s contribution to the total portfolio standard deviation, or its Risk Contribution (RC) is:\n\\[\n    RC_i = \\frac{w_i(\\Sigma w)_i}{\\sqrt{w^T \\Sigma w}}\n\\]\nThe constraint that needs to be imposed is to have each asset have the same risk contribution, which should be an equal amount of the total risk of the portfolio.\n\\[\n    RC_i = \\frac{1}{N} \\sigma(w) = \\frac{1}{N} \\sqrt{w^T \\Sigma w}\n\\] Dividing both sides by \\(\\sigma(w)\\) normalizes by total portfolio risk, giving us:\n\\[\n\\begin{align}\n    \\frac{w_i(\\Sigma w)_i}{\\sqrt{w^T \\Sigma w}} &= \\frac{1}{N} \\sqrt{w^T \\Sigma w} \\\\\n    \\frac{w_i(\\Sigma w)_i}{w^T \\Sigma w} &= \\frac{1}{N}\n\\end{align}\n\\]\nNext comes a change of variables in two places. One is substituting \\(b_i = \\frac{1}{N}\\) which is a generalization and will allow us to specify something other than equal risk contributions if desired later. Another is \\(x_i = \\frac{w_i}{\\sqrt{w^T \\Sigma w}}\\), or said another way, \\(x\\) is just the weight vector \\(w\\) normalized by total portfolio standard deviation. After this change of variables, the last equation now looks like this:\n\\[\n\\begin{align}\n    \\frac{w_i(\\Sigma w)_i}{w^T \\Sigma w} &= \\frac{1}{N} \\\\\n    \\frac{w_i}{\\sqrt{w^T \\Sigma w}} \\frac{(\\Sigma w)_i}{\\sqrt{w^T \\Sigma w}} &= b_i \\\\\n    x_i (\\Sigma x)_i &= b_i \\\\\n    x_i (\\Sigma x)_i - b_i &= 0\n\\end{align}\n\\]\nThis can be written out further to separate \\(x_i\\) from other weights \\(x_j\\) to yield a quadratic equation.\n\\[\n\\begin{align}\n    x_i (\\Sigma x)_i - b_i &= 0 \\\\\n    \\sigma_{ii}^2 x_i^2 + x_i\\sum_{i=/=j}\\sigma_{ij}^2 x_j - b_i &= 0\n\\end{align}\n\\]\nThen the solution \\(x_i^*\\) comes from the quadratic formula \\(x^* = \\frac{-b + \\sqrt{b^2 - 4ac}}{2a}\\)\n\\[\n    x_i^* = \\frac{-\\sum_{i=/=j}\\sigma_{ij}^2 x_j + \\sqrt{(\\sum_{i=/=j}\\sigma_{ij}^2 x_j)^2 + 4 \\sigma_{ii}^2 b_i}}{2\\sigma_{ii}^2}\n\\]\nSince each weight \\(x_i\\) is written in terms of the others \\(x_j\\) we iteratively update the weights until convergence is reached.\n\n\nImplementation\n\n# write it as a function that takes a covariance matrix as input\nrpp &lt;- function(cov_mat, b = NULL, tolerance = 0.00000001, max_iter = 100){\n    # initialize weights\n    # as equal weights normalized by total portfolio risk\n    x = rep(1, ncol(cov_mat))\n    x = x/sqrt(sum(cov_mat))\n    sigma_x = cov_mat %*% x\n    # use equal risk weights by default\n    if(is.null(b)){\n        b = rep(1, ncol(cov_mat))\n        b = b/sum(b)\n    }\n    # update weights\n    for(n in 1:max_iter){\n        for(i in 1:nrow(cov_mat)){\n            z = sigma_x[i] - cov_mat[i,i]*x[i]\n            x_star = (-z + sqrt(z^2 + 4*cov_mat[i,i]*b[i])) / (2*cov_mat[i,i])\n            x[i] = x_star\n            sigma_x = cov_mat %*% x\n        }\n        # check for convergence, stop if converged\n        if(max(abs(x * as.vector(cov_mat %*% x) - b)) &lt; tolerance){\n            break\n        }\n    }\n    return(x/sum(x))\n}\n\n# get RPP weights\nw3 = rpp(cov_mat)\nw3\n\n[1] 0.20723268 0.19614619 0.30177870 0.14234206 0.08513825 0.06736213\n\n# check that the weights result in equal risk contributions\nw3 * as.vector(cov_mat %*% w3) / sqrt(as.numeric(w3 %*% cov_mat %*% w3))\n\n[1] 0.001235473 0.001235473 0.001235473 0.001235473 0.001235473 0.001235473"
  },
  {
    "objectID": "projects/risk_parity_portfolio/risk_parity_portfolio.html#performance",
    "href": "projects/risk_parity_portfolio/risk_parity_portfolio.html#performance",
    "title": "Risk Parity Portfolio",
    "section": "Performance",
    "text": "Performance\nNow we can compare these three methods and see how they differ in terms of returns and how the weights change over time. For simplicity I will assume that re-balancing happens daily with no transaction costs, and that the weights are calculated at the start of each month by looking backwards at the prior 3-month period.\n\n# pull out month as a feature\ndf_returns$month = as.integer(substr(df_returns$Date, 6, 7))\n\n# for each month get weights based on previous 3 months\ndf_backtest = data.frame()\ndf_w = data.frame()\nfor(i in 4:12){\n    # previous 3 months of returns\n    df = df_returns[(df_returns$month &gt;= i-3) & (df_returns$month &lt;= i-1),]\n    \n    # equal weights\n    w1 = rep(1, ncol(df)-2)\n    w1 = w1/sum(w1)\n    \n    # minimum variance weights\n    cov_mat = cov(df[,!(colnames(df) %in% c(\"Date\",\"month\"))])\n    cov_mat_inv = solve(cov_mat)\n    vec1 = rep(1, nrow(cov_mat_inv))\n    lambda = vec1 %*% cov_mat_inv %*% vec1\n    lambda = as.numeric(1/lambda)\n    w2 = as.vector(lambda * cov_mat_inv %*% vec1)\n    \n    # risk parity portfolio weights\n    w3 = rpp(cov_mat)\n    \n    # returns for the new month\n    df = df_returns[df_returns$month == i,]\n    dt = df$Date\n    df = df[,!(colnames(df) %in% c(\"Date\",\"month\"))]\n    r1 = rowSums(sweep(df, 2, w1, FUN=\"*\"))\n    r2 = rowSums(sweep(df, 2, w2, FUN=\"*\"))\n    r3 = rowSums(sweep(df, 2, w3, FUN=\"*\"))\n    rets = data.frame(Date = dt, w_eq = r1, w_min_var = r2, w_rpp = r3)\n    df_backtest = rbind(df_backtest, rets)\n    \n    # store weight vectors over time\n    w_month = data.frame(\n        Date = as.Date(paste0(\"2023-\",formatC(i, width=2, format=\"d\", flag=\"0\"),\"-01\")),\n        type = c(rep(\"eq\", 6), rep(\"min_var\", 6), rep(\"rpp\", 6)),\n        asset = rep(colnames(df), 3),\n        weight = c(w1, w2, w3)\n    )\n    df_w = rbind(df_w, w_month)\n}\n\n# plot of performance\ndf_backtest %&gt;%\n    mutate(Date = as.Date(Date)) %&gt;%\n    mutate(w_eq = cumprod(1 + w_eq) - 1) %&gt;%\n    mutate(w_min_var = cumprod(1 + w_min_var) - 1) %&gt;%\n    mutate(w_rpp = cumprod(1 + w_rpp) - 1) %&gt;%\n    pivot_longer(!Date, names_to = \"portfolio\", values_to = \"cum_ret\") %&gt;%\n    ggplot(aes(x = Date, y = cum_ret, color = portfolio)) +\n    geom_line() +\n    labs(y = \"Cumulative Returns\") +\n    theme(legend.position = \"bottom\")\n\n\n\n# plot of weights over time for the 3 different methods\ndf_w %&gt;%\n    ggplot(aes(x = Date, y = weight, fill = asset)) +\n    geom_bar(position = \"stack\", stat = \"identity\") +\n    facet_grid(type ~ .)"
  },
  {
    "objectID": "projects/risk_parity_portfolio/risk_parity_portfolio.html#conclusion",
    "href": "projects/risk_parity_portfolio/risk_parity_portfolio.html#conclusion",
    "title": "Risk Parity Portfolio",
    "section": "Conclusion",
    "text": "Conclusion\nThe risk parity portfolio represents an interesting way to look at the portfolio allocation problem, focusing on the constraints as the objective. I only scratched the surface of the implementation, as there are other conditions that could also be considered such as box constraints for the weights or adding other conditions to optimize for. The riskParityPortfolio R package has excellent vignettes to explain the material for the interested reader."
  }
]